Namespace(model='nnet', skip_preprocess=True, train_prop='0.50')

Training a nnet model

Iteration 1, loss = 0.83492094
Iteration 2, loss = 0.53467035
Iteration 3, loss = 0.47469591
Iteration 4, loss = 0.45591858
Iteration 5, loss = 0.44445919
Iteration 6, loss = 0.43317608
Iteration 7, loss = 0.42498634
Iteration 8, loss = 0.41844472
Iteration 9, loss = 0.41534307
Iteration 10, loss = 0.40697048
Iteration 11, loss = 0.40522595
Iteration 12, loss = 0.39786538
Iteration 13, loss = 0.39530238
Iteration 14, loss = 0.39431728
Iteration 15, loss = 0.38856267
Iteration 16, loss = 0.38441477
Iteration 17, loss = 0.38637713
Iteration 18, loss = 0.37912554
Iteration 19, loss = 0.37800011
Iteration 20, loss = 0.37881570
Iteration 21, loss = 0.37245251
Iteration 22, loss = 0.37325549
Iteration 23, loss = 0.36855997
Iteration 24, loss = 0.36822180
Iteration 25, loss = 0.36645371
Iteration 26, loss = 0.36461582
Iteration 27, loss = 0.36443504
Iteration 28, loss = 0.36120158
Iteration 29, loss = 0.36097894
Iteration 30, loss = 0.35921699
Iteration 31, loss = 0.35798202
Iteration 32, loss = 0.35508390
Iteration 33, loss = 0.35581222
Iteration 34, loss = 0.35205508
Iteration 35, loss = 0.35105548
Iteration 36, loss = 0.35216919
Iteration 37, loss = 0.34957546
Iteration 38, loss = 0.34927880
Iteration 39, loss = 0.34762033
Iteration 40, loss = 0.34753184
Iteration 41, loss = 0.34561070
Iteration 42, loss = 0.34387430
Iteration 43, loss = 0.34449659
Iteration 44, loss = 0.34355768
Iteration 45, loss = 0.34195359
Iteration 46, loss = 0.34356244
Iteration 47, loss = 0.34134127
Iteration 48, loss = 0.33745643
Iteration 49, loss = 0.34084330
Iteration 50, loss = 0.33690544
Iteration 51, loss = 0.33938888
Iteration 52, loss = 0.33564104
Iteration 53, loss = 0.33580140
Iteration 54, loss = 0.33567607
Iteration 55, loss = 0.33305689
Iteration 56, loss = 0.33302054
Iteration 57, loss = 0.33554619
Iteration 58, loss = 0.33159574
Iteration 59, loss = 0.33156182
Iteration 60, loss = 0.32832256
Iteration 61, loss = 0.33046146
Iteration 62, loss = 0.32840941
Iteration 63, loss = 0.33151319
Iteration 64, loss = 0.33015725
Iteration 65, loss = 0.32866752
Iteration 66, loss = 0.32847886
Iteration 67, loss = 0.32734340
Iteration 68, loss = 0.32606155
Iteration 69, loss = 0.32715975
Iteration 70, loss = 0.32571665
Iteration 71, loss = 0.32529687
Iteration 72, loss = 0.32521000
Iteration 73, loss = 0.32657550
Iteration 74, loss = 0.32361981
Iteration 75, loss = 0.32577831
Iteration 76, loss = 0.32243088
Iteration 77, loss = 0.32125126
Iteration 78, loss = 0.32140286
Iteration 79, loss = 0.32268695
Iteration 80, loss = 0.32275430
Iteration 81, loss = 0.32234230
Iteration 82, loss = 0.31969117
Iteration 83, loss = 0.32077555
Iteration 84, loss = 0.32184251
Iteration 85, loss = 0.31917922
Iteration 86, loss = 0.32009297
Iteration 87, loss = 0.32184447
Iteration 88, loss = 0.32022327
Iteration 89, loss = 0.32090922
Iteration 90, loss = 0.31831857
Iteration 91, loss = 0.31716666
Iteration 92, loss = 0.31805393
Iteration 93, loss = 0.31777714
Iteration 94, loss = 0.31986395
Iteration 95, loss = 0.31755465
Iteration 96, loss = 0.31727123
Iteration 97, loss = 0.31761357
Iteration 98, loss = 0.31723150
Iteration 99, loss = 0.31709314
Iteration 100, loss = 0.31683703
Iteration 101, loss = 0.31774486
Iteration 102, loss = 0.31649907
Iteration 103, loss = 0.31580338
Iteration 104, loss = 0.31467521
Iteration 105, loss = 0.31644613
Iteration 106, loss = 0.31663085
Iteration 107, loss = 0.31540521
Iteration 108, loss = 0.31393324
Iteration 109, loss = 0.31518289
Iteration 110, loss = 0.31492865
Iteration 111, loss = 0.31537728
Iteration 112, loss = 0.31403348
Iteration 113, loss = 0.31473062
Iteration 114, loss = 0.31517917
Iteration 115, loss = 0.31303210
Iteration 116, loss = 0.31804680
Iteration 117, loss = 0.31238948
Iteration 118, loss = 0.31240728
Iteration 119, loss = 0.31329946
Iteration 120, loss = 0.31308106
Iteration 121, loss = 0.31159087
Iteration 122, loss = 0.31170701
Iteration 123, loss = 0.31234951
Iteration 124, loss = 0.31187824
Iteration 125, loss = 0.31096133
Iteration 126, loss = 0.31077569
Iteration 127, loss = 0.31109886
Iteration 128, loss = 0.31065944
Iteration 129, loss = 0.30962200
Iteration 130, loss = 0.31201749
Iteration 131, loss = 0.31224800
Iteration 132, loss = 0.31036936
Iteration 133, loss = 0.31102850
Iteration 134, loss = 0.31256451
Iteration 135, loss = 0.30925354
Iteration 136, loss = 0.31016579
Iteration 137, loss = 0.31219977
Iteration 138, loss = 0.31039127
Iteration 139, loss = 0.30979831
Iteration 140, loss = 0.30861560
Iteration 141, loss = 0.30930690
Iteration 142, loss = 0.30812657
Iteration 143, loss = 0.30967741
Iteration 144, loss = 0.30970127
Iteration 145, loss = 0.30641047
Iteration 146, loss = 0.30847115
Iteration 147, loss = 0.30738963
Iteration 148, loss = 0.30786202
Iteration 149, loss = 0.30724673
Iteration 150, loss = 0.30743862
Iteration 151, loss = 0.30693733
Iteration 152, loss = 0.30759032
Iteration 153, loss = 0.30754170
Iteration 154, loss = 0.30932164
Iteration 155, loss = 0.30886462
Iteration 156, loss = 0.30626111
Iteration 157, loss = 0.30792360
Iteration 158, loss = 0.30464498
Iteration 159, loss = 0.30882433
Iteration 160, loss = 0.30724446
Iteration 161, loss = 0.30784309
Iteration 162, loss = 0.30852614
Iteration 163, loss = 0.30671827
Iteration 164, loss = 0.30851829
Iteration 165, loss = 0.30896467
Iteration 166, loss = 0.30695755
Iteration 167, loss = 0.30715855
Iteration 168, loss = 0.30664219
Iteration 169, loss = 0.30423594
Iteration 170, loss = 0.30476310
Iteration 171, loss = 0.30756977
Iteration 172, loss = 0.30607697
Iteration 173, loss = 0.30798526
Iteration 174, loss = 0.30553632
Iteration 175, loss = 0.30509652
Iteration 176, loss = 0.30747785
Iteration 177, loss = 0.30564686
Iteration 178, loss = 0.30669914
Iteration 179, loss = 0.30432057
Iteration 180, loss = 0.30542806
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Training time took 445.8099875450134 seconds

Trained a model using MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(120, 120, 120), learning_rate='adaptive',
              learning_rate_init=0.001, max_iter=200, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=123, shuffle=True, solver='adam', tol=0.0001,
              validation_fraction=0.1, verbose=True, warm_start=False)
Train FNC Score:
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |   1305    |    48     |    225    |    267    |
-------------------------------------------------------------
| disagree  |    60     |    206    |    78     |    98     |
-------------------------------------------------------------
|  discuss  |    171    |    26     |   3589    |    667    |
-------------------------------------------------------------
| unrelated |    112    |    23     |    325    |   17786   |
-------------------------------------------------------------
Score: 9698.5 out of 11301.5	(85.81604211830287%)
Train F1 Score: 0.7719716210433804
Train Accuracy Score: 0.91595293364284

Test FNC Score:
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    231    |    16     |    822    |    834    |
-------------------------------------------------------------
| disagree  |    57     |     1     |    214    |    425    |
-------------------------------------------------------------
|  discuss  |    576    |    48     |   1948    |   1892    |
-------------------------------------------------------------
| unrelated |    564    |    35     |   1112    |   16638   |
-------------------------------------------------------------
Score: 6772.75 out of 11651.25	(58.12895612058792%)
Test F1 Score: 0.36721557160091345
Test Accuracy Score: 0.740487152244914

Namespace(model='nnet', skip_preprocess=True, test_prop=1, train_prop='0.25')

Training a nnet model

Iteration 1, loss = 1.73711461
Iteration 2, loss = 1.63259606
Iteration 3, loss = 1.54279340
Iteration 4, loss = 1.44889047
Iteration 5, loss = 1.07261071
Iteration 6, loss = 0.80382165
Iteration 7, loss = 0.75635593
Iteration 8, loss = 0.73023282
Iteration 9, loss = 0.70897260
Iteration 10, loss = 0.68909345
Iteration 11, loss = 0.67019322
Iteration 12, loss = 0.65254132
Iteration 13, loss = 0.63608149
Iteration 14, loss = 0.62033060
Iteration 15, loss = 0.60503336
Iteration 16, loss = 0.59041671
Iteration 17, loss = 0.57642601
Iteration 18, loss = 0.56292342
Iteration 19, loss = 0.54982747
Iteration 20, loss = 0.53727429
Iteration 21, loss = 0.52477674
Iteration 22, loss = 0.51284230
Iteration 23, loss = 0.50114989
Iteration 24, loss = 0.48976553
Iteration 25, loss = 0.47880062
Iteration 26, loss = 0.46815019
Iteration 27, loss = 0.45787621
Iteration 28, loss = 0.44811926
Iteration 29, loss = 0.43818527
Iteration 30, loss = 0.42880946
Iteration 31, loss = 0.41989014
Iteration 32, loss = 0.41115157
Iteration 33, loss = 0.40293081
Iteration 34, loss = 0.39489573
Iteration 35, loss = 0.38729258
Iteration 36, loss = 0.37992736
Iteration 37, loss = 0.37287454
Iteration 38, loss = 0.36616585
Iteration 39, loss = 0.35951603
Iteration 40, loss = 0.35334077
Iteration 41, loss = 0.34749634
Iteration 42, loss = 0.34214850
Iteration 43, loss = 0.33584126
Iteration 44, loss = 0.33094667
Iteration 45, loss = 0.32559153
Iteration 46, loss = 0.32063948
Iteration 47, loss = 0.31587515
Iteration 48, loss = 0.31147162
Iteration 49, loss = 0.30729546
Iteration 50, loss = 0.30329084
Iteration 51, loss = 0.29959379
Iteration 52, loss = 0.29563372
Iteration 53, loss = 0.29211319
Iteration 54, loss = 0.28905153
Iteration 55, loss = 0.28582413
Iteration 56, loss = 0.28284561
Iteration 57, loss = 0.27988138
Iteration 58, loss = 0.27695956
Iteration 59, loss = 0.27464838
Iteration 60, loss = 0.27227633
Iteration 61, loss = 0.26974880
Iteration 62, loss = 0.26748310
Iteration 63, loss = 0.26481416
Iteration 64, loss = 0.26259498
Iteration 65, loss = 0.26106008
Iteration 66, loss = 0.25926439
Iteration 67, loss = 0.25740042
Iteration 68, loss = 0.25591229
Iteration 69, loss = 0.25453686
Iteration 70, loss = 0.25304480
Iteration 71, loss = 0.25090668
Iteration 72, loss = 0.25014640
Iteration 73, loss = 0.24873463
Iteration 74, loss = 0.24718574
Iteration 75, loss = 0.24623267
Iteration 76, loss = 0.24510428
Iteration 77, loss = 0.24468944
Iteration 78, loss = 0.24300655
Iteration 79, loss = 0.24157474
Iteration 80, loss = 0.24173072
Iteration 81, loss = 0.24001031
Iteration 82, loss = 0.23912978
Iteration 83, loss = 0.23804989
Iteration 84, loss = 0.23735032
Iteration 85, loss = 0.23647035
Iteration 86, loss = 0.23600099
Iteration 87, loss = 0.23508322
Iteration 88, loss = 0.23455667
Iteration 89, loss = 0.23344253
Iteration 90, loss = 0.23310008
Iteration 91, loss = 0.23244865
Iteration 92, loss = 0.23177352
Iteration 93, loss = 0.23091017
Iteration 94, loss = 0.23144986
Iteration 95, loss = 0.23069602
Iteration 96, loss = 0.22965930
Iteration 97, loss = 0.22983836
Iteration 98, loss = 0.22922530
Iteration 99, loss = 0.22863543
Iteration 100, loss = 0.22804096
Iteration 101, loss = 0.22788882
Iteration 102, loss = 0.22758870
Iteration 103, loss = 0.22674179
Iteration 104, loss = 0.22685558
Iteration 105, loss = 0.22637226
Iteration 106, loss = 0.22645117
Iteration 107, loss = 0.22613349
Iteration 108, loss = 0.22502440
Iteration 109, loss = 0.22518127
Iteration 110, loss = 0.22541039
Iteration 111, loss = 0.22500780
Iteration 112, loss = 0.22462330
Iteration 113, loss = 0.22434136
Iteration 114, loss = 0.22373977
Iteration 115, loss = 0.22387882
Iteration 116, loss = 0.22382103
Iteration 117, loss = 0.22320805
Iteration 118, loss = 0.22330296
Iteration 119, loss = 0.22317026
Iteration 120, loss = 0.22251382
Iteration 121, loss = 0.22335304
Iteration 122, loss = 0.22247624
Iteration 123, loss = 0.22212406
Iteration 124, loss = 0.22186155
Iteration 125, loss = 0.22168539
Iteration 126, loss = 0.22172392
Iteration 127, loss = 0.22156954
Iteration 128, loss = 0.22137957
Iteration 129, loss = 0.22086386
Iteration 130, loss = 0.22110184
Iteration 131, loss = 0.22110207
Iteration 132, loss = 0.22062157
Iteration 133, loss = 0.22089961
Iteration 134, loss = 0.22068762
Iteration 135, loss = 0.22049166
Iteration 136, loss = 0.22048361
Iteration 137, loss = 0.21992380
Iteration 138, loss = 0.22038417
Iteration 139, loss = 0.21930309
Iteration 140, loss = 0.21945871
Iteration 141, loss = 0.21964763
Iteration 142, loss = 0.21901146
Iteration 143, loss = 0.21906345
Iteration 144, loss = 0.21841973
Iteration 145, loss = 0.21862405
Iteration 146, loss = 0.21794103
Iteration 147, loss = 0.21831499
Iteration 148, loss = 0.21772620
Iteration 149, loss = 0.21769681
Iteration 150, loss = 0.21788364
Iteration 151, loss = 0.21767650
Iteration 152, loss = 0.21782762
Iteration 153, loss = 0.21727796
Iteration 154, loss = 0.21735700
Iteration 155, loss = 0.21709963
Iteration 156, loss = 0.21741877
Iteration 157, loss = 0.21708506
Iteration 158, loss = 0.21705690
Iteration 159, loss = 0.21722972
Iteration 160, loss = 0.21713308
Iteration 161, loss = 0.21675316
Iteration 162, loss = 0.21706504
Iteration 163, loss = 0.21689369
Iteration 164, loss = 0.21636430
Iteration 165, loss = 0.21711134
Iteration 166, loss = 0.21634417
Iteration 167, loss = 0.21596606
Iteration 168, loss = 0.21628047
Iteration 169, loss = 0.21590580
Iteration 170, loss = 0.21591473
Iteration 171, loss = 0.21591073
Iteration 172, loss = 0.21561322
Iteration 173, loss = 0.21558956
Iteration 174, loss = 0.21516836
Iteration 175, loss = 0.21505079
Iteration 176, loss = 0.21504883
Iteration 177, loss = 0.21545106
Iteration 178, loss = 0.21528048
Iteration 179, loss = 0.21528578
Iteration 180, loss = 0.21549554
Iteration 181, loss = 0.21501270
Iteration 182, loss = 0.21535856
Iteration 183, loss = 0.21482919
Iteration 184, loss = 0.21475372
Iteration 185, loss = 0.21454596
Iteration 186, loss = 0.21437744
Iteration 187, loss = 0.21421482
Iteration 188, loss = 0.21447437
Iteration 189, loss = 0.21426319
Iteration 190, loss = 0.21380735
Iteration 191, loss = 0.21440905
Iteration 192, loss = 0.21408389
Iteration 193, loss = 0.21368345
Iteration 194, loss = 0.21460201
Iteration 195, loss = 0.21368845
Iteration 196, loss = 0.21411965
Iteration 197, loss = 0.21287957
Iteration 198, loss = 0.21444780
Iteration 199, loss = 0.21297083
Iteration 200, loss = 0.21320950
Training time took 69.87309694290161 seconds

Trained a model using MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(3, 3), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
       random_state=123, shuffle=True, solver='adam', tol=0.0001,
       validation_fraction=0.1, verbose=True, warm_start=False)

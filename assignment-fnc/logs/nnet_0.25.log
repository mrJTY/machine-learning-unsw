Namespace(model='nnet', skip_preprocess=True, train_prop='0.25')

Training a nnet model

Iteration 1, loss = 0.91560350
Iteration 2, loss = 0.73217355
Iteration 3, loss = 0.56428104
Iteration 4, loss = 0.48211304
Iteration 5, loss = 0.46082582
Iteration 6, loss = 0.44951144
Iteration 7, loss = 0.44260246
Iteration 8, loss = 0.43404670
Iteration 9, loss = 0.42650657
Iteration 10, loss = 0.42174893
Iteration 11, loss = 0.42023649
Iteration 12, loss = 0.41135496
Iteration 13, loss = 0.40770847
Iteration 14, loss = 0.40018747
Iteration 15, loss = 0.39882025
Iteration 16, loss = 0.39422416
Iteration 17, loss = 0.38972497
Iteration 18, loss = 0.38700845
Iteration 19, loss = 0.38671732
Iteration 20, loss = 0.38029344
Iteration 21, loss = 0.38062111
Iteration 22, loss = 0.37533441
Iteration 23, loss = 0.37126303
Iteration 24, loss = 0.36957630
Iteration 25, loss = 0.36662344
Iteration 26, loss = 0.36474596
Iteration 27, loss = 0.36250009
Iteration 28, loss = 0.35954973
Iteration 29, loss = 0.35692721
Iteration 30, loss = 0.35607796
Iteration 31, loss = 0.35344983
Iteration 32, loss = 0.35490433
Iteration 33, loss = 0.35425559
Iteration 34, loss = 0.34983089
Iteration 35, loss = 0.35038051
Iteration 36, loss = 0.34713991
Iteration 37, loss = 0.34488454
Iteration 38, loss = 0.34460714
Iteration 39, loss = 0.35043606
Iteration 40, loss = 0.34261673
Iteration 41, loss = 0.34445712
Iteration 42, loss = 0.33751967
Iteration 43, loss = 0.34054005
Iteration 44, loss = 0.33848482
Iteration 45, loss = 0.33315427
Iteration 46, loss = 0.33549876
Iteration 47, loss = 0.33428410
Iteration 48, loss = 0.33163912
Iteration 49, loss = 0.33138714
Iteration 50, loss = 0.32918022
Iteration 51, loss = 0.32858860
Iteration 52, loss = 0.32959594
Iteration 53, loss = 0.32623721
Iteration 54, loss = 0.32461297
Iteration 55, loss = 0.32745674
Iteration 56, loss = 0.32510065
Iteration 57, loss = 0.32136567
Iteration 58, loss = 0.32259211
Iteration 59, loss = 0.32443761
Iteration 60, loss = 0.32018314
Iteration 61, loss = 0.31978197
Iteration 62, loss = 0.31756056
Iteration 63, loss = 0.31791992
Iteration 64, loss = 0.31918443
Iteration 65, loss = 0.31363007
Iteration 66, loss = 0.31475020
Iteration 67, loss = 0.31932911
Iteration 68, loss = 0.31641932
Iteration 69, loss = 0.31207485
Iteration 70, loss = 0.31251381
Iteration 71, loss = 0.30915123
Iteration 72, loss = 0.31071925
Iteration 73, loss = 0.30926203
Iteration 74, loss = 0.30898992
Iteration 75, loss = 0.30906191
Iteration 76, loss = 0.30951830
Iteration 77, loss = 0.30924226
Iteration 78, loss = 0.30822004
Iteration 79, loss = 0.30584812
Iteration 80, loss = 0.30427371
Iteration 81, loss = 0.30856025
Iteration 82, loss = 0.31253007
Iteration 83, loss = 0.30379522
Iteration 84, loss = 0.30254403
Iteration 85, loss = 0.30148801
Iteration 86, loss = 0.30245099
Iteration 87, loss = 0.30161342
Iteration 88, loss = 0.30272719
Iteration 89, loss = 0.29968117
Iteration 90, loss = 0.30310459
Iteration 91, loss = 0.29845453
Iteration 92, loss = 0.30474373
Iteration 93, loss = 0.29886144
Iteration 94, loss = 0.30614523
Iteration 95, loss = 0.29802529
Iteration 96, loss = 0.29803147
Iteration 97, loss = 0.29699611
Iteration 98, loss = 0.29860385
Iteration 99, loss = 0.29618014
Iteration 100, loss = 0.29891130
Iteration 101, loss = 0.29811574
Iteration 102, loss = 0.29338351
Iteration 103, loss = 0.29244581
Iteration 104, loss = 0.30159649
Iteration 105, loss = 0.29229475
Iteration 106, loss = 0.29814687
Iteration 107, loss = 0.28967328
Iteration 108, loss = 0.29277744
Iteration 109, loss = 0.29203751
Iteration 110, loss = 0.28918966
Iteration 111, loss = 0.28948345
Iteration 112, loss = 0.29030192
Iteration 113, loss = 0.28952351
Iteration 114, loss = 0.28852713
Iteration 115, loss = 0.29052409
Iteration 116, loss = 0.29186259
Iteration 117, loss = 0.28834268
Iteration 118, loss = 0.28773205
Iteration 119, loss = 0.28816430
Iteration 120, loss = 0.28841133
Iteration 121, loss = 0.28605309
Iteration 122, loss = 0.29119171
Iteration 123, loss = 0.28662182
Iteration 124, loss = 0.28623848
Iteration 125, loss = 0.28680584
Iteration 126, loss = 0.28378103
Iteration 127, loss = 0.28627968
Iteration 128, loss = 0.29006855
Iteration 129, loss = 0.28491341
Iteration 130, loss = 0.28245721
Iteration 131, loss = 0.28737858
Iteration 132, loss = 0.28577331
Iteration 133, loss = 0.28303485
Iteration 134, loss = 0.28567725
Iteration 135, loss = 0.28292849
Iteration 136, loss = 0.28249701
Iteration 137, loss = 0.28181692
Iteration 138, loss = 0.28286512
Iteration 139, loss = 0.28353907
Iteration 140, loss = 0.28201794
Iteration 141, loss = 0.28114306
Iteration 142, loss = 0.28305813
Iteration 143, loss = 0.27937231
Iteration 144, loss = 0.28448589
Iteration 145, loss = 0.27754135
Iteration 146, loss = 0.27871985
Iteration 147, loss = 0.27880802
Iteration 148, loss = 0.28158045
Iteration 149, loss = 0.27733196
Iteration 150, loss = 0.28208874
Iteration 151, loss = 0.28067477
Iteration 152, loss = 0.27978830
Iteration 153, loss = 0.27933312
Iteration 154, loss = 0.27486527
Iteration 155, loss = 0.27613900
Iteration 156, loss = 0.27523378
Iteration 157, loss = 0.27789026
Iteration 158, loss = 0.28088715
Iteration 159, loss = 0.28118571
Iteration 160, loss = 0.27561281
Iteration 161, loss = 0.27675090
Iteration 162, loss = 0.28146298
Iteration 163, loss = 0.27781296
Iteration 164, loss = 0.27675524
Iteration 165, loss = 0.28080638
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Training time took 124.49916958808899 seconds

Trained a model using MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(120, 120, 120), learning_rate='adaptive',
              learning_rate_init=0.001, max_iter=200, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=123, shuffle=True, solver='adam', tol=0.0001,
              validation_fraction=0.1, verbose=True, warm_start=False)
Train FNC Score:
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    776    |    16     |    76     |    58     |
-------------------------------------------------------------
| disagree  |    26     |    141    |    29     |    23     |
-------------------------------------------------------------
|  discuss  |    121    |    23     |   1868    |    182    |
-------------------------------------------------------------
| unrelated |    116    |    27     |    216    |   8795    |
-------------------------------------------------------------
Score: 5056.5 out of 5627.5	(89.85339848956019%)
Train F1 Score: 0.8175061550379621
Train Accuracy Score: 0.9269190746818218

Test FNC Score:
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    323    |    54     |    832    |    694    |
-------------------------------------------------------------
| disagree  |    162    |     1     |    240    |    294    |
-------------------------------------------------------------
|  discuss  |    882    |    108    |   2085    |   1389    |
-------------------------------------------------------------
| unrelated |    995    |    273    |   1066    |   16015   |
-------------------------------------------------------------
Score: 6982.25 out of 11651.25	(59.927046454243104%)
Test F1 Score: 0.376259097816788
Test Accuracy Score: 0.7249832762759217

Namespace(model='nnet', skip_preprocess=True, train_prop=1)

Training a nnet model

Iteration 1, loss = 0.55492023
Iteration 2, loss = 0.44427097
Iteration 3, loss = 0.41363167
Iteration 4, loss = 0.39410589
Iteration 5, loss = 0.37941627
Iteration 6, loss = 0.36770461
Iteration 7, loss = 0.35718331
Iteration 8, loss = 0.35066793
Iteration 9, loss = 0.34397630
Iteration 10, loss = 0.33734509
Iteration 11, loss = 0.33121306
Iteration 12, loss = 0.32767313
Iteration 13, loss = 0.32325744
Iteration 14, loss = 0.32071715
Iteration 15, loss = 0.31821523
Iteration 16, loss = 0.31416242
Iteration 17, loss = 0.31212322
Iteration 18, loss = 0.30981930
Iteration 19, loss = 0.30616747
Iteration 20, loss = 0.30348622
Iteration 21, loss = 0.30359568
Iteration 22, loss = 0.30102353
Iteration 23, loss = 0.29888949
Iteration 24, loss = 0.29716453
Iteration 25, loss = 0.29547215
Iteration 26, loss = 0.29527249
Iteration 27, loss = 0.29353185
Iteration 28, loss = 0.29294824
Iteration 29, loss = 0.29149211
Iteration 30, loss = 0.28924950
Iteration 31, loss = 0.28906244
Iteration 32, loss = 0.28848651
Iteration 33, loss = 0.28748478
Iteration 34, loss = 0.28689473
Iteration 35, loss = 0.28515157
Iteration 36, loss = 0.28600929
Iteration 37, loss = 0.28448681
Iteration 38, loss = 0.28294069
Iteration 39, loss = 0.28315309
Iteration 40, loss = 0.28350752
Iteration 41, loss = 0.28169916
Iteration 42, loss = 0.28115212
Iteration 43, loss = 0.27988304
Iteration 44, loss = 0.27950140
Iteration 45, loss = 0.27741885
Iteration 46, loss = 0.27856044
Iteration 47, loss = 0.27698590
Iteration 48, loss = 0.27878457
Iteration 49, loss = 0.27733434
Iteration 50, loss = 0.27674047
Iteration 51, loss = 0.27611628
Iteration 52, loss = 0.27667712
Iteration 53, loss = 0.27667299
Iteration 54, loss = 0.27509081
Iteration 55, loss = 0.27551161
Iteration 56, loss = 0.27441301
Iteration 57, loss = 0.27357475
Iteration 58, loss = 0.27309632
Iteration 59, loss = 0.27323763
Iteration 60, loss = 0.27359890
Iteration 61, loss = 0.27271451
Iteration 62, loss = 0.27250061
Iteration 63, loss = 0.27140399
Iteration 64, loss = 0.27224513
Iteration 65, loss = 0.27145642
Iteration 66, loss = 0.27146192
Iteration 67, loss = 0.27097694
Iteration 68, loss = 0.27031676
Iteration 69, loss = 0.27163653
Iteration 70, loss = 0.26931611
Iteration 71, loss = 0.27004446
Iteration 72, loss = 0.26977984
Iteration 73, loss = 0.26949372
Iteration 74, loss = 0.26996382
Iteration 75, loss = 0.26879371
Iteration 76, loss = 0.26920602
Iteration 77, loss = 0.26891261
Iteration 78, loss = 0.26941467
Iteration 79, loss = 0.26931749
Iteration 80, loss = 0.26790725
Iteration 81, loss = 0.26765104
Iteration 82, loss = 0.26846766
Iteration 83, loss = 0.26831777
Iteration 84, loss = 0.26783615
Iteration 85, loss = 0.26777282
Iteration 86, loss = 0.26669186
Iteration 87, loss = 0.26663556
Iteration 88, loss = 0.26753613
Iteration 89, loss = 0.26675477
Iteration 90, loss = 0.26604023
Iteration 91, loss = 0.26617155
Iteration 92, loss = 0.26642021
Iteration 93, loss = 0.26518544
Iteration 94, loss = 0.26665525
Iteration 95, loss = 0.26534657
Iteration 96, loss = 0.26581893
Iteration 97, loss = 0.26529242
Iteration 98, loss = 0.26520185
Iteration 99, loss = 0.26590997
Iteration 100, loss = 0.26447071
Iteration 101, loss = 0.26451121
Iteration 102, loss = 0.26601530
Iteration 103, loss = 0.26399376
Iteration 104, loss = 0.26433640
Iteration 105, loss = 0.26415057
Iteration 106, loss = 0.26435470
Iteration 107, loss = 0.26422543
Iteration 108, loss = 0.26457520
Iteration 109, loss = 0.26373094
Iteration 110, loss = 0.26378452
Iteration 111, loss = 0.26364129
Iteration 112, loss = 0.26411415
Iteration 113, loss = 0.26299684
Iteration 114, loss = 0.26427220
Iteration 115, loss = 0.26344036
Iteration 116, loss = 0.26358884
Iteration 117, loss = 0.26178825
Iteration 118, loss = 0.26252310
Iteration 119, loss = 0.26314573
Iteration 120, loss = 0.26265868
Iteration 121, loss = 0.26273695
Iteration 122, loss = 0.26187753
Iteration 123, loss = 0.26205755
Iteration 124, loss = 0.26240359
Iteration 125, loss = 0.26207340
Iteration 126, loss = 0.26057567
Iteration 127, loss = 0.26153602
Iteration 128, loss = 0.26283326
Iteration 129, loss = 0.26120968
Iteration 130, loss = 0.26194781
Iteration 131, loss = 0.26064260
Iteration 132, loss = 0.26162283
Iteration 133, loss = 0.26107286
Iteration 134, loss = 0.26123214
Iteration 135, loss = 0.26126858
Iteration 136, loss = 0.26053354
Iteration 137, loss = 0.26153349
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Training time took 766.1473081111908 seconds

Trained a model using MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(120, 120, 120), learning_rate='adaptive',
              learning_rate_init=0.001, max_iter=200, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=123, shuffle=True, solver='adam', tol=0.0001,
              validation_fraction=0.1, verbose=True, warm_start=False)
Train FNC Score:
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |   2496    |    88     |    528    |    566    |
-------------------------------------------------------------
| disagree  |    70     |    480    |    137    |    153    |
-------------------------------------------------------------
|  discuss  |    267    |    67     |   7619    |    956    |
-------------------------------------------------------------
| unrelated |    181    |    49     |    950    |   35365   |
-------------------------------------------------------------
Score: 19725.5 out of 22563.25	(87.4231327490499%)
Train F1 Score: 0.7942427684753621
Train Accuracy Score: 0.9197150404226366

Test FNC Score:
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    301    |    15     |    894    |    693    |
-------------------------------------------------------------
| disagree  |    126    |     1     |    252    |    318    |
-------------------------------------------------------------
|  discuss  |    909    |    32     |   2090    |   1433    |
-------------------------------------------------------------
| unrelated |    420    |     6     |   1231    |   16692   |
-------------------------------------------------------------
Score: 7122.0 out of 11651.25	(61.12648857418732%)
Test F1 Score: 0.3814544690441207
Test Accuracy Score: 0.7509542360209341
Plotting learning plots...
Iteration 1, loss = 0.71670228
Iteration 2, loss = 0.50190046
Iteration 3, loss = 0.45613001
Iteration 4, loss = 0.43416312
Iteration 5, loss = 0.41669320
Iteration 6, loss = 0.39897683
Iteration 7, loss = 0.39030302
Iteration 8, loss = 0.37818618
Iteration 9, loss = 0.37051689
Iteration 10, loss = 0.35580336
Iteration 11, loss = 0.35208409
Iteration 12, loss = 0.34535133
Iteration 13, loss = 0.34140898
Iteration 14, loss = 0.33192640
Iteration 15, loss = 0.33010526
Iteration 16, loss = 0.31842151
Iteration 17, loss = 0.31581047
Iteration 18, loss = 0.31054394
Iteration 19, loss = 0.30593579
Iteration 20, loss = 0.29839919
Iteration 21, loss = 0.29664402
Iteration 22, loss = 0.29395393
Iteration 23, loss = 0.29301901
Iteration 24, loss = 0.28751037
Iteration 25, loss = 0.28303189
Iteration 26, loss = 0.28391674
Iteration 27, loss = 0.28022667
Iteration 28, loss = 0.27273767
Iteration 29, loss = 0.27452284
Iteration 30, loss = 0.27208062
Iteration 31, loss = 0.26538485
Iteration 32, loss = 0.26590316
Iteration 33, loss = 0.26314028
Iteration 34, loss = 0.25981198
Iteration 35, loss = 0.25943631
Iteration 36, loss = 0.25646983
Iteration 37, loss = 0.25352015
Iteration 38, loss = 0.25069733
Iteration 39, loss = 0.25052366
Iteration 40, loss = 0.24773987
Iteration 41, loss = 0.24670487
Iteration 42, loss = 0.24513537
Iteration 43, loss = 0.24469674
Iteration 44, loss = 0.24103860
Iteration 45, loss = 0.24101093
Iteration 46, loss = 0.24119082
Iteration 47, loss = 0.23905164
Iteration 48, loss = 0.23671126
Iteration 49, loss = 0.23355784
Iteration 50, loss = 0.23730589
Iteration 51, loss = 0.23264228
Iteration 52, loss = 0.23249285
Iteration 53, loss = 0.23101398
Iteration 54, loss = 0.23125353
Iteration 55, loss = 0.22865657
Iteration 56, loss = 0.22927523
Iteration 57, loss = 0.23087486
Iteration 58, loss = 0.22388651
Iteration 59, loss = 0.22360091
Iteration 60, loss = 0.22859696
Iteration 61, loss = 0.22402414
Iteration 62, loss = 0.22324584
Iteration 63, loss = 0.22330312
Iteration 64, loss = 0.22061503
Iteration 65, loss = 0.22313936
Iteration 66, loss = 0.22164210
Iteration 67, loss = 0.22267838
Iteration 68, loss = 0.21763227
Iteration 69, loss = 0.22080595
Iteration 70, loss = 0.21964038
Iteration 71, loss = 0.21651693
Iteration 72, loss = 0.21320909
Iteration 73, loss = 0.21490960
Iteration 74, loss = 0.21260251
Iteration 75, loss = 0.21395410
Iteration 76, loss = 0.21512107
Iteration 77, loss = 0.21254442
Iteration 78, loss = 0.21040619
Iteration 79, loss = 0.21254984
Iteration 80, loss = 0.21273573
Iteration 81, loss = 0.20744372
Iteration 82, loss = 0.21319562
Iteration 83, loss = 0.21147242
Iteration 84, loss = 0.20380597
Iteration 85, loss = 0.20483571
Iteration 86, loss = 0.20929440
Iteration 87, loss = 0.20363116
Iteration 88, loss = 0.20450433
Iteration 89, loss = 0.20554654
Iteration 90, loss = 0.20912568
Iteration 91, loss = 0.20363192
Iteration 92, loss = 0.20451261
Iteration 93, loss = 0.20645132
Iteration 94, loss = 0.20002115
Iteration 95, loss = 0.20327205
Iteration 96, loss = 0.20331525
Iteration 97, loss = 0.20387433
Iteration 98, loss = 0.19862977
Iteration 99, loss = 0.20086680
Iteration 100, loss = 0.19632530
Iteration 101, loss = 0.20580210
Iteration 102, loss = 0.20459887
Iteration 103, loss = 0.20277366
Iteration 104, loss = 0.20048732
Iteration 105, loss = 0.19884805
Iteration 106, loss = 0.19734968
Iteration 107, loss = 0.19724186
Iteration 108, loss = 0.20316208
Iteration 109, loss = 0.20161819
Iteration 110, loss = 0.19661988
Iteration 111, loss = 0.19960224
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.91477919
Iteration 2, loss = 0.69703531
Iteration 3, loss = 0.57949695
Iteration 4, loss = 0.50922807
Iteration 5, loss = 0.46328796
Iteration 6, loss = 0.43918528
Iteration 7, loss = 0.41821730
Iteration 8, loss = 0.39671547
Iteration 9, loss = 0.38376608
Iteration 10, loss = 0.36900044
Iteration 11, loss = 0.35437793
Iteration 12, loss = 0.34440320
Iteration 13, loss = 0.33312853
Iteration 14, loss = 0.31891323
Iteration 15, loss = 0.31254557
Iteration 16, loss = 0.31206090
Iteration 17, loss = 0.29630620
Iteration 18, loss = 0.29812030
Iteration 19, loss = 0.28444676
Iteration 20, loss = 0.27187415
Iteration 21, loss = 0.26886523
Iteration 22, loss = 0.26378853
Iteration 23, loss = 0.25811646
Iteration 24, loss = 0.25956572
Iteration 25, loss = 0.25264748
Iteration 26, loss = 0.24398184
Iteration 27, loss = 0.24005002
Iteration 28, loss = 0.24020695
Iteration 29, loss = 0.23677817
Iteration 30, loss = 0.23908598
Iteration 31, loss = 0.23259445
Iteration 32, loss = 0.22931133
Iteration 33, loss = 0.22591726
Iteration 34, loss = 0.21374738
Iteration 35, loss = 0.21056414
Iteration 36, loss = 0.20625890
Iteration 37, loss = 0.20398199
Iteration 38, loss = 0.20794595
Iteration 39, loss = 0.20186824
Iteration 40, loss = 0.20713216
Iteration 41, loss = 0.19773629
Iteration 42, loss = 0.19678318
Iteration 43, loss = 0.19616673
Iteration 44, loss = 0.19164098
Iteration 45, loss = 0.18991904
Iteration 46, loss = 0.18903370
Iteration 47, loss = 0.18954825
Iteration 48, loss = 0.19186114
Iteration 49, loss = 0.18359801
Iteration 50, loss = 0.18238052
Iteration 51, loss = 0.18165551
Iteration 52, loss = 0.17938603
Iteration 53, loss = 0.17870165
Iteration 54, loss = 0.17668728
Iteration 55, loss = 0.17527307
Iteration 56, loss = 0.17399689
Iteration 57, loss = 0.17589134
Iteration 58, loss = 0.17849734
Iteration 59, loss = 0.17362688
Iteration 60, loss = 0.17124105
Iteration 61, loss = 0.16875975
Iteration 62, loss = 0.17163217
Iteration 63, loss = 0.16964994
Iteration 64, loss = 0.16668424
Iteration 65, loss = 0.16675057
Iteration 66, loss = 0.17035110
Iteration 67, loss = 0.16698315
Iteration 68, loss = 0.15995128
Iteration 69, loss = 0.16287301
Iteration 70, loss = 0.16380379
Iteration 71, loss = 0.15910811
Iteration 72, loss = 0.15996938
Iteration 73, loss = 0.16189314
Iteration 74, loss = 0.16139345
Iteration 75, loss = 0.15717135
Iteration 76, loss = 0.15569131
Iteration 77, loss = 0.16433773
Iteration 78, loss = 0.17637179
Iteration 79, loss = 0.17432106
Iteration 80, loss = 0.16834400
Iteration 81, loss = 0.15785156
Iteration 82, loss = 0.15729316
Iteration 83, loss = 0.14983792
Iteration 84, loss = 0.15167256
Iteration 85, loss = 0.15123915
Iteration 86, loss = 0.15625449
Iteration 87, loss = 0.15537357
Iteration 88, loss = 0.15055219
Iteration 89, loss = 0.15056184
Iteration 90, loss = 0.15381656
Iteration 91, loss = 0.14941531
Iteration 92, loss = 0.15280287
Iteration 93, loss = 0.14508834
Iteration 94, loss = 0.14538286
Iteration 95, loss = 0.14519725
Iteration 96, loss = 0.15261154
Iteration 97, loss = 0.15432010
Iteration 98, loss = 0.15518114
Iteration 99, loss = 0.14956065
Iteration 100, loss = 0.14287542
Iteration 101, loss = 0.14770725
Iteration 102, loss = 0.15357928
Iteration 103, loss = 0.15382038
Iteration 104, loss = 0.15428981
Iteration 105, loss = 0.14086557
Iteration 106, loss = 0.13907089
Iteration 107, loss = 0.14451675
Iteration 108, loss = 0.14271697
Iteration 109, loss = 0.14939001
Iteration 110, loss = 0.14164646
Iteration 111, loss = 0.14207605
Iteration 112, loss = 0.13916286
Iteration 113, loss = 0.13826109
Iteration 114, loss = 0.13564532
Iteration 115, loss = 0.13707314
Iteration 116, loss = 0.13858151
Iteration 117, loss = 0.13712935
Iteration 118, loss = 0.14351292
Iteration 119, loss = 0.14463139
Iteration 120, loss = 0.13856093
Iteration 121, loss = 0.13738787
Iteration 122, loss = 0.14001193
Iteration 123, loss = 0.13787302
Iteration 124, loss = 0.13657888
Iteration 125, loss = 0.14665306
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73499297
Iteration 2, loss = 0.51581944
Iteration 3, loss = 0.47122558
Iteration 4, loss = 0.44870911
Iteration 5, loss = 0.42649110
Iteration 6, loss = 0.41232267
Iteration 7, loss = 0.40171243
Iteration 8, loss = 0.38990782
Iteration 9, loss = 0.37944132
Iteration 10, loss = 0.37047062
Iteration 11, loss = 0.35999931
Iteration 12, loss = 0.35304484
Iteration 13, loss = 0.34895550
Iteration 14, loss = 0.34240465
Iteration 15, loss = 0.33550257Iteration 1, loss = 0.89062668
Iteration 2, loss = 0.68865489
Iteration 3, loss = 0.57880614
Iteration 4, loss = 0.50593152
Iteration 5, loss = 0.46414251
Iteration 6, loss = 0.43912634
Iteration 7, loss = 0.41507698
Iteration 8, loss = 0.39605991
Iteration 9, loss = 0.38195941
Iteration 10, loss = 0.36784425
Iteration 11, loss = 0.35447070
Iteration 12, loss = 0.34613713
Iteration 13, loss = 0.33377047
Iteration 14, loss = 0.32269692
Iteration 15, loss = 0.31340259
Iteration 16, loss = 0.30772210
Iteration 17, loss = 0.30643781
Iteration 18, loss = 0.29528130
Iteration 19, loss = 0.29201863
Iteration 20, loss = 0.28084231
Iteration 21, loss = 0.27856233
Iteration 22, loss = 0.26738644
Iteration 23, loss = 0.26331319
Iteration 24, loss = 0.25425777
Iteration 25, loss = 0.25198552
Iteration 26, loss = 0.24930138
Iteration 27, loss = 0.24660533
Iteration 28, loss = 0.24518784
Iteration 29, loss = 0.24667986
Iteration 30, loss = 0.23670946
Iteration 31, loss = 0.22995493
Iteration 32, loss = 0.22782122
Iteration 33, loss = 0.22167593
Iteration 34, loss = 0.22252675
Iteration 35, loss = 0.21416645
Iteration 36, loss = 0.21507602
Iteration 37, loss = 0.21717567
Iteration 38, loss = 0.21097418
Iteration 39, loss = 0.21286586
Iteration 40, loss = 0.20576351
Iteration 41, loss = 0.20302161
Iteration 42, loss = 0.20432022
Iteration 43, loss = 0.20457991
Iteration 44, loss = 0.20673160
Iteration 45, loss = 0.20705639
Iteration 46, loss = 0.20037852
Iteration 47, loss = 0.18909714
Iteration 48, loss = 0.20375419
Iteration 49, loss = 0.19847754
Iteration 50, loss = 0.19302549
Iteration 51, loss = 0.18907625
Iteration 52, loss = 0.18864359
Iteration 53, loss = 0.18614993
Iteration 54, loss = 0.18290588
Iteration 55, loss = 0.18443183
Iteration 56, loss = 0.18893214
Iteration 57, loss = 0.18418770
Iteration 58, loss = 0.18151277
Iteration 59, loss = 0.18402338
Iteration 60, loss = 0.18235055
Iteration 61, loss = 0.18182755
Iteration 62, loss = 0.18018676
Iteration 63, loss = 0.17316003
Iteration 64, loss = 0.17700848
Iteration 65, loss = 0.17331123
Iteration 66, loss = 0.17851255
Iteration 67, loss = 0.17522373
Iteration 68, loss = 0.17562219
Iteration 69, loss = 0.17554864
Iteration 70, loss = 0.17248814
Iteration 71, loss = 0.16741707
Iteration 72, loss = 0.17054128
Iteration 73, loss = 0.16923834
Iteration 74, loss = 0.16710554
Iteration 75, loss = 0.16902109
Iteration 76, loss = 0.17263576
Iteration 77, loss = 0.17293777
Iteration 78, loss = 0.17762437
Iteration 79, loss = 0.17041477
Iteration 80, loss = 0.16918425
Iteration 81, loss = 0.16483363
Iteration 82, loss = 0.16490873
Iteration 83, loss = 0.16152393
Iteration 84, loss = 0.16386469
Iteration 85, loss = 0.17098679
Iteration 86, loss = 0.16418568
Iteration 87, loss = 0.16242178
Iteration 88, loss = 0.16192341
Iteration 89, loss = 0.15750097
Iteration 90, loss = 0.15756894
Iteration 91, loss = 0.15314135
Iteration 92, loss = 0.15253051
Iteration 93, loss = 0.16200719
Iteration 94, loss = 0.15686524
Iteration 95, loss = 0.15857950
Iteration 96, loss = 0.15212718
Iteration 97, loss = 0.15407583
Iteration 98, loss = 0.15348476
Iteration 99, loss = 0.15945502
Iteration 100, loss = 0.15451450
Iteration 101, loss = 0.15202157
Iteration 102, loss = 0.14785026
Iteration 103, loss = 0.15082676
Iteration 104, loss = 0.15097902
Iteration 105, loss = 0.14864115
Iteration 106, loss = 0.15206952
Iteration 107, loss = 0.14633622
Iteration 108, loss = 0.14709536
Iteration 109, loss = 0.14590059
Iteration 110, loss = 0.14557205
Iteration 111, loss = 0.14826869
Iteration 112, loss = 0.14783958
Iteration 113, loss = 0.14559620
Iteration 114, loss = 0.14823568
Iteration 115, loss = 0.14708532
Iteration 116, loss = 0.14800583
Iteration 117, loss = 0.14401014
Iteration 118, loss = 0.14372459
Iteration 119, loss = 0.14410714
Iteration 120, loss = 0.14074071
Iteration 121, loss = 0.14331745
Iteration 122, loss = 0.14008984
Iteration 123, loss = 0.14428464
Iteration 124, loss = 0.14701723
Iteration 125, loss = 0.14745921
Iteration 126, loss = 0.14128728
Iteration 127, loss = 0.14069404
Iteration 128, loss = 0.14211873
Iteration 129, loss = 0.14057976
Iteration 130, loss = 0.13765791
Iteration 131, loss = 0.13926273
Iteration 132, loss = 0.14053352
Iteration 133, loss = 0.14369935
Iteration 134, loss = 0.14546523
Iteration 135, loss = 0.13878357
Iteration 136, loss = 0.13503067
Iteration 137, loss = 0.13636463
Iteration 138, loss = 0.13483879
Iteration 139, loss = 0.13502037
Iteration 140, loss = 0.13837119
Iteration 141, loss = 0.14068062
Iteration 142, loss = 0.13820486
Iteration 143, loss = 0.13339523
Iteration 144, loss = 0.13370565
Iteration 145, loss = 0.13462993
Iteration 146, loss = 0.13802052
Iteration 147, loss = 0.13721346
Iteration 148, loss = 0.13761994
Iteration 149, loss = 0.13557853
Iteration 150, loss = 0.13308510
Iteration 151, loss = 0.13524961
Iteration 152, loss = 0.13309680
Iteration 153, loss = 0.13120686
Iteration 154, loss = 0.13733496
Iteration 155, loss = 0.13549148
Iteration 156, loss = 0.13646009
Iteration 157, loss = 0.14101480
Iteration 158, loss = 0.13968161
Iteration 159, loss = 0.14630744
Iteration 160, loss = 0.13955534
Iteration 161, loss = 0.13780765
Iteration 162, loss = 0.13598964
Iteration 163, loss = 0.12779370
Iteration 164, loss = 0.12417894
Iteration 165, loss = 0.12135908
Iteration 166, loss = 0.12448016
Iteration 167, loss = 0.12748922
Iteration 168, loss = 0.12785629
Iteration 169, loss = 0.12685593
Iteration 170, loss = 0.12261054
Iteration 171, loss = 0.12777252
Iteration 172, loss = 0.13255394
Iteration 173, loss = 0.13286458
Iteration 174, loss = 0.13307461
Iteration 175, loss = 0.12978411
Iteration 176, loss = 0.12491568
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.58487708
Iteration 2, loss = 0.45588491
Iteration 3, loss = 0.42419867
Iteration 4, loss = 0.40196877
Iteration 5, loss = 0.38752911
Iteration 6, loss = 0.37523002
Iteration 7, loss = 0.36449330
Iteration 8, loss = 0.35578697
Iteration 9, loss = 0.34975540
Iteration 10, loss = 0.34215362
Iteration 11, loss = 0.33743789
Iteration 12, loss = 0.33071477
Iteration 13, loss = 0.32941740
Iteration 14, loss = 0.32586834
Iteration 15, loss = 0.31970412
Iteration 16, loss = 0.31704760
Iteration 17, loss = 0.31410705
Iteration 18, loss = 0.30944754
Iteration 19, loss = 0.30850870
Iteration 20, loss = 0.30783679
Iteration 21, loss = 0.30328048
Iteration 22, loss = 0.30126940
Iteration 23, loss = 0.29885843
Iteration 24, loss = 0.29753664
Iteration 25, loss = 0.29996505
Iteration 26, loss = 0.29514983
Iteration 27, loss = 0.29341006
Iteration 28, loss = 0.29231869
Iteration 29, loss = 0.29124821
Iteration 30, loss = 0.29011975
Iteration 31, loss = 0.28877261
Iteration 32, loss = 0.28698400
Iteration 33, loss = 0.28648926
Iteration 34, loss = 0.28707171
Iteration 35, loss = 0.28353285
Iteration 36, loss = 0.28362216
Iteration 37, loss = 0.28285794
Iteration 38, loss = 0.28282931
Iteration 39, loss = 0.28205405
Iteration 40, loss = 0.28028703
Iteration 41, loss = 0.28041290
Iteration 42, loss = 0.27890183
Iteration 43, loss = 0.28065378
Iteration 44, loss = 0.27782408
Iteration 45, loss = 0.27883355
Iteration 46, loss = 0.27606389
Iteration 47, loss = 0.27734787
Iteration 48, loss = 0.27656098
Iteration 49, loss = 0.27510823
Iteration 50, loss = 0.27516491
Iteration 51, loss = 0.27315776
Iteration 52, loss = 0.27382566
Iteration 53, loss = 0.27216748
Iteration 54, loss = 0.27162368
Iteration 55, loss = 0.27391558
Iteration 56, loss = 0.27219215
Iteration 57, loss = 0.27138619
Iteration 58, loss = 0.27178381
Iteration 59, loss = 0.27112663
Iteration 60, loss = 0.26972779
Iteration 61, loss = 0.26974895
Iteration 62, loss = 0.26903921
Iteration 63, loss = 0.26960001
Iteration 64, loss = 0.26927864
Iteration 65, loss = 0.26800054
Iteration 66, loss = 0.26794840
Iteration 67, loss = 0.26709742
Iteration 68, loss = 0.26655677
Iteration 69, loss = 0.26733618
Iteration 70, loss = 0.26640105
Iteration 71, loss = 0.26588928
Iteration 72, loss = 0.26497717
Iteration 73, loss = 0.26498880
Iteration 74, loss = 0.26483190
Iteration 75, loss = 0.26604193
Iteration 76, loss = 0.26494046
Iteration 16, loss = 0.33092098
Iteration 17, loss = 0.32471797
Iteration 18, loss = 0.32015169
Iteration 19, loss = 0.31922469
Iteration 20, loss = 0.31430771
Iteration 21, loss = 0.30827649
Iteration 22, loss = 0.30528481
Iteration 23, loss = 0.29962936
Iteration 24, loss = 0.29870652
Iteration 25, loss = 0.29538942
Iteration 26, loss = 0.29065054
Iteration 27, loss = 0.28782272
Iteration 28, loss = 0.28538873
Iteration 29, loss = 0.28528367
Iteration 30, loss = 0.28361828
Iteration 31, loss = 0.28103713
Iteration 32, loss = 0.27784828
Iteration 33, loss = 0.27692865
Iteration 34, loss = 0.27173803
Iteration 35, loss = 0.26895498
Iteration 36, loss = 0.27481026
Iteration 37, loss = 0.26716603
Iteration 38, loss = 0.26612708
Iteration 39, loss = 0.26514525
Iteration 40, loss = 0.26019867
Iteration 41, loss = 0.26273306
Iteration 42, loss = 0.25905886
Iteration 43, loss = 0.25993169
Iteration 44, loss = 0.25705828
Iteration 45, loss = 0.25423603
Iteration 46, loss = 0.25329315
Iteration 47, loss = 0.25105387
Iteration 48, loss = 0.25428512
Iteration 49, loss = 0.24655462
Iteration 50, loss = 0.24322858
Iteration 51, loss = 0.24262908
Iteration 52, loss = 0.24312162
Iteration 53, loss = 0.24417758
Iteration 54, loss = 0.23908243
Iteration 55, loss = 0.24196319
Iteration 56, loss = 0.24285958
Iteration 57, loss = 0.24113970
Iteration 58, loss = 0.23806581
Iteration 59, loss = 0.23469387
Iteration 60, loss = 0.23354280
Iteration 61, loss = 0.23035175
Iteration 62, loss = 0.23921494
Iteration 63, loss = 0.23461854
Iteration 64, loss = 0.23062719
Iteration 65, loss = 0.23051981
Iteration 66, loss = 0.22875908
Iteration 67, loss = 0.23201173
Iteration 68, loss = 0.22589269
Iteration 69, loss = 0.22886387
Iteration 70, loss = 0.22615784
Iteration 71, loss = 0.22363521
Iteration 72, loss = 0.22412946
Iteration 73, loss = 0.22161323
Iteration 74, loss = 0.22047717
Iteration 75, loss = 0.22328102
Iteration 76, loss = 0.22259084
Iteration 77, loss = 0.22274933
Iteration 78, loss = 0.22172983
Iteration 79, loss = 0.22256788
Iteration 80, loss = 0.21854630
Iteration 81, loss = 0.22229435
Iteration 82, loss = 0.22109530
Iteration 83, loss = 0.21482216
Iteration 84, loss = 0.21641119
Iteration 85, loss = 0.21604727
Iteration 86, loss = 0.21721421
Iteration 87, loss = 0.21583065
Iteration 88, loss = 0.21304032
Iteration 89, loss = 0.21831444
Iteration 90, loss = 0.21443300
Iteration 91, loss = 0.21118603
Iteration 92, loss = 0.21134291
Iteration 93, loss = 0.21318228
Iteration 94, loss = 0.20863931
Iteration 95, loss = 0.21015569
Iteration 96, loss = 0.21005337
Iteration 97, loss = 0.21335093
Iteration 98, loss = 0.21333686
Iteration 99, loss = 0.21083315
Iteration 100, loss = 0.21122432
Iteration 101, loss = 0.20789137
Iteration 102, loss = 0.20977394
Iteration 103, loss = 0.20515237
Iteration 104, loss = 0.20698963
Iteration 105, loss = 0.20661754
Iteration 106, loss = 0.20730005
Iteration 107, loss = 0.20548977
Iteration 108, loss = 0.20571297
Iteration 109, loss = 0.20537906
Iteration 110, loss = 0.20703819
Iteration 111, loss = 0.20364019
Iteration 112, loss = 0.20683830
Iteration 113, loss = 0.20717953
Iteration 114, loss = 0.21499012
Iteration 115, loss = 0.20669915
Iteration 116, loss = 0.20587954
Iteration 117, loss = 0.20053583
Iteration 118, loss = 0.20275302
Iteration 119, loss = 0.20494194
Iteration 120, loss = 0.19855123
Iteration 121, loss = 0.19926168
Iteration 122, loss = 0.19931594
Iteration 123, loss = 0.20099477
Iteration 124, loss = 0.20021793
Iteration 125, loss = 0.19686419
Iteration 126, loss = 0.19829243
Iteration 127, loss = 0.20070589
Iteration 128, loss = 0.20191408
Iteration 129, loss = 0.20121915
Iteration 130, loss = 0.20110655
Iteration 131, loss = 0.19700888
Iteration 132, loss = 0.19644523
Iteration 133, loss = 0.20017268
Iteration 134, loss = 0.19956575
Iteration 135, loss = 0.19626780
Iteration 136, loss = 0.19811468
Iteration 137, loss = 0.19913510
Iteration 138, loss = 0.19714920
Iteration 139, loss = 0.19632436
Iteration 140, loss = 0.19492908
Iteration 141, loss = 0.19661502
Iteration 142, loss = 0.19640424
Iteration 143, loss = 0.20012836
Iteration 144, loss = 0.19652430
Iteration 145, loss = 0.19834444
Iteration 146, loss = 0.19440452
Iteration 147, loss = 0.19642355
Iteration 148, loss = 0.19384057
Iteration 149, loss = 0.19458118
Iteration 150, loss = 0.19820932
Iteration 151, loss = 0.19580854
Iteration 152, loss = 0.20113309
Iteration 153, loss = 0.19671230
Iteration 154, loss = 0.19522483
Iteration 155, loss = 0.19098250
Iteration 156, loss = 0.19143558
Iteration 157, loss = 0.19516519
Iteration 158, loss = 0.19372559
Iteration 159, loss = 0.19039821
Iteration 160, loss = 0.19397792
Iteration 161, loss = 0.19283089
Iteration 162, loss = 0.19129312
Iteration 163, loss = 0.19319935
Iteration 164, loss = 0.19193321
Iteration 165, loss = 0.19073206
Iteration 166, loss = 0.19225694
Iteration 167, loss = 0.19028046
Iteration 168, loss = 0.18954370
Iteration 169, loss = 0.18840327
Iteration 170, loss = 0.18888564
Iteration 171, loss = 0.19007127
Iteration 172, loss = 0.18994334
Iteration 173, loss = 0.19147219
Iteration 174, loss = 0.19049624
Iteration 175, loss = 0.19092173
Iteration 176, loss = 0.18944289
Iteration 177, loss = 0.18814972
Iteration 178, loss = 0.19003542
Iteration 179, loss = 0.18782001
Iteration 180, loss = 0.18866357
Iteration 181, loss = 0.20432340
Iteration 182, loss = 0.18829177
Iteration 183, loss = 0.19064218
Iteration 184, loss = 0.19027260
Iteration 185, loss = 0.19218817
Iteration 186, loss = 0.18728634
Iteration 187, loss = 0.18892614
Iteration 188, loss = 0.18746844
Iteration 189, loss = 0.18653906
Iteration 190, loss = 0.18887291
Iteration 191, loss = 0.18745608
Iteration 192, loss = 0.18467339
Iteration 193, loss = 0.18801233
Iteration 194, loss = 0.18637231
Iteration 195, loss = 0.18465973
Iteration 196, loss = 0.19211096
Iteration 197, loss = 0.18682792
Iteration 198, loss = 0.18738275
Iteration 199, loss = 0.18745100
Iteration 200, loss = 0.18672483
Iteration 1, loss = 0.60631361
Iteration 2, loss = 0.46610747
Iteration 3, loss = 0.43430102
Iteration 4, loss = 0.41327813
Iteration 5, loss = 0.39551889
Iteration 6, loss = 0.38301008
Iteration 7, loss = 0.37680423
Iteration 8, loss = 0.36631950
Iteration 9, loss = 0.35674223
Iteration 10, loss = 0.35090898
Iteration 11, loss = 0.34388235
Iteration 12, loss = 0.34007360
Iteration 13, loss = 0.33687868
Iteration 14, loss = 0.32891545
Iteration 15, loss = 0.32657490
Iteration 16, loss = 0.32107117
Iteration 17, loss = 0.32012221
Iteration 18, loss = 0.31506115
Iteration 19, loss = 0.31399851
Iteration 20, loss = 0.31137428
Iteration 21, loss = 0.30681579
Iteration 22, loss = 0.30508242
Iteration 23, loss = 0.30406646
Iteration 24, loss = 0.30215355
Iteration 25, loss = 0.30061118
Iteration 26, loss = 0.29644745
Iteration 27, loss = 0.29631839
Iteration 28, loss = 0.29534580
Iteration 29, loss = 0.29157253
Iteration 30, loss = 0.29030691
Iteration 31, loss = 0.28850982
Iteration 32, loss = 0.28903989
Iteration 33, loss = 0.28565478
Iteration 34, loss = 0.28455061
Iteration 35, loss = 0.28598381
Iteration 36, loss = 0.28259327
Iteration 37, loss = 0.28298305
Iteration 38, loss = 0.27953260
Iteration 39, loss = 0.28026588
Iteration 40, loss = 0.28081038
Iteration 41, loss = 0.27940609
Iteration 42, loss = 0.27802175
Iteration 43, loss = 0.27740057
Iteration 44, loss = 0.27409268
Iteration 45, loss = 0.27459674
Iteration 46, loss = 0.27532138
Iteration 47, loss = 0.27374033
Iteration 48, loss = 0.27299241
Iteration 49, loss = 0.27229832
Iteration 50, loss = 0.27027587
Iteration 51, loss = 0.27270933
Iteration 52, loss = 0.27025538
Iteration 53, loss = 0.27028950
Iteration 54, loss = 0.26882643
Iteration 55, loss = 0.26954504
Iteration 56, loss = 0.26598702
Iteration 57, loss = 0.26736334
Iteration 58, loss = 0.26876007
Iteration 59, loss = 0.26627503
Iteration 60, loss = 0.26593134
Iteration 61, loss = 0.26543662
Iteration 62, loss = 0.26379360
Iteration 63, loss = 0.26436131
Iteration 64, loss = 0.26578707
Iteration 65, loss = 0.26267443
Iteration 66, loss = 0.26292066
Iteration 67, loss = 0.26493047
Iteration 68, loss = 0.26092541
Iteration 69, loss = 0.26236936Iteration 1, loss = 0.63625595
Iteration 2, loss = 0.47522226
Iteration 3, loss = 0.44114691
Iteration 4, loss = 0.42322391
Iteration 5, loss = 0.40447369
Iteration 6, loss = 0.38841819
Iteration 7, loss = 0.37781244
Iteration 8, loss = 0.37054605
Iteration 9, loss = 0.36117832
Iteration 10, loss = 0.35177844
Iteration 11, loss = 0.34386089
Iteration 12, loss = 0.33877003
Iteration 13, loss = 0.33418321
Iteration 14, loss = 0.33163831
Iteration 15, loss = 0.32149875
Iteration 16, loss = 0.32104930
Iteration 17, loss = 0.31670292
Iteration 18, loss = 0.31320677
Iteration 19, loss = 0.30932206
Iteration 20, loss = 0.30397815
Iteration 21, loss = 0.30436668
Iteration 22, loss = 0.29746245
Iteration 23, loss = 0.29567700
Iteration 24, loss = 0.29844398
Iteration 25, loss = 0.29028957
Iteration 26, loss = 0.28938454
Iteration 27, loss = 0.28753148
Iteration 28, loss = 0.28493098
Iteration 29, loss = 0.28289243
Iteration 30, loss = 0.28350516
Iteration 31, loss = 0.27802605
Iteration 32, loss = 0.27878803
Iteration 33, loss = 0.27669822
Iteration 34, loss = 0.27442258
Iteration 35, loss = 0.27501892
Iteration 36, loss = 0.27262740
Iteration 37, loss = 0.27071744
Iteration 38, loss = 0.27114907
Iteration 39, loss = 0.26749249
Iteration 40, loss = 0.26786419
Iteration 41, loss = 0.26737633
Iteration 42, loss = 0.26672867
Iteration 43, loss = 0.26712109
Iteration 44, loss = 0.26121964
Iteration 45, loss = 0.26206816
Iteration 46, loss = 0.26244550
Iteration 47, loss = 0.26076989
Iteration 48, loss = 0.26037605
Iteration 49, loss = 0.25828209
Iteration 50, loss = 0.25657098
Iteration 51, loss = 0.25722624
Iteration 52, loss = 0.26113612
Iteration 53, loss = 0.25846366
Iteration 54, loss = 0.25751878
Iteration 55, loss = 0.25291397
Iteration 56, loss = 0.25408925
Iteration 57, loss = 0.25070341
Iteration 58, loss = 0.25203630
Iteration 59, loss = 0.25103105
Iteration 60, loss = 0.25145941
Iteration 61, loss = 0.24962035
Iteration 62, loss = 0.24889738
Iteration 63, loss = 0.24893401
Iteration 64, loss = 0.24693521
Iteration 65, loss = 0.24814737
Iteration 66, loss = 0.24603186
Iteration 67, loss = 0.24453029
Iteration 68, loss = 0.24564747
Iteration 69, loss = 0.24641225
Iteration 70, loss = 0.24769523
Iteration 71, loss = 0.24584783
Iteration 72, loss = 0.24372946
Iteration 73, loss = 0.24523307
Iteration 74, loss = 0.24156348
Iteration 75, loss = 0.24069051
Iteration 76, loss = 0.24324935
Iteration 77, loss = 0.24220806
Iteration 78, loss = 0.24308226
Iteration 79, loss = 0.23944209
Iteration 80, loss = 0.24138411
Iteration 81, loss = 0.24032094
Iteration 82, loss = 0.23919226
Iteration 83, loss = 0.23936780
Iteration 84, loss = 0.23985348
Iteration 85, loss = 0.24037744
Iteration 86, loss = 0.23983929
Iteration 87, loss = 0.24012582
Iteration 88, loss = 0.23823679
Iteration 89, loss = 0.23619960
Iteration 90, loss = 0.23883479
Iteration 91, loss = 0.23523855
Iteration 92, loss = 0.23730221
Iteration 93, loss = 0.23663388
Iteration 94, loss = 0.23541148
Iteration 95, loss = 0.23588395
Iteration 96, loss = 0.23569402
Iteration 97, loss = 0.23424309
Iteration 98, loss = 0.23336397
Iteration 99, loss = 0.23755905
Iteration 100, loss = 0.23371361
Iteration 101, loss = 0.23300418
Iteration 102, loss = 0.23541113
Iteration 103, loss = 0.23356598
Iteration 104, loss = 0.23434935
Iteration 105, loss = 0.23431290
Iteration 106, loss = 0.23268134
Iteration 107, loss = 0.23286422
Iteration 108, loss = 0.23190761
Iteration 109, loss = 0.23005705
Iteration 110, loss = 0.23149457
Iteration 111, loss = 0.23032612
Iteration 112, loss = 0.23193093
Iteration 113, loss = 0.23055419
Iteration 114, loss = 0.22789324
Iteration 115, loss = 0.23047552
Iteration 116, loss = 0.23016961
Iteration 117, loss = 0.22857997
Iteration 118, loss = 0.23267342
Iteration 119, loss = 0.23004539
Iteration 120, loss = 0.22714460
Iteration 121, loss = 0.23086031
Iteration 122, loss = 0.22908319
Iteration 123, loss = 0.22804754
Iteration 124, loss = 0.22996722
Iteration 125, loss = 0.22815772
Iteration 126, loss = 0.22759737
Iteration 127, loss = 0.22731455
Iteration 128, loss = 0.22696051
Iteration 129, loss = 0.22650026
Iteration 130, loss = 0.22616509
Iteration 131, loss = 0.22476763
Iteration 132, loss = 0.22702285
Iteration 133, loss = 0.22682729
Iteration 134, loss = 0.22629353
Iteration 135, loss = 0.22483863
Iteration 136, loss = 0.22761834
Iteration 137, loss = 0.22563255
Iteration 138, loss = 0.22480640
Iteration 139, loss = 0.22421741
Iteration 140, loss = 0.22604830
Iteration 141, loss = 0.22532720
Iteration 142, loss = 0.22613640
Iteration 143, loss = 0.22582124
Iteration 144, loss = 0.22514319
Iteration 145, loss = 0.22317214
Iteration 146, loss = 0.22582337
Iteration 147, loss = 0.22762362
Iteration 148, loss = 0.22503895
Iteration 149, loss = 0.22466249
Iteration 150, loss = 0.22427752
Iteration 151, loss = 0.22376946
Iteration 152, loss = 0.22232086
Iteration 153, loss = 0.22394141
Iteration 154, loss = 0.22387258
Iteration 155, loss = 0.22405819
Iteration 156, loss = 0.22246578
Iteration 157, loss = 0.22184370
Iteration 158, loss = 0.22050096
Iteration 159, loss = 0.22459343
Iteration 160, loss = 0.22296975
Iteration 161, loss = 0.22256012
Iteration 162, loss = 0.22187810
Iteration 163, loss = 0.21964056
Iteration 164, loss = 0.22047195
Iteration 165, loss = 0.22388719
Iteration 166, loss = 0.22240472
Iteration 167, loss = 0.22061021
Iteration 168, loss = 0.21919114
Iteration 169, loss = 0.22241220
Iteration 170, loss = 0.22109965
Iteration 171, loss = 0.22346314
Iteration 172, loss = 0.22232710
Iteration 173, loss = 0.21807113
Iteration 174, loss = 0.22104117
Iteration 175, loss = 0.22018786
Iteration 176, loss = 0.21951942
Iteration 177, loss = 0.21880062
Iteration 178, loss = 0.22045975
Iteration 179, loss = 0.22088689
Iteration 180, loss = 0.22031571
Iteration 181, loss = 0.21926525
Iteration 182, loss = 0.22018574
Iteration 183, loss = 0.21740214
Iteration 184, loss = 0.21960884
Iteration 185, loss = 0.21824895
Iteration 186, loss = 0.22339630
Iteration 187, loss = 0.21930320
Iteration 188, loss = 0.21899812
Iteration 189, loss = 0.22147075
Iteration 190, loss = 0.22027160
Iteration 191, loss = 0.22186544
Iteration 192, loss = 0.21895405
Iteration 193, loss = 0.21904826
Iteration 194, loss = 0.21860151
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.57839443
Iteration 2, loss = 0.45299335
Iteration 3, loss = 0.42171920
Iteration 4, loss = 0.40214725
Iteration 5, loss = 0.38711197
Iteration 6, loss = 0.37629280
Iteration 7, loss = 0.36655962
Iteration 8, loss = 0.35916409
Iteration 9, loss = 0.35169597
Iteration 10, loss = 0.34387696
Iteration 11, loss = 0.33880655
Iteration 12, loss = 0.33378131
Iteration 13, loss = 0.32972310
Iteration 14, loss = 0.32710481
Iteration 15, loss = 0.32158469
Iteration 16, loss = 0.31762826
Iteration 17, loss = 0.31566775
Iteration 18, loss = 0.31177050
Iteration 19, loss = 0.31032573
Iteration 20, loss = 0.30744950
Iteration 21, loss = 0.30504084
Iteration 22, loss = 0.30273603
Iteration 23, loss = 0.30306971
Iteration 24, loss = 0.29860859
Iteration 25, loss = 0.29724779
Iteration 26, loss = 0.29680670
Iteration 27, loss = 0.29287669
Iteration 28, loss = 0.29283634
Iteration 29, loss = 0.29218098
Iteration 30, loss = 0.29143988
Iteration 31, loss = 0.29061033
Iteration 32, loss = 0.29137788
Iteration 33, loss = 0.28742930
Iteration 34, loss = 0.28663700
Iteration 35, loss = 0.28604599
Iteration 36, loss = 0.28450517
Iteration 37, loss = 0.28473657
Iteration 38, loss = 0.28255470
Iteration 39, loss = 0.28262583
Iteration 40, loss = 0.28092343
Iteration 41, loss = 0.28018811
Iteration 42, loss = 0.28142820
Iteration 43, loss = 0.28023008
Iteration 44, loss = 0.27756393
Iteration 45, loss = 0.27908742
Iteration 46, loss = 0.27681103
Iteration 47, loss = 0.27792297
Iteration 48, loss = 0.27603711
Iteration 49, loss = 0.27777521
Iteration 50, loss = 0.27440762
Iteration 51, loss = 0.27364501
Iteration 52, loss = 0.27474848
Iteration 53, loss = 0.27239223
Iteration 54, loss = 0.27323112
Iteration 55, loss = 0.27260189
Iteration 56, loss = 0.27259729
Iteration 57, loss = 0.27159175Iteration 1, loss = 0.60090692
Iteration 2, loss = 0.46381631
Iteration 3, loss = 0.43250989
Iteration 4, loss = 0.40994799
Iteration 5, loss = 0.39597606
Iteration 6, loss = 0.38200032
Iteration 7, loss = 0.37327762
Iteration 8, loss = 0.36390764
Iteration 9, loss = 0.35536073
Iteration 10, loss = 0.34994215
Iteration 11, loss = 0.34482218
Iteration 12, loss = 0.33722437
Iteration 13, loss = 0.33500121
Iteration 14, loss = 0.33002935
Iteration 15, loss = 0.32297033
Iteration 16, loss = 0.32280930
Iteration 17, loss = 0.32110320
Iteration 18, loss = 0.31320955
Iteration 19, loss = 0.31198357
Iteration 20, loss = 0.30807506
Iteration 21, loss = 0.30851497
Iteration 22, loss = 0.30476929
Iteration 23, loss = 0.30100468
Iteration 24, loss = 0.30064262
Iteration 25, loss = 0.29825669
Iteration 26, loss = 0.29787766
Iteration 27, loss = 0.29586580
Iteration 28, loss = 0.29434434
Iteration 29, loss = 0.29134457
Iteration 30, loss = 0.28866001
Iteration 31, loss = 0.28838860
Iteration 32, loss = 0.28664695
Iteration 33, loss = 0.28551980
Iteration 34, loss = 0.28346364
Iteration 35, loss = 0.28324595
Iteration 36, loss = 0.28122293
Iteration 37, loss = 0.28020988
Iteration 38, loss = 0.27974590
Iteration 39, loss = 0.27746141
Iteration 40, loss = 0.27779178
Iteration 41, loss = 0.27949669
Iteration 42, loss = 0.27672372
Iteration 43, loss = 0.27509498
Iteration 44, loss = 0.27635803
Iteration 45, loss = 0.27460767
Iteration 46, loss = 0.27443401
Iteration 47, loss = 0.27040165
Iteration 48, loss = 0.27094011
Iteration 49, loss = 0.27216963
Iteration 50, loss = 0.26840856
Iteration 51, loss = 0.26943083
Iteration 52, loss = 0.26964425
Iteration 53, loss = 0.26640155
Iteration 54, loss = 0.26865938
Iteration 55, loss = 0.26884069
Iteration 56, loss = 0.26549797
Iteration 57, loss = 0.26541858
Iteration 58, loss = 0.26502176
Iteration 59, loss = 0.26636613
Iteration 60, loss = 0.26547140
Iteration 61, loss = 0.26332307
Iteration 62, loss = 0.26192554
Iteration 63, loss = 0.26367984
Iteration 64, loss = 0.26350388
Iteration 65, loss = 0.26162735
Iteration 66, loss = 0.26127146
Iteration 67, loss = 0.26382356
Iteration 68, loss = 0.26205007
Iteration 69, loss = 0.26013163
Iteration 70, loss = 0.26244390
Iteration 71, loss = 0.26157407
Iteration 72, loss = 0.26031416
Iteration 73, loss = 0.26006879
Iteration 74, loss = 0.26045920
Iteration 75, loss = 0.25919850
Iteration 76, loss = 0.25762323
Iteration 77, loss = 0.25795665
Iteration 78, loss = 0.25807570
Iteration 79, loss = 0.25680805
Iteration 80, loss = 0.25810415
Iteration 81, loss = 0.25785447
Iteration 82, loss = 0.25631173
Iteration 83, loss = 0.25714018
Iteration 84, loss = 0.25554055
Iteration 85, loss = 0.25768522
Iteration 86, loss = 0.25589027
Iteration 87, loss = 0.25441831
Iteration 88, loss = 0.25593418
Iteration 89, loss = 0.25721591
Iteration 90, loss = 0.25466120
Iteration 91, loss = 0.25386604
Iteration 92, loss = 0.25530873
Iteration 93, loss = 0.25566158
Iteration 94, loss = 0.25551622
Iteration 95, loss = 0.25376243
Iteration 96, loss = 0.25307272
Iteration 97, loss = 0.25301273
Iteration 98, loss = 0.25358549
Iteration 99, loss = 0.25256160
Iteration 100, loss = 0.25241261
Iteration 101, loss = 0.25120164
Iteration 102, loss = 0.25367508
Iteration 103, loss = 0.25229820
Iteration 104, loss = 0.25038882
Iteration 105, loss = 0.24958419
Iteration 106, loss = 0.24990134
Iteration 107, loss = 0.25031230
Iteration 108, loss = 0.25220354
Iteration 109, loss = 0.24870686
Iteration 110, loss = 0.24907816
Iteration 111, loss = 0.25144931
Iteration 112, loss = 0.24932829
Iteration 113, loss = 0.24870530
Iteration 114, loss = 0.24994491
Iteration 115, loss = 0.25018547
Iteration 116, loss = 0.24932737
Iteration 117, loss = 0.24798267
Iteration 118, loss = 0.24967556
Iteration 119, loss = 0.24900543
Iteration 120, loss = 0.24729827
Iteration 121, loss = 0.24669126
Iteration 122, loss = 0.24639480
Iteration 123, loss = 0.24820871
Iteration 124, loss = 0.24949266
Iteration 125, loss = 0.24856359
Iteration 126, loss = 0.24702690
Iteration 127, loss = 0.24747362
Iteration 128, loss = 0.24781960
Iteration 129, loss = 0.24710608
Iteration 130, loss = 0.24674993
Iteration 131, loss = 0.24697437
Iteration 132, loss = 0.24629937
Iteration 133, loss = 0.24629093
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.64518348
Iteration 2, loss = 0.48247410
Iteration 3, loss = 0.44653834
Iteration 4, loss = 0.42720607
Iteration 5, loss = 0.40804219
Iteration 6, loss = 0.39820282
Iteration 7, loss = 0.38657733
Iteration 8, loss = 0.37820638
Iteration 9, loss = 0.36646057
Iteration 10, loss = 0.35844837
Iteration 11, loss = 0.35079626
Iteration 12, loss = 0.34584530
Iteration 13, loss = 0.34211964
Iteration 14, loss = 0.33823088
Iteration 15, loss = 0.33172957
Iteration 16, loss = 0.32488641
Iteration 17, loss = 0.32005233
Iteration 18, loss = 0.31903730
Iteration 19, loss = 0.31716218
Iteration 20, loss = 0.31249851
Iteration 21, loss = 0.30858460
Iteration 22, loss = 0.30665384
Iteration 23, loss = 0.30270405
Iteration 24, loss = 0.30024140
Iteration 25, loss = 0.29787909
Iteration 26, loss = 0.29346635
Iteration 27, loss = 0.29222912
Iteration 28, loss = 0.29113905
Iteration 29, loss = 0.29119954
Iteration 30, loss = 0.28486934
Iteration 31, loss = 0.28730569
Iteration 32, loss = 0.28337461
Iteration 33, loss = 0.28250553
Iteration 34, loss = 0.28184500
Iteration 35, loss = 0.28104807
Iteration 36, loss = 0.27708137
Iteration 37, loss = 0.27440337
Iteration 38, loss = 0.27266785
Iteration 39, loss = 0.27545017
Iteration 40, loss = 0.27054887
Iteration 41, loss = 0.26945250
Iteration 42, loss = 0.27013947
Iteration 43, loss = 0.26787609
Iteration 44, loss = 0.26728478
Iteration 45, loss = 0.26717914
Iteration 46, loss = 0.26572521
Iteration 47, loss = 0.26617579
Iteration 48, loss = 0.26433712
Iteration 49, loss = 0.26457026
Iteration 50, loss = 0.26066451
Iteration 51, loss = 0.26047211
Iteration 52, loss = 0.25822270
Iteration 53, loss = 0.25753686
Iteration 54, loss = 0.25830845
Iteration 55, loss = 0.25871870
Iteration 56, loss = 0.25783426
Iteration 57, loss = 0.25568937
Iteration 58, loss = 0.25527713
Iteration 59, loss = 0.25663306
Iteration 60, loss = 0.25483813
Iteration 61, loss = 0.25601782
Iteration 62, loss = 0.25520135
Iteration 63, loss = 0.25224878
Iteration 64, loss = 0.25262381
Iteration 65, loss = 0.25310232
Iteration 66, loss = 0.24908123
Iteration 67, loss = 0.25030097
Iteration 68, loss = 0.25000021
Iteration 69, loss = 0.24940412
Iteration 70, loss = 0.25111956
Iteration 71, loss = 0.24779054
Iteration 72, loss = 0.24924670
Iteration 73, loss = 0.24795044
Iteration 74, loss = 0.24391706
Iteration 75, loss = 0.24513568
Iteration 76, loss = 0.24850035
Iteration 77, loss = 0.24494200
Iteration 78, loss = 0.24633471
Iteration 79, loss = 0.24393567
Iteration 80, loss = 0.24334309
Iteration 81, loss = 0.24596041
Iteration 82, loss = 0.24344393
Iteration 83, loss = 0.24488651
Iteration 84, loss = 0.24474253
Iteration 85, loss = 0.24251026
Iteration 86, loss = 0.24230614
Iteration 87, loss = 0.24234656
Iteration 88, loss = 0.24261668
Iteration 89, loss = 0.24151290
Iteration 90, loss = 0.24024343
Iteration 91, loss = 0.23846412
Iteration 92, loss = 0.23895520
Iteration 93, loss = 0.23767067
Iteration 94, loss = 0.24058898
Iteration 95, loss = 0.23874896
Iteration 96, loss = 0.24100562
Iteration 97, loss = 0.24165961
Iteration 98, loss = 0.23816384
Iteration 99, loss = 0.23799780
Iteration 100, loss = 0.23582489
Iteration 101, loss = 0.23729921
Iteration 102, loss = 0.23599209
Iteration 103, loss = 0.23703972
Iteration 104, loss = 0.23536265
Iteration 105, loss = 0.23661592
Iteration 106, loss = 0.23688494
Iteration 107, loss = 0.23591052
Iteration 108, loss = 0.23581238
Iteration 109, loss = 0.23376541
Iteration 110, loss = 0.23632519
Iteration 111, loss = 0.23580528
Iteration 112, loss = 0.23689120
Iteration 113, loss = 0.23581580
Iteration 114, loss = 0.23486273
Iteration 115, loss = 0.23237282
Iteration 116, loss = 0.23512383
Iteration 117, loss = 0.23471494
Iteration 118, loss = 0.23456298
Iteration 119, loss = 0.23177474
Iteration 120, loss = 0.23130471
Iteration 121, loss = 0.23290503
Iteration 122, loss = 0.23247577
Iteration 123, loss = 0.23255227
Iteration 124, loss = 0.23286062
Iteration 125, loss = 0.23147478
Iteration 126, loss = 0.22862948
Iteration 127, loss = 0.23236458
Iteration 128, loss = 0.23200046
Iteration 129, loss = 0.22948066
Iteration 130, loss = 0.23164824
Iteration 131, loss = 0.23002949
Iteration 132, loss = 0.23031581
Iteration 133, loss = 0.22944829
Iteration 134, loss = 0.22887425
Iteration 135, loss = 0.22970526
Iteration 136, loss = 0.23023577
Iteration 137, loss = 0.23065014
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.89927270
Iteration 2, loss = 0.68243442
Iteration 3, loss = 0.57265520
Iteration 4, loss = 0.50317260
Iteration 5, loss = 0.46303695
Iteration 6, loss = 0.43353180
Iteration 7, loss = 0.41523665
Iteration 8, loss = 0.39702837
Iteration 9, loss = 0.37965073
Iteration 10, loss = 0.36602984
Iteration 11, loss = 0.35341742
Iteration 12, loss = 0.34203223
Iteration 13, loss = 0.33081387
Iteration 14, loss = 0.33065245
Iteration 15, loss = 0.32104245
Iteration 16, loss = 0.30686133
Iteration 17, loss = 0.29836664
Iteration 18, loss = 0.28859550
Iteration 19, loss = 0.27822964
Iteration 20, loss = 0.28177547
Iteration 21, loss = 0.27087874
Iteration 22, loss = 0.26191065
Iteration 23, loss = 0.25690244
Iteration 24, loss = 0.26520317
Iteration 25, loss = 0.25164254
Iteration 26, loss = 0.24590742
Iteration 27, loss = 0.23931575
Iteration 28, loss = 0.23627748
Iteration 29, loss = 0.23357859
Iteration 30, loss = 0.22831680
Iteration 31, loss = 0.22162627
Iteration 32, loss = 0.21615356
Iteration 33, loss = 0.21322053
Iteration 34, loss = 0.21275638
Iteration 35, loss = 0.20541182
Iteration 36, loss = 0.20455785
Iteration 37, loss = 0.20738361
Iteration 38, loss = 0.19843185
Iteration 39, loss = 0.19751114
Iteration 40, loss = 0.19519692
Iteration 41, loss = 0.21393199
Iteration 42, loss = 0.21089524
Iteration 43, loss = 0.20541887
Iteration 44, loss = 0.19214106
Iteration 45, loss = 0.19384958
Iteration 46, loss = 0.18343370
Iteration 47, loss = 0.18610807
Iteration 48, loss = 0.18012576
Iteration 49, loss = 0.17979836
Iteration 50, loss = 0.18105358
Iteration 51, loss = 0.18302800
Iteration 52, loss = 0.17457456
Iteration 53, loss = 0.18075859
Iteration 54, loss = 0.18635627
Iteration 55, loss = 0.18722403
Iteration 56, loss = 0.17488062
Iteration 57, loss = 0.18102170
Iteration 58, loss = 0.17196151
Iteration 59, loss = 0.16678578
Iteration 60, loss = 0.16510430
Iteration 61, loss = 0.16767555
Iteration 62, loss = 0.16474796
Iteration 63, loss = 0.16805441
Iteration 64, loss = 0.16158125
Iteration 65, loss = 0.16568509
Iteration 66, loss = 0.16717052
Iteration 67, loss = 0.16829912
Iteration 68, loss = 0.17292068
Iteration 69, loss = 0.16332505
Iteration 70, loss = 0.16153136
Iteration 71, loss = 0.16533628
Iteration 72, loss = 0.16946896
Iteration 73, loss = 0.16533051
Iteration 74, loss = 0.17082184
Iteration 75, loss = 0.16628684
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72481693
Iteration 2, loss = 0.50840104
Iteration 3, loss = 0.46125243
Iteration 4, loss = 0.43224015
Iteration 5, loss = 0.42031890
Iteration 6, loss = 0.40442722
Iteration 7, loss = 0.38849547
Iteration 8, loss = 0.38010347
Iteration 9, loss = 0.36902794
Iteration 10, loss = 0.35989026
Iteration 11, loss = 0.35155404
Iteration 12, loss = 0.34594488
Iteration 13, loss = 0.33394333
Iteration 14, loss = 0.33735841
Iteration 15, loss = 0.33000999
Iteration 16, loss = 0.32117819
Iteration 17, loss = 0.32218384
Iteration 18, loss = 0.31259707
Iteration 19, loss = 0.30764331
Iteration 20, loss = 0.30448472
Iteration 21, loss = 0.29953381
Iteration 22, loss = 0.29615151
Iteration 23, loss = 0.29568643
Iteration 24, loss = 0.29241357
Iteration 25, loss = 0.28745159
Iteration 26, loss = 0.28094673
Iteration 27, loss = 0.27922584
Iteration 28, loss = 0.28053681
Iteration 29, loss = 0.27800257
Iteration 30, loss = 0.26856124
Iteration 31, loss = 0.27110389
Iteration 32, loss = 0.26344322
Iteration 33, loss = 0.26823276
Iteration 34, loss = 0.26324653
Iteration 35, loss = 0.26222919
Iteration 36, loss = 0.25773435
Iteration 37, loss = 0.25625300
Iteration 38, loss = 0.25719687
Iteration 39, loss = 0.25148216
Iteration 40, loss = 0.25032279
Iteration 41, loss = 0.24704645
Iteration 42, loss = 0.24904754
Iteration 43, loss = 0.24909894
Iteration 44, loss = 0.24991759
Iteration 45, loss = 0.24530094
Iteration 46, loss = 0.24141351
Iteration 47, loss = 0.24091838
Iteration 48, loss = 0.23883234
Iteration 49, loss = 0.23650325
Iteration 50, loss = 0.23717928
Iteration 51, loss = 0.23799410
Iteration 52, loss = 0.23403696
Iteration 53, loss = 0.23470764
Iteration 54, loss = 0.23480852
Iteration 55, loss = 0.23149769
Iteration 56, loss = 0.23206399
Iteration 57, loss = 0.23223221
Iteration 58, loss = 0.22913893
Iteration 59, loss = 0.22901069
Iteration 60, loss = 0.22972518
Iteration 61, loss = 0.22801017
Iteration 62, loss = 0.22758286
Iteration 63, loss = 0.22548726
Iteration 64, loss = 0.22632920
Iteration 65, loss = 0.22350061
Iteration 66, loss = 0.22102402
Iteration 67, loss = 0.22067546
Iteration 68, loss = 0.22048364
Iteration 69, loss = 0.22077658
Iteration 70, loss = 0.22099634
Iteration 71, loss = 0.21881668
Iteration 72, loss = 0.21845368
Iteration 73, loss = 0.21836304
Iteration 74, loss = 0.21904100
Iteration 75, loss = 0.21788374
Iteration 76, loss = 0.21830369
Iteration 77, loss = 0.21683153
Iteration 78, loss = 0.21360501
Iteration 79, loss = 0.21245932
Iteration 80, loss = 0.21952925
Iteration 81, loss = 0.21739245
Iteration 82, loss = 0.21478910
Iteration 83, loss = 0.21154505
Iteration 84, loss = 0.20998407
Iteration 85, loss = 0.20868221
Iteration 86, loss = 0.21161281
Iteration 87, loss = 0.20644078
Iteration 88, loss = 0.21187510
Iteration 89, loss = 0.21162602
Iteration 90, loss = 0.21177481
Iteration 91, loss = 0.20667966
Iteration 92, loss = 0.20646674
Iteration 93, loss = 0.20546222
Iteration 94, loss = 0.20848293
Iteration 95, loss = 0.20873879
Iteration 96, loss = 0.20914168
Iteration 97, loss = 0.20599820
Iteration 98, loss = 0.20859139
Iteration 99, loss = 0.20415144
Iteration 100, loss = 0.20502866
Iteration 101, loss = 0.20255467
Iteration 102, loss = 0.20390739
Iteration 103, loss = 0.20250401
Iteration 104, loss = 0.20272950
Iteration 105, loss = 0.20136369
Iteration 106, loss = 0.20378160
Iteration 107, loss = 0.20525040
Iteration 108, loss = 0.20038659
Iteration 109, loss = 0.20302004
Iteration 110, loss = 0.20493672
Iteration 111, loss = 0.20109313
Iteration 112, loss = 0.20193828
Iteration 113, loss = 0.19841920
Iteration 114, loss = 0.20030588
Iteration 115, loss = 0.20153776
Iteration 116, loss = 0.20059971
Iteration 117, loss = 0.19896513
Iteration 118, loss = 0.19963041
Iteration 119, loss = 0.20004140
Iteration 120, loss = 0.19936617
Iteration 121, loss = 0.19716340
Iteration 122, loss = 0.19506411
Iteration 123, loss = 0.19684895
Iteration 124, loss = 0.19735489
Iteration 125, loss = 0.19676203
Iteration 126, loss = 0.19772973
Iteration 127, loss = 0.19568936
Iteration 128, loss = 0.19733770
Iteration 129, loss = 0.19654630
Iteration 130, loss = 0.19610873
Iteration 131, loss = 0.19362386
Iteration 132, loss = 0.19581184
Iteration 133, loss = 0.19378149
Iteration 134, loss = 0.19512910
Iteration 135, loss = 0.19199743
Iteration 136, loss = 0.19598830
Iteration 137, loss = 0.19418101
Iteration 138, loss = 0.19369655
Iteration 139, loss = 0.19410002
Iteration 140, loss = 0.19485844
Iteration 141, loss = 0.19016670
Iteration 142, loss = 0.19292304
Iteration 143, loss = 0.19238160
Iteration 144, loss = 0.19321965
Iteration 145, loss = 0.19059247
Iteration 146, loss = 0.19277816
Iteration 147, loss = 0.19274252
Iteration 148, loss = 0.18861031
Iteration 149, loss = 0.19055400
Iteration 150, loss = 0.19092707
Iteration 151, loss = 0.19264158
Iteration 152, loss = 0.19134321
Iteration 153, loss = 0.19076882
Iteration 154, loss = 0.18961584
Iteration 155, loss = 0.18924259
Iteration 156, loss = 0.19021734
Iteration 157, loss = 0.19068098
Iteration 158, loss = 0.19018856
Iteration 159, loss = 0.19185351
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.64389624
Iteration 2, loss = 0.48281671
Iteration 3, loss = 0.44540813
Iteration 4, loss = 0.42541327
Iteration 5, loss = 0.40693600
Iteration 6, loss = 0.39271020
Iteration 7, loss = 0.38758970
Iteration 8, loss = 0.37338183
Iteration 9, loss = 0.36468610
Iteration 10, loss = 0.36114094
Iteration 11, loss = 0.35523960
Iteration 12, loss = 0.34667142
Iteration 13, loss = 0.34100336
Iteration 14, loss = 0.33900517
Iteration 15, loss = 0.32965241
Iteration 16, loss = 0.32560555
Iteration 17, loss = 0.32221521
Iteration 18, loss = 0.32022208
Iteration 19, loss = 0.31416873
Iteration 20, loss = 0.31295314
Iteration 21, loss = 0.30644901
Iteration 22, loss = 0.30796258
Iteration 23, loss = 0.30320187
Iteration 24, loss = 0.30151886
Iteration 25, loss = 0.29738954
Iteration 26, loss = 0.29799128
Iteration 27, loss = 0.29340521
Iteration 28, loss = 0.29203701
Iteration 29, loss = 0.28975907
Iteration 30, loss = 0.28658259
Iteration 31, loss = 0.28605094
Iteration 32, loss = 0.28521598
Iteration 33, loss = 0.28268083
Iteration 34, loss = 0.28175939
Iteration 35, loss = 0.28179665
Iteration 36, loss = 0.27809480
Iteration 37, loss = 0.27986135
Iteration 38, loss = 0.27334275
Iteration 39, loss = 0.27475686
Iteration 40, loss = 0.27379905
Iteration 41, loss = 0.27406698
Iteration 42, loss = 0.26904529
Iteration 43, loss = 0.26995590
Iteration 44, loss = 0.26691164
Iteration 45, loss = 0.26863293
Iteration 46, loss = 0.26774001
Iteration 47, loss = 0.26694219
Iteration 48, loss = 0.26366178
Iteration 49, loss = 0.26498626
Iteration 50, loss = 0.26249841
Iteration 51, loss = 0.26351510
Iteration 52, loss = 0.26003998
Iteration 53, loss = 0.26001219
Iteration 54, loss = 0.25971488
Iteration 55, loss = 0.25751790
Iteration 56, loss = 0.25697068
Iteration 57, loss = 0.25625581
Iteration 58, loss = 0.25770374
Iteration 59, loss = 0.25525414
Iteration 60, loss = 0.25559062
Iteration 61, loss = 0.25503302
Iteration 62, loss = 0.25647767
Iteration 63, loss = 0.25521614
Iteration 64, loss = 0.25134037
Iteration 65, loss = 0.25429083
Iteration 66, loss = 0.25004459
Iteration 67, loss = 0.25072795
Iteration 68, loss = 0.25233797
Iteration 69, loss = 0.25125023
Iteration 70, loss = 0.24968021
Iteration 71, loss = 0.24818171
Iteration 72, loss = 0.24830010
Iteration 73, loss = 0.24928697
Iteration 74, loss = 0.24612940
Iteration 75, loss = 0.24825141
Iteration 76, loss = 0.24577013
Iteration 77, loss = 0.24607312
Iteration 78, loss = 0.24435635
Iteration 79, loss = 0.24609407
Iteration 80, loss = 0.24834190
Iteration 81, loss = 0.24353761
Iteration 82, loss = 0.24574163
Iteration 83, loss = 0.24260641
Iteration 84, loss = 0.24394653
Iteration 85, loss = 0.24443000
Iteration 86, loss = 0.24020369
Iteration 87, loss = 0.24184650
Iteration 88, loss = 0.24270438
Iteration 89, loss = 0.24211488
Iteration 90, loss = 0.24338704
Iteration 91, loss = 0.24155483
Iteration 92, loss = 0.23943628
Iteration 93, loss = 0.24139895
Iteration 94, loss = 0.23865167
Iteration 95, loss = 0.24075618
Iteration 96, loss = 0.23917765
Iteration 97, loss = 0.23953872
Iteration 98, loss = 0.23999079
Iteration 99, loss = 0.24053408
Iteration 100, loss = 0.23797580
Iteration 101, loss = 0.23857048
Iteration 102, loss = 0.23704314
Iteration 103, loss = 0.23787733
Iteration 104, loss = 0.23808832
Iteration 105, loss = 0.23619723
Iteration 106, loss = 0.23549674
Iteration 107, loss = 0.23723357
Iteration 108, loss = 0.23437925
Iteration 109, loss = 0.23638365
Iteration 110, loss = 0.23513588
Iteration 111, loss = 0.23497795
Iteration 112, loss = 0.23389649
Iteration 113, loss = 0.23357865
Iteration 114, loss = 0.23374855
Iteration 115, loss = 0.23355506
Iteration 116, loss = 0.23363142
Iteration 117, loss = 0.23446407
Iteration 118, loss = 0.23183224
Iteration 119, loss = 0.23080728
Iteration 120, loss = 0.23509479
Iteration 121, loss = 0.23219258
Iteration 122, loss = 0.23417983
Iteration 123, loss = 0.23364743
Iteration 124, loss = 0.23279178
Iteration 125, loss = 0.23143867
Iteration 126, loss = 0.23025275
Iteration 127, loss = 0.23276910
Iteration 128, loss = 0.22767683
Iteration 129, loss = 0.23170507
Iteration 130, loss = 0.23333184
Iteration 131, loss = 0.22993405
Iteration 132, loss = 0.22850095
Iteration 133, loss = 0.22820117
Iteration 134, loss = 0.22980066
Iteration 135, loss = 0.22958175
Iteration 136, loss = 0.23033639
Iteration 137, loss = 0.23106529
Iteration 138, loss = 0.22879487
Iteration 139, loss = 0.23121638
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.90474323
Iteration 2, loss = 0.68031817
Iteration 3, loss = 0.57003127
Iteration 4, loss = 0.49926802
Iteration 5, loss = 0.46517590
Iteration 6, loss = 0.43802357
Iteration 7, loss = 0.41429129
Iteration 8, loss = 0.39832508
Iteration 9, loss = 0.38355354
Iteration 10, loss = 0.37168607
Iteration 11, loss = 0.35601394
Iteration 12, loss = 0.34617528
Iteration 13, loss = 0.33920468
Iteration 14, loss = 0.32607675
Iteration 15, loss = 0.31686693
Iteration 16, loss = 0.30691476
Iteration 17, loss = 0.29981491
Iteration 18, loss = 0.29264502
Iteration 19, loss = 0.28453148
Iteration 20, loss = 0.28151432
Iteration 21, loss = 0.27522358
Iteration 22, loss = 0.26788420
Iteration 23, loss = 0.26908726
Iteration 24, loss = 0.26378736
Iteration 25, loss = 0.25569641
Iteration 26, loss = 0.24804503
Iteration 27, loss = 0.24360669
Iteration 28, loss = 0.24135850
Iteration 29, loss = 0.24144053
Iteration 30, loss = 0.23807798
Iteration 31, loss = 0.23673086
Iteration 32, loss = 0.22642011
Iteration 33, loss = 0.22392372
Iteration 34, loss = 0.22884066
Iteration 35, loss = 0.22342955
Iteration 36, loss = 0.22087629
Iteration 37, loss = 0.22090014
Iteration 38, loss = 0.21248858
Iteration 39, loss = 0.21660328
Iteration 40, loss = 0.21004530
Iteration 41, loss = 0.20915019
Iteration 42, loss = 0.20933609
Iteration 43, loss = 0.20352921
Iteration 44, loss = 0.20453499
Iteration 45, loss = 0.19674585
Iteration 46, loss = 0.19695611
Iteration 47, loss = 0.19492905
Iteration 48, loss = 0.19247967
Iteration 49, loss = 0.19056622
Iteration 50, loss = 0.19477776
Iteration 51, loss = 0.19397529
Iteration 52, loss = 0.19296497
Iteration 53, loss = 0.19727652
Iteration 54, loss = 0.20022856
Iteration 55, loss = 0.19818147
Iteration 56, loss = 0.18291020
Iteration 57, loss = 0.18834281
Iteration 58, loss = 0.19039453
Iteration 59, loss = 0.18253761
Iteration 60, loss = 0.17466187
Iteration 61, loss = 0.17851112
Iteration 62, loss = 0.17401451
Iteration 63, loss = 0.17368697
Iteration 64, loss = 0.18065882
Iteration 65, loss = 0.17663138
Iteration 66, loss = 0.17474607
Iteration 67, loss = 0.17624886
Iteration 68, loss = 0.16853674
Iteration 69, loss = 0.16952622
Iteration 70, loss = 0.17369456
Iteration 71, loss = 0.16832268
Iteration 72, loss = 0.16997450
Iteration 73, loss = 0.16567543
Iteration 74, loss = 0.16707779
Iteration 75, loss = 0.16890447
Iteration 76, loss = 0.16708924
Iteration 77, loss = 0.16314120
Iteration 78, loss = 0.16691236
Iteration 79, loss = 0.16494122
Iteration 80, loss = 0.16267272
Iteration 81, loss = 0.16516186
Iteration 82, loss = 0.16397199
Iteration 83, loss = 0.16585166
Iteration 84, loss = 0.16357208
Iteration 85, loss = 0.16026016
Iteration 86, loss = 0.15940668
Iteration 87, loss = 0.16402071
Iteration 88, loss = 0.15678895
Iteration 89, loss = 0.15637788
Iteration 90, loss = 0.15262678
Iteration 91, loss = 0.15502785
Iteration 92, loss = 0.15621516
Iteration 93, loss = 0.15747503
Iteration 94, loss = 0.15905434
Iteration 95, loss = 0.15060183
Iteration 96, loss = 0.16059399
Iteration 97, loss = 0.15897849
Iteration 98, loss = 0.16092879
Iteration 99, loss = 0.16075330
Iteration 100, loss = 0.15522192
Iteration 101, loss = 0.15900756
Iteration 102, loss = 0.15222671
Iteration 103, loss = 0.15520146
Iteration 104, loss = 0.15174189
Iteration 105, loss = 0.14960321
Iteration 106, loss = 0.14860358
Iteration 107, loss = 0.15574191
Iteration 108, loss = 0.15208843
Iteration 109, loss = 0.14873334
Iteration 77, loss = 0.26527590
Iteration 78, loss = 0.26585082
Iteration 79, loss = 0.26517710
Iteration 80, loss = 0.26349367
Iteration 81, loss = 0.26283174
Iteration 82, loss = 0.26383451
Iteration 83, loss = 0.26289869
Iteration 84, loss = 0.26310852
Iteration 85, loss = 0.26393344
Iteration 86, loss = 0.26285706
Iteration 87, loss = 0.26167153
Iteration 88, loss = 0.26255270
Iteration 89, loss = 0.26144537
Iteration 90, loss = 0.26229320
Iteration 91, loss = 0.26047816
Iteration 92, loss = 0.25975530
Iteration 93, loss = 0.26051792
Iteration 94, loss = 0.26058615
Iteration 95, loss = 0.26111687
Iteration 96, loss = 0.26113992
Iteration 97, loss = 0.26043700
Iteration 98, loss = 0.26072147
Iteration 99, loss = 0.25935209
Iteration 100, loss = 0.25920312
Iteration 101, loss = 0.25935673
Iteration 102, loss = 0.25890292
Iteration 103, loss = 0.26077510
Iteration 104, loss = 0.25895707
Iteration 105, loss = 0.26055869
Iteration 106, loss = 0.25805259
Iteration 107, loss = 0.25822030
Iteration 108, loss = 0.25911628
Iteration 109, loss = 0.25961458
Iteration 110, loss = 0.25660722
Iteration 111, loss = 0.25825956
Iteration 112, loss = 0.25721516
Iteration 113, loss = 0.25799000
Iteration 114, loss = 0.25735947
Iteration 115, loss = 0.25671756
Iteration 116, loss = 0.25707144
Iteration 117, loss = 0.25791363
Iteration 118, loss = 0.25639257
Iteration 119, loss = 0.25742685
Iteration 120, loss = 0.25666184
Iteration 121, loss = 0.25717348
Iteration 122, loss = 0.25745705
Iteration 123, loss = 0.25677378
Iteration 124, loss = 0.25599601
Iteration 125, loss = 0.25690878
Iteration 126, loss = 0.25740006
Iteration 127, loss = 0.25444384
Iteration 128, loss = 0.25726789
Iteration 129, loss = 0.25640802
Iteration 130, loss = 0.25729081
Iteration 131, loss = 0.25557172
Iteration 132, loss = 0.25413341
Iteration 133, loss = 0.25565237
Iteration 134, loss = 0.25479467
Iteration 135, loss = 0.25450158
Iteration 136, loss = 0.25479603
Iteration 137, loss = 0.25450606
Iteration 138, loss = 0.25568942
Iteration 139, loss = 0.25406464
Iteration 140, loss = 0.25336252
Iteration 141, loss = 0.25546989
Iteration 142, loss = 0.25375390
Iteration 143, loss = 0.25435755
Iteration 144, loss = 0.25550528
Iteration 145, loss = 0.25380166
Iteration 146, loss = 0.25335172
Iteration 147, loss = 0.25283315
Iteration 148, loss = 0.25305750
Iteration 149, loss = 0.25429984
Iteration 150, loss = 0.25289041
Iteration 151, loss = 0.25166210
Iteration 152, loss = 0.25408550
Iteration 153, loss = 0.25449609
Iteration 154, loss = 0.25417117
Iteration 155, loss = 0.25181563
Iteration 156, loss = 0.25215081
Iteration 157, loss = 0.25324932
Iteration 158, loss = 0.25351619
Iteration 159, loss = 0.25367413
Iteration 160, loss = 0.25225537
Iteration 161, loss = 0.25300634
Iteration 162, loss = 0.25235804
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.60373723
Iteration 2, loss = 0.46359762
Iteration 3, loss = 0.43354734
Iteration 4, loss = 0.41328742
Iteration 5, loss = 0.39804074
Iteration 6, loss = 0.38381245
Iteration 7, loss = 0.37547750
Iteration 8, loss = 0.36452684
Iteration 9, loss = 0.35877942
Iteration 10, loss = 0.35477836
Iteration 11, loss = 0.34415442
Iteration 12, loss = 0.34143028
Iteration 13, loss = 0.33388674
Iteration 14, loss = 0.33326229
Iteration 15, loss = 0.32618191
Iteration 16, loss = 0.32408684
Iteration 17, loss = 0.32175509
Iteration 18, loss = 0.31671694
Iteration 19, loss = 0.31243350
Iteration 20, loss = 0.31187342
Iteration 21, loss = 0.30841666
Iteration 22, loss = 0.30635418
Iteration 23, loss = 0.30211262
Iteration 24, loss = 0.30245007
Iteration 25, loss = 0.29961593
Iteration 26, loss = 0.30034023
Iteration 27, loss = 0.29640127
Iteration 28, loss = 0.29612798
Iteration 29, loss = 0.29182841
Iteration 30, loss = 0.29051985
Iteration 31, loss = 0.29067204
Iteration 32, loss = 0.28948539
Iteration 33, loss = 0.28756556
Iteration 34, loss = 0.28685254
Iteration 35, loss = 0.28480471
Iteration 36, loss = 0.28247745
Iteration 37, loss = 0.28303262
Iteration 38, loss = 0.28290077
Iteration 39, loss = 0.28018765
Iteration 40, loss = 0.27976182
Iteration 41, loss = 0.27830193
Iteration 42, loss = 0.27836745
Iteration 43, loss = 0.27650918
Iteration 44, loss = 0.27619426
Iteration 45, loss = 0.27526915
Iteration 46, loss = 0.27614633
Iteration 47, loss = 0.27445904
Iteration 48, loss = 0.27254413
Iteration 49, loss = 0.27302873
Iteration 50, loss = 0.27176010
Iteration 51, loss = 0.27151874
Iteration 52, loss = 0.27053320
Iteration 53, loss = 0.27313218
Iteration 54, loss = 0.26978146
Iteration 55, loss = 0.26954306
Iteration 56, loss = 0.26743303
Iteration 57, loss = 0.26786911
Iteration 58, loss = 0.26820694
Iteration 59, loss = 0.26678747
Iteration 60, loss = 0.26620632
Iteration 61, loss = 0.26655159
Iteration 62, loss = 0.26453935
Iteration 63, loss = 0.26617866
Iteration 64, loss = 0.26256095
Iteration 65, loss = 0.26389254
Iteration 66, loss = 0.26230885
Iteration 67, loss = 0.26451268
Iteration 68, loss = 0.26245302
Iteration 69, loss = 0.26280032
Iteration 70, loss = 0.26370303
Iteration 71, loss = 0.26141539
Iteration 72, loss = 0.26100443
Iteration 73, loss = 0.26186301
Iteration 74, loss = 0.25899817
Iteration 75, loss = 0.26160533
Iteration 76, loss = 0.26097701
Iteration 77, loss = 0.25908817
Iteration 78, loss = 0.25785193
Iteration 79, loss = 0.25798240
Iteration 80, loss = 0.25747672
Iteration 81, loss = 0.26024626
Iteration 82, loss = 0.25830628
Iteration 83, loss = 0.25776221
Iteration 84, loss = 0.25877422
Iteration 85, loss = 0.25700475
Iteration 86, loss = 0.25847926
Iteration 87, loss = 0.25625645
Iteration 88, loss = 0.25729385
Iteration 89, loss = 0.25526849
Iteration 90, loss = 0.25588930
Iteration 91, loss = 0.25504099
Iteration 92, loss = 0.25602308
Iteration 93, loss = 0.25681059
Iteration 94, loss = 0.25512968
Iteration 95, loss = 0.25274158
Iteration 96, loss = 0.25332293
Iteration 97, loss = 0.25472248
Iteration 98, loss = 0.25445139
Iteration 99, loss = 0.25376854
Iteration 100, loss = 0.25415215
Iteration 101, loss = 0.25273696
Iteration 102, loss = 0.25483106
Iteration 103, loss = 0.25395597
Iteration 104, loss = 0.25333485
Iteration 105, loss = 0.25319376
Iteration 106, loss = 0.25240555
Iteration 107, loss = 0.25150388
Iteration 108, loss = 0.25162166
Iteration 109, loss = 0.25171825
Iteration 110, loss = 0.25219135
Iteration 111, loss = 0.25140171
Iteration 112, loss = 0.24961306
Iteration 113, loss = 0.25127750
Iteration 114, loss = 0.24931160
Iteration 115, loss = 0.25075902
Iteration 116, loss = 0.25096240
Iteration 117, loss = 0.25039754
Iteration 118, loss = 0.24810897
Iteration 119, loss = 0.24991010
Iteration 120, loss = 0.25032783
Iteration 121, loss = 0.24916419
Iteration 122, loss = 0.24951591
Iteration 123, loss = 0.24896087
Iteration 124, loss = 0.25066901
Iteration 125, loss = 0.25156302
Iteration 126, loss = 0.24890730
Iteration 127, loss = 0.24804939
Iteration 128, loss = 0.24875647
Iteration 129, loss = 0.24676807
Iteration 130, loss = 0.24916177
Iteration 131, loss = 0.24879487
Iteration 132, loss = 0.24709387
Iteration 133, loss = 0.24778520
Iteration 134, loss = 0.24777606
Iteration 135, loss = 0.24778763
Iteration 136, loss = 0.24752959
Iteration 137, loss = 0.24695398
Iteration 138, loss = 0.24829259
Iteration 139, loss = 0.24704370
Iteration 140, loss = 0.24892243
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.64361053
Iteration 2, loss = 0.47983285
Iteration 3, loss = 0.44339975
Iteration 4, loss = 0.42236043
Iteration 5, loss = 0.40610866
Iteration 6, loss = 0.39271235
Iteration 7, loss = 0.38247642
Iteration 8, loss = 0.36990143
Iteration 9, loss = 0.36770648
Iteration 10, loss = 0.35639065
Iteration 11, loss = 0.35235731
Iteration 12, loss = 0.34243114
Iteration 13, loss = 0.33811961
Iteration 14, loss = 0.33374934
Iteration 15, loss = 0.32807676
Iteration 16, loss = 0.32265403
Iteration 17, loss = 0.32001789
Iteration 18, loss = 0.31587168
Iteration 19, loss = 0.31211320
Iteration 20, loss = 0.30849927
Iteration 21, loss = 0.30606555
Iteration 22, loss = 0.30313849
Iteration 110, loss = 0.14644311
Iteration 111, loss = 0.15040655
Iteration 112, loss = 0.14935079
Iteration 113, loss = 0.14425072
Iteration 114, loss = 0.14199030
Iteration 115, loss = 0.14618576
Iteration 116, loss = 0.14842269
Iteration 117, loss = 0.15623489
Iteration 118, loss = 0.16003246
Iteration 119, loss = 0.14979192
Iteration 120, loss = 0.14513944
Iteration 121, loss = 0.14788880
Iteration 122, loss = 0.14321341
Iteration 123, loss = 0.14435681
Iteration 124, loss = 0.14320158
Iteration 125, loss = 0.13963804
Iteration 126, loss = 0.14339161
Iteration 127, loss = 0.14187584
Iteration 128, loss = 0.14035238
Iteration 129, loss = 0.13614338
Iteration 130, loss = 0.14021927
Iteration 131, loss = 0.13775103
Iteration 132, loss = 0.13687182
Iteration 133, loss = 0.14272332
Iteration 134, loss = 0.14492015
Iteration 135, loss = 0.14074494
Iteration 136, loss = 0.13557694
Iteration 137, loss = 0.13676863
Iteration 138, loss = 0.13966978
Iteration 139, loss = 0.14357509
Iteration 140, loss = 0.14370418
Iteration 141, loss = 0.14110728
Iteration 142, loss = 0.14588816
Iteration 143, loss = 0.13627086
Iteration 144, loss = 0.13807116
Iteration 145, loss = 0.13505449
Iteration 146, loss = 0.13481823
Iteration 147, loss = 0.13832708
Iteration 148, loss = 0.13616832
Iteration 149, loss = 0.13699008
Iteration 150, loss = 0.13756207
Iteration 151, loss = 0.13224440
Iteration 152, loss = 0.13271697
Iteration 153, loss = 0.13631905
Iteration 154, loss = 0.13730042
Iteration 155, loss = 0.13318744
Iteration 156, loss = 0.13460914
Iteration 157, loss = 0.13374143
Iteration 158, loss = 0.13478207
Iteration 159, loss = 0.13826371
Iteration 160, loss = 0.13894319
Iteration 161, loss = 0.13454312
Iteration 162, loss = 0.14005201
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71706458
Iteration 2, loss = 0.50684175
Iteration 3, loss = 0.46544606
Iteration 4, loss = 0.44022763
Iteration 5, loss = 0.42621195
Iteration 6, loss = 0.40482451
Iteration 7, loss = 0.39328029
Iteration 8, loss = 0.38665288
Iteration 9, loss = 0.37164138
Iteration 10, loss = 0.36403112
Iteration 11, loss = 0.35668485
Iteration 12, loss = 0.34755592
Iteration 13, loss = 0.34511191
Iteration 14, loss = 0.33939874
Iteration 15, loss = 0.33531717
Iteration 16, loss = 0.32524555
Iteration 17, loss = 0.32983501
Iteration 18, loss = 0.31794343
Iteration 19, loss = 0.31371394
Iteration 20, loss = 0.31239212
Iteration 21, loss = 0.30440275
Iteration 22, loss = 0.30328475
Iteration 23, loss = 0.29681348
Iteration 24, loss = 0.29649732
Iteration 25, loss = 0.29120471
Iteration 26, loss = 0.29302985
Iteration 27, loss = 0.29046412
Iteration 28, loss = 0.28749469
Iteration 29, loss = 0.28268285
Iteration 30, loss = 0.28018416
Iteration 31, loss = 0.27722268
Iteration 32, loss = 0.27308913
Iteration 33, loss = 0.27453917
Iteration 34, loss = 0.27196935
Iteration 35, loss = 0.26905901
Iteration 36, loss = 0.26871987
Iteration 37, loss = 0.26772064
Iteration 38, loss = 0.26116767
Iteration 39, loss = 0.26288886
Iteration 40, loss = 0.26017770
Iteration 41, loss = 0.25922477
Iteration 42, loss = 0.25794165
Iteration 43, loss = 0.25534430
Iteration 44, loss = 0.25478291
Iteration 45, loss = 0.25821532
Iteration 46, loss = 0.25218371
Iteration 47, loss = 0.25036412
Iteration 48, loss = 0.24912846
Iteration 49, loss = 0.24817872
Iteration 50, loss = 0.24989117
Iteration 51, loss = 0.24443627
Iteration 52, loss = 0.24474137
Iteration 53, loss = 0.24308049
Iteration 54, loss = 0.24070595
Iteration 55, loss = 0.24726008
Iteration 56, loss = 0.24008556
Iteration 57, loss = 0.23977634
Iteration 58, loss = 0.23761472
Iteration 59, loss = 0.23758440
Iteration 60, loss = 0.23512751
Iteration 61, loss = 0.23413552
Iteration 62, loss = 0.23382508
Iteration 63, loss = 0.23369993
Iteration 64, loss = 0.23365982
Iteration 65, loss = 0.23562912
Iteration 66, loss = 0.23367127
Iteration 67, loss = 0.23153477
Iteration 68, loss = 0.23139136
Iteration 69, loss = 0.23142726
Iteration 70, loss = 0.22966299
Iteration 71, loss = 0.22985333
Iteration 72, loss = 0.22666977
Iteration 73, loss = 0.22471906
Iteration 74, loss = 0.22524457
Iteration 75, loss = 0.22328188
Iteration 76, loss = 0.22412239
Iteration 77, loss = 0.22461425
Iteration 78, loss = 0.22471142
Iteration 79, loss = 0.22190161
Iteration 80, loss = 0.22229867
Iteration 81, loss = 0.22165981
Iteration 82, loss = 0.21713364
Iteration 83, loss = 0.22036582
Iteration 84, loss = 0.22163566
Iteration 85, loss = 0.21995804
Iteration 86, loss = 0.22307756
Iteration 87, loss = 0.21865274
Iteration 88, loss = 0.21873499
Iteration 89, loss = 0.21673100
Iteration 90, loss = 0.21658073
Iteration 91, loss = 0.21748372
Iteration 92, loss = 0.21395049
Iteration 93, loss = 0.21567347
Iteration 94, loss = 0.21557903
Iteration 95, loss = 0.21138620
Iteration 96, loss = 0.21668599
Iteration 97, loss = 0.21308639
Iteration 98, loss = 0.21186887
Iteration 99, loss = 0.21589780
Iteration 100, loss = 0.21082834
Iteration 101, loss = 0.21358195
Iteration 102, loss = 0.21065981
Iteration 103, loss = 0.21289454
Iteration 104, loss = 0.21063802
Iteration 105, loss = 0.21018459
Iteration 106, loss = 0.21046540
Iteration 107, loss = 0.21038280
Iteration 108, loss = 0.20738646
Iteration 109, loss = 0.20646293
Iteration 110, loss = 0.20772291
Iteration 111, loss = 0.21014798
Iteration 112, loss = 0.20690186
Iteration 113, loss = 0.20629955
Iteration 114, loss = 0.20748549
Iteration 115, loss = 0.20659876
Iteration 116, loss = 0.20586544
Iteration 117, loss = 0.20488294
Iteration 118, loss = 0.20825389
Iteration 119, loss = 0.20693274
Iteration 120, loss = 0.20658068
Iteration 121, loss = 0.20557619
Iteration 122, loss = 0.20908366
Iteration 123, loss = 0.20613362
Iteration 124, loss = 0.20088546
Iteration 125, loss = 0.20080798
Iteration 126, loss = 0.20397581
Iteration 127, loss = 0.20271514
Iteration 128, loss = 0.20269244
Iteration 129, loss = 0.20449211
Iteration 130, loss = 0.20737291
Iteration 131, loss = 0.20285046
Iteration 132, loss = 0.20271215
Iteration 133, loss = 0.20249545
Iteration 134, loss = 0.19965067
Iteration 135, loss = 0.20169546
Iteration 136, loss = 0.19870842
Iteration 137, loss = 0.20608153
Iteration 138, loss = 0.19940787
Iteration 139, loss = 0.19888343
Iteration 140, loss = 0.20176873
Iteration 141, loss = 0.19875811
Iteration 142, loss = 0.20117119
Iteration 143, loss = 0.19617973
Iteration 144, loss = 0.20063773
Iteration 145, loss = 0.19799192
Iteration 146, loss = 0.19973386
Iteration 147, loss = 0.20040210
Iteration 148, loss = 0.19752444
Iteration 149, loss = 0.19706335
Iteration 150, loss = 0.20074187
Iteration 151, loss = 0.19789749
Iteration 152, loss = 0.20023984
Iteration 153, loss = 0.20188159
Iteration 154, loss = 0.19624353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.57520683
Iteration 2, loss = 0.45314557
Iteration 3, loss = 0.41952403
Iteration 4, loss = 0.40037114
Iteration 5, loss = 0.38648193
Iteration 6, loss = 0.37626495
Iteration 7, loss = 0.36340276
Iteration 8, loss = 0.35639428
Iteration 9, loss = 0.35023195
Iteration 10, loss = 0.34262194
Iteration 11, loss = 0.33642045
Iteration 12, loss = 0.33209396
Iteration 13, loss = 0.32698348
Iteration 14, loss = 0.32379890
Iteration 15, loss = 0.31905751
Iteration 16, loss = 0.31645255
Iteration 17, loss = 0.31345659
Iteration 18, loss = 0.31200130
Iteration 19, loss = 0.30898005
Iteration 20, loss = 0.30481910
Iteration 21, loss = 0.30376173
Iteration 22, loss = 0.30253908
Iteration 23, loss = 0.30052285
Iteration 24, loss = 0.29801924
Iteration 25, loss = 0.29451591
Iteration 26, loss = 0.29360550
Iteration 27, loss = 0.29436570
Iteration 28, loss = 0.29220491
Iteration 29, loss = 0.29187421
Iteration 30, loss = 0.29021052
Iteration 31, loss = 0.28760341
Iteration 32, loss = 0.28707109
Iteration 33, loss = 0.28753847
Iteration 34, loss = 0.28552797
Iteration 35, loss = 0.28280960
Iteration 36, loss = 0.28352805
Iteration 37, loss = 0.28217888
Iteration 38, loss = 0.28174126
Iteration 39, loss = 0.28229719
Iteration 40, loss = 0.27894370
Iteration 41, loss = 0.28116740
Iteration 70, loss = 0.26222548
Iteration 71, loss = 0.26012177
Iteration 72, loss = 0.26067885
Iteration 73, loss = 0.25886645
Iteration 74, loss = 0.25828635
Iteration 75, loss = 0.25986242
Iteration 76, loss = 0.25992563
Iteration 77, loss = 0.25955132
Iteration 78, loss = 0.25808230
Iteration 79, loss = 0.25856013
Iteration 80, loss = 0.25833317
Iteration 81, loss = 0.25633333
Iteration 82, loss = 0.25752111
Iteration 83, loss = 0.25610464
Iteration 84, loss = 0.25653741
Iteration 85, loss = 0.25605068
Iteration 86, loss = 0.25437727
Iteration 87, loss = 0.25800873
Iteration 88, loss = 0.25317755
Iteration 89, loss = 0.25559633
Iteration 90, loss = 0.25674007
Iteration 91, loss = 0.25610036
Iteration 92, loss = 0.25495557
Iteration 93, loss = 0.25461744
Iteration 94, loss = 0.25537294
Iteration 95, loss = 0.25630676
Iteration 96, loss = 0.25252281
Iteration 97, loss = 0.25191394
Iteration 98, loss = 0.25261313
Iteration 99, loss = 0.25214467
Iteration 100, loss = 0.25248418
Iteration 101, loss = 0.25281149
Iteration 102, loss = 0.25234140
Iteration 103, loss = 0.25220957
Iteration 104, loss = 0.25222331
Iteration 105, loss = 0.25030188
Iteration 106, loss = 0.25222883
Iteration 107, loss = 0.25064122
Iteration 108, loss = 0.25001242
Iteration 109, loss = 0.24974236
Iteration 110, loss = 0.24912928
Iteration 111, loss = 0.25057976
Iteration 112, loss = 0.24956611
Iteration 113, loss = 0.24994058
Iteration 114, loss = 0.25031941
Iteration 115, loss = 0.24996427
Iteration 116, loss = 0.24878766
Iteration 117, loss = 0.24781530
Iteration 118, loss = 0.24791644
Iteration 119, loss = 0.24843890
Iteration 120, loss = 0.24833967
Iteration 121, loss = 0.24878294
Iteration 122, loss = 0.24950012
Iteration 123, loss = 0.24719081
Iteration 124, loss = 0.24719343
Iteration 125, loss = 0.24722262
Iteration 126, loss = 0.24766429
Iteration 127, loss = 0.24777045
Iteration 128, loss = 0.24618992
Iteration 129, loss = 0.24836898
Iteration 130, loss = 0.24564929
Iteration 131, loss = 0.24625857
Iteration 132, loss = 0.24646116
Iteration 133, loss = 0.24822668
Iteration 134, loss = 0.24515349
Iteration 135, loss = 0.24891421
Iteration 136, loss = 0.24503078
Iteration 137, loss = 0.24651403
Iteration 138, loss = 0.24525622
Iteration 139, loss = 0.24676227
Iteration 140, loss = 0.24542430
Iteration 141, loss = 0.24535232
Iteration 142, loss = 0.24343054
Iteration 143, loss = 0.24395732
Iteration 144, loss = 0.24559376
Iteration 145, loss = 0.24637477
Iteration 146, loss = 0.24616404
Iteration 147, loss = 0.24446819
Iteration 148, loss = 0.24425039
Iteration 149, loss = 0.24643733
Iteration 150, loss = 0.24326914
Iteration 151, loss = 0.24595661
Iteration 152, loss = 0.24518491
Iteration 153, loss = 0.24357891
Iteration 154, loss = 0.24317273
Iteration 155, loss = 0.24395053
Iteration 156, loss = 0.24713468
Iteration 157, loss = 0.24247131
Iteration 158, loss = 0.24516733
Iteration 159, loss = 0.24269467
Iteration 160, loss = 0.24304564
Iteration 161, loss = 0.24365861
Iteration 162, loss = 0.24181737
Iteration 163, loss = 0.24112580
Iteration 164, loss = 0.24221531
Iteration 165, loss = 0.24221156
Iteration 166, loss = 0.24376793
Iteration 167, loss = 0.24483716
Iteration 168, loss = 0.24358819
Iteration 169, loss = 0.24244816
Iteration 170, loss = 0.24382901
Iteration 171, loss = 0.24306619
Iteration 172, loss = 0.24175746
Iteration 173, loss = 0.24397254
Iteration 174, loss = 0.24151225
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.57930677
Iteration 2, loss = 0.45650077
Iteration 3, loss = 0.42256076
Iteration 4, loss = 0.40427183
Iteration 5, loss = 0.39037593
Iteration 6, loss = 0.37774412
Iteration 7, loss = 0.36835924
Iteration 8, loss = 0.35947756
Iteration 9, loss = 0.35173743
Iteration 10, loss = 0.34541502
Iteration 11, loss = 0.33976601
Iteration 12, loss = 0.33641361
Iteration 13, loss = 0.33171408
Iteration 14, loss = 0.32714186
Iteration 15, loss = 0.32368262
Iteration 16, loss = 0.31988101
Iteration 17, loss = 0.31861169
Iteration 18, loss = 0.31536384
Iteration 19, loss = 0.31321251
Iteration 20, loss = 0.30895082
Iteration 21, loss = 0.30936862
Iteration 22, loss = 0.30458408
Iteration 23, loss = 0.30372386
Iteration 24, loss = 0.30159724
Iteration 25, loss = 0.30144211
Iteration 26, loss = 0.29788135
Iteration 27, loss = 0.29785451
Iteration 28, loss = 0.29574542
Iteration 29, loss = 0.29443248
Iteration 30, loss = 0.29243178
Iteration 31, loss = 0.29191266
Iteration 32, loss = 0.29131620
Iteration 33, loss = 0.28977863
Iteration 34, loss = 0.29100285
Iteration 35, loss = 0.28872896
Iteration 36, loss = 0.28597217
Iteration 37, loss = 0.28519663
Iteration 38, loss = 0.28670331
Iteration 39, loss = 0.28420729
Iteration 40, loss = 0.28482633
Iteration 41, loss = 0.28302372
Iteration 42, loss = 0.28074196
Iteration 43, loss = 0.27979534
Iteration 44, loss = 0.28167816
Iteration 45, loss = 0.27987346
Iteration 46, loss = 0.28144813
Iteration 47, loss = 0.28083573
Iteration 48, loss = 0.28030061
Iteration 49, loss = 0.27876229
Iteration 50, loss = 0.27854314
Iteration 51, loss = 0.27843399
Iteration 52, loss = 0.27583596
Iteration 53, loss = 0.27626289
Iteration 54, loss = 0.27490186
Iteration 55, loss = 0.27514021
Iteration 56, loss = 0.27719169
Iteration 57, loss = 0.27464099
Iteration 58, loss = 0.27356992
Iteration 59, loss = 0.27249288
Iteration 60, loss = 0.27525314
Iteration 61, loss = 0.27380865
Iteration 62, loss = 0.27485916
Iteration 63, loss = 0.27271334
Iteration 64, loss = 0.27136260
Iteration 65, loss = 0.27146025
Iteration 66, loss = 0.27053969
Iteration 67, loss = 0.26951950
Iteration 68, loss = 0.27052476
Iteration 69, loss = 0.26994485
Iteration 70, loss = 0.27096696
Iteration 71, loss = 0.27005555
Iteration 72, loss = 0.26895143
Iteration 73, loss = 0.26966864
Iteration 74, loss = 0.26738209
Iteration 75, loss = 0.26792168
Iteration 76, loss = 0.26744369
Iteration 77, loss = 0.26711712
Iteration 78, loss = 0.26810492
Iteration 79, loss = 0.26866047
Iteration 80, loss = 0.26670870
Iteration 81, loss = 0.26693479
Iteration 82, loss = 0.26678606
Iteration 83, loss = 0.26804806
Iteration 84, loss = 0.26466832
Iteration 85, loss = 0.26618885
Iteration 86, loss = 0.26603503
Iteration 87, loss = 0.26559544
Iteration 88, loss = 0.26486568
Iteration 89, loss = 0.26547523
Iteration 90, loss = 0.26438216
Iteration 91, loss = 0.26470031
Iteration 92, loss = 0.26333754
Iteration 93, loss = 0.26399516
Iteration 94, loss = 0.26390705
Iteration 95, loss = 0.26287683
Iteration 96, loss = 0.26549335
Iteration 97, loss = 0.26249000
Iteration 98, loss = 0.26314616
Iteration 99, loss = 0.26278405
Iteration 100, loss = 0.26461203
Iteration 101, loss = 0.26251820
Iteration 102, loss = 0.26241246
Iteration 103, loss = 0.26210021
Iteration 104, loss = 0.26130027
Iteration 105, loss = 0.26230224
Iteration 106, loss = 0.26152624
Iteration 107, loss = 0.26278269
Iteration 108, loss = 0.26158353
Iteration 109, loss = 0.26105494
Iteration 110, loss = 0.26207168
Iteration 111, loss = 0.26204699
Iteration 112, loss = 0.26021013
Iteration 113, loss = 0.26128743
Iteration 114, loss = 0.26101471
Iteration 115, loss = 0.26174618
Iteration 116, loss = 0.26123488
Iteration 117, loss = 0.25985225
Iteration 118, loss = 0.25996125
Iteration 119, loss = 0.26018725
Iteration 120, loss = 0.25979424
Iteration 121, loss = 0.26073798
Iteration 122, loss = 0.25939215
Iteration 123, loss = 0.25969997
Iteration 124, loss = 0.26021257
Iteration 125, loss = 0.25760286
Iteration 126, loss = 0.25869178
Iteration 127, loss = 0.26058484
Iteration 128, loss = 0.25883408
Iteration 129, loss = 0.25878859
Iteration 130, loss = 0.25836224
Iteration 131, loss = 0.25954203
Iteration 132, loss = 0.25758202
Iteration 133, loss = 0.25988425
Iteration 134, loss = 0.25807691
Iteration 135, loss = 0.25759929
Iteration 136, loss = 0.25897609
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.89974268
Iteration 2, loss = 0.68178445
Iteration 3, loss = 0.56330486
Iteration 4, loss = 0.49323290
Iteration 5, loss = 0.45703697
Iteration 6, loss = 0.43438055
Iteration 7, loss = 0.41293041
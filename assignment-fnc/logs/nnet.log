Namespace(model='nnet', skip_preprocess=True, train_prop=1)

Training a nnet model

Iteration 1, loss = 0.55492023
Iteration 2, loss = 0.44427097
Iteration 3, loss = 0.41363167
Iteration 4, loss = 0.39410589
Iteration 5, loss = 0.37941627
Iteration 6, loss = 0.36770461
Iteration 7, loss = 0.35718331
Iteration 8, loss = 0.35066793
Iteration 9, loss = 0.34397630
Iteration 10, loss = 0.33734509
Iteration 11, loss = 0.33121306
Iteration 12, loss = 0.32767313
Iteration 13, loss = 0.32325744
Iteration 14, loss = 0.32071715
Iteration 15, loss = 0.31821523
Iteration 16, loss = 0.31416242
Iteration 17, loss = 0.31212322
Iteration 18, loss = 0.30981930
Iteration 19, loss = 0.30616747
Iteration 20, loss = 0.30348622
Iteration 21, loss = 0.30359568
Iteration 22, loss = 0.30102353
Iteration 23, loss = 0.29888949
Iteration 24, loss = 0.29716453
Iteration 25, loss = 0.29547215
Iteration 26, loss = 0.29527249
Iteration 27, loss = 0.29353185
Iteration 28, loss = 0.29294824
Iteration 29, loss = 0.29149211
Iteration 30, loss = 0.28924950
Iteration 31, loss = 0.28906244
Iteration 32, loss = 0.28848651
Iteration 33, loss = 0.28748478
Iteration 34, loss = 0.28689473
Iteration 35, loss = 0.28515157
Iteration 36, loss = 0.28600929
Iteration 37, loss = 0.28448681
Iteration 38, loss = 0.28294069
Iteration 39, loss = 0.28315309
Iteration 40, loss = 0.28350752
Iteration 41, loss = 0.28169916
Iteration 42, loss = 0.28115212
Iteration 43, loss = 0.27988304
Iteration 44, loss = 0.27950140
Iteration 45, loss = 0.27741885
Iteration 46, loss = 0.27856044
Iteration 47, loss = 0.27698590
Iteration 48, loss = 0.27878457
Iteration 49, loss = 0.27733434
Iteration 50, loss = 0.27674047
Iteration 51, loss = 0.27611628
Iteration 52, loss = 0.27667712
Iteration 53, loss = 0.27667299
Iteration 54, loss = 0.27509081
Iteration 55, loss = 0.27551161
Iteration 56, loss = 0.27441301
Iteration 57, loss = 0.27357475
Iteration 58, loss = 0.27309632
Iteration 59, loss = 0.27323763
Iteration 60, loss = 0.27359890
Iteration 61, loss = 0.27271451
Iteration 62, loss = 0.27250061
Iteration 63, loss = 0.27140399
Iteration 64, loss = 0.27224513
Iteration 65, loss = 0.27145642
Iteration 66, loss = 0.27146192
Iteration 67, loss = 0.27097694
Iteration 68, loss = 0.27031676
Iteration 69, loss = 0.27163653
Iteration 70, loss = 0.26931611
Iteration 71, loss = 0.27004446
Iteration 72, loss = 0.26977984
Iteration 73, loss = 0.26949372
Iteration 74, loss = 0.26996382
Iteration 75, loss = 0.26879371
Iteration 76, loss = 0.26920602
Iteration 77, loss = 0.26891261
Iteration 78, loss = 0.26941467
Iteration 79, loss = 0.26931749
Iteration 80, loss = 0.26790725
Iteration 81, loss = 0.26765104
Iteration 82, loss = 0.26846766
Iteration 83, loss = 0.26831777
Iteration 84, loss = 0.26783615
Iteration 85, loss = 0.26777282
Iteration 86, loss = 0.26669186
Iteration 87, loss = 0.26663556
Iteration 88, loss = 0.26753613
Iteration 89, loss = 0.26675477
Iteration 90, loss = 0.26604023
Iteration 91, loss = 0.26617155
Iteration 92, loss = 0.26642021
Iteration 93, loss = 0.26518544
Iteration 94, loss = 0.26665525
Iteration 95, loss = 0.26534657
Iteration 96, loss = 0.26581893
Iteration 97, loss = 0.26529242
Iteration 98, loss = 0.26520185
Iteration 99, loss = 0.26590997
Iteration 100, loss = 0.26447071
Iteration 101, loss = 0.26451121
Iteration 102, loss = 0.26601530
Iteration 103, loss = 0.26399376
Iteration 104, loss = 0.26433640
Iteration 105, loss = 0.26415057
Iteration 106, loss = 0.26435470
Iteration 107, loss = 0.26422543
Iteration 108, loss = 0.26457520
Iteration 109, loss = 0.26373094
Iteration 110, loss = 0.26378452
Iteration 111, loss = 0.26364129
Iteration 112, loss = 0.26411415
Iteration 113, loss = 0.26299684
Iteration 114, loss = 0.26427220
Iteration 115, loss = 0.26344036
Iteration 116, loss = 0.26358884
Iteration 117, loss = 0.26178825
Iteration 118, loss = 0.26252310
Iteration 119, loss = 0.26314573
Iteration 120, loss = 0.26265868
Iteration 121, loss = 0.26273695
Iteration 122, loss = 0.26187753
Iteration 123, loss = 0.26205755
Iteration 124, loss = 0.26240359
Iteration 125, loss = 0.26207340
Iteration 126, loss = 0.26057567
Iteration 127, loss = 0.26153602
Iteration 128, loss = 0.26283326
Iteration 129, loss = 0.26120968
Iteration 130, loss = 0.26194781
Iteration 131, loss = 0.26064260
Iteration 132, loss = 0.26162283
Iteration 133, loss = 0.26107286
Iteration 134, loss = 0.26123214
Iteration 135, loss = 0.26126858
Iteration 136, loss = 0.26053354
Iteration 137, loss = 0.26153349
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Training time took 766.1473081111908 seconds

Trained a model using MLPClassifier(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(120, 120, 120), learning_rate='adaptive',
              learning_rate_init=0.001, max_iter=200, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=123, shuffle=True, solver='adam', tol=0.0001,
              validation_fraction=0.1, verbose=True, warm_start=False)
Train FNC Score:
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |   2496    |    88     |    528    |    566    |
-------------------------------------------------------------
| disagree  |    70     |    480    |    137    |    153    |
-------------------------------------------------------------
|  discuss  |    267    |    67     |   7619    |    956    |
-------------------------------------------------------------
| unrelated |    181    |    49     |    950    |   35365   |
-------------------------------------------------------------
Score: 19725.5 out of 22563.25	(87.4231327490499%)
Train F1 Score: 0.7942427684753621
Train Accuracy Score: 0.9197150404226366

Test FNC Score:
-------------------------------------------------------------
|           |   agree   | disagree  |  discuss  | unrelated |
-------------------------------------------------------------
|   agree   |    301    |    15     |    894    |    693    |
-------------------------------------------------------------
| disagree  |    126    |     1     |    252    |    318    |
-------------------------------------------------------------
|  discuss  |    909    |    32     |   2090    |   1433    |
-------------------------------------------------------------
| unrelated |    420    |     6     |   1231    |   16692   |
-------------------------------------------------------------
Score: 7122.0 out of 11651.25	(61.12648857418732%)
Test F1 Score: 0.3814544690441207
Test Accuracy Score: 0.7509542360209341
Plotting learning plots...
Iteration 1, loss = 0.71670228
Iteration 2, loss = 0.50190046
Iteration 3, loss = 0.45613001
Iteration 4, loss = 0.43416312
Iteration 5, loss = 0.41669320
Iteration 6, loss = 0.39897683
Iteration 7, loss = 0.39030302
Iteration 8, loss = 0.37818618
Iteration 9, loss = 0.37051689
Iteration 10, loss = 0.35580336
Iteration 11, loss = 0.35208409
Iteration 12, loss = 0.34535133
Iteration 13, loss = 0.34140898
Iteration 14, loss = 0.33192640
Iteration 15, loss = 0.33010526
Iteration 16, loss = 0.31842151
Iteration 17, loss = 0.31581047
Iteration 18, loss = 0.31054394
Iteration 19, loss = 0.30593579
Iteration 20, loss = 0.29839919
Iteration 21, loss = 0.29664402
Iteration 22, loss = 0.29395393
Iteration 23, loss = 0.29301901
Iteration 24, loss = 0.28751037
Iteration 25, loss = 0.28303189
Iteration 26, loss = 0.28391674
Iteration 27, loss = 0.28022667
Iteration 28, loss = 0.27273767
Iteration 29, loss = 0.27452284
Iteration 30, loss = 0.27208062
Iteration 31, loss = 0.26538485
Iteration 32, loss = 0.26590316
Iteration 33, loss = 0.26314028
Iteration 34, loss = 0.25981198
Iteration 35, loss = 0.25943631
Iteration 36, loss = 0.25646983
Iteration 37, loss = 0.25352015
Iteration 38, loss = 0.25069733
Iteration 39, loss = 0.25052366
Iteration 40, loss = 0.24773987
Iteration 41, loss = 0.24670487
Iteration 42, loss = 0.24513537
Iteration 43, loss = 0.24469674
Iteration 44, loss = 0.24103860
Iteration 45, loss = 0.24101093
Iteration 46, loss = 0.24119082
Iteration 47, loss = 0.23905164
Iteration 48, loss = 0.23671126
Iteration 49, loss = 0.23355784
Iteration 50, loss = 0.23730589
Iteration 51, loss = 0.23264228
Iteration 52, loss = 0.23249285
Iteration 53, loss = 0.23101398
Iteration 54, loss = 0.23125353
Iteration 55, loss = 0.22865657
Iteration 56, loss = 0.22927523
Iteration 57, loss = 0.23087486
Iteration 58, loss = 0.22388651
Iteration 59, loss = 0.22360091
Iteration 60, loss = 0.22859696
Iteration 61, loss = 0.22402414
Iteration 62, loss = 0.22324584
Iteration 63, loss = 0.22330312
Iteration 64, loss = 0.22061503
Iteration 65, loss = 0.22313936
Iteration 66, loss = 0.22164210
Iteration 67, loss = 0.22267838
Iteration 68, loss = 0.21763227
Iteration 69, loss = 0.22080595
Iteration 70, loss = 0.21964038
Iteration 71, loss = 0.21651693
Iteration 72, loss = 0.21320909
Iteration 73, loss = 0.21490960
Iteration 74, loss = 0.21260251
Iteration 75, loss = 0.21395410
Iteration 76, loss = 0.21512107
Iteration 77, loss = 0.21254442
Iteration 78, loss = 0.21040619
Iteration 79, loss = 0.21254984
Iteration 80, loss = 0.21273573
Iteration 81, loss = 0.20744372
Iteration 82, loss = 0.21319562
Iteration 83, loss = 0.21147242
Iteration 84, loss = 0.20380597
Iteration 85, loss = 0.20483571
Iteration 86, loss = 0.20929440
Iteration 87, loss = 0.20363116
Iteration 88, loss = 0.20450433
Iteration 89, loss = 0.20554654
Iteration 90, loss = 0.20912568
Iteration 91, loss = 0.20363192
Iteration 92, loss = 0.20451261
Iteration 93, loss = 0.20645132
Iteration 94, loss = 0.20002115
Iteration 95, loss = 0.20327205
Iteration 96, loss = 0.20331525
Iteration 97, loss = 0.20387433
Iteration 98, loss = 0.19862977
Iteration 99, loss = 0.20086680
Iteration 100, loss = 0.19632530
Iteration 101, loss = 0.20580210
Iteration 102, loss = 0.20459887
Iteration 103, loss = 0.20277366
Iteration 104, loss = 0.20048732
Iteration 105, loss = 0.19884805
Iteration 106, loss = 0.19734968
Iteration 107, loss = 0.19724186
Iteration 108, loss = 0.20316208
Iteration 109, loss = 0.20161819
Iteration 110, loss = 0.19661988
Iteration 111, loss = 0.19960224
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.91477919
Iteration 2, loss = 0.69703531
Iteration 3, loss = 0.57949695
Iteration 4, loss = 0.50922807
Iteration 5, loss = 0.46328796
Iteration 6, loss = 0.43918528
Iteration 7, loss = 0.41821730
Iteration 8, loss = 0.39671547
Iteration 9, loss = 0.38376608
Iteration 10, loss = 0.36900044
Iteration 11, loss = 0.35437793
Iteration 12, loss = 0.34440320
Iteration 13, loss = 0.33312853
Iteration 14, loss = 0.31891323
Iteration 15, loss = 0.31254557
Iteration 16, loss = 0.31206090
Iteration 17, loss = 0.29630620
Iteration 18, loss = 0.29812030
Iteration 19, loss = 0.28444676
Iteration 20, loss = 0.27187415
Iteration 21, loss = 0.26886523
Iteration 22, loss = 0.26378853
Iteration 23, loss = 0.25811646
Iteration 24, loss = 0.25956572
Iteration 25, loss = 0.25264748
Iteration 26, loss = 0.24398184
Iteration 27, loss = 0.24005002
Iteration 28, loss = 0.24020695
Iteration 29, loss = 0.23677817
Iteration 30, loss = 0.23908598
Iteration 31, loss = 0.23259445
Iteration 32, loss = 0.22931133
Iteration 33, loss = 0.22591726
Iteration 34, loss = 0.21374738
Iteration 35, loss = 0.21056414
Iteration 36, loss = 0.20625890
Iteration 37, loss = 0.20398199
Iteration 38, loss = 0.20794595
Iteration 39, loss = 0.20186824
Iteration 40, loss = 0.20713216
Iteration 41, loss = 0.19773629
Iteration 42, loss = 0.19678318
Iteration 43, loss = 0.19616673
Iteration 44, loss = 0.19164098
Iteration 45, loss = 0.18991904
Iteration 46, loss = 0.18903370
Iteration 47, loss = 0.18954825
Iteration 48, loss = 0.19186114
Iteration 49, loss = 0.18359801
Iteration 50, loss = 0.18238052
Iteration 51, loss = 0.18165551
Iteration 52, loss = 0.17938603
Iteration 53, loss = 0.17870165
Iteration 54, loss = 0.17668728
Iteration 55, loss = 0.17527307
Iteration 56, loss = 0.17399689
Iteration 57, loss = 0.17589134
Iteration 58, loss = 0.17849734
Iteration 59, loss = 0.17362688
Iteration 60, loss = 0.17124105
Iteration 61, loss = 0.16875975
Iteration 62, loss = 0.17163217
Iteration 63, loss = 0.16964994
Iteration 64, loss = 0.16668424
Iteration 65, loss = 0.16675057
Iteration 66, loss = 0.17035110
Iteration 67, loss = 0.16698315
Iteration 68, loss = 0.15995128
Iteration 69, loss = 0.16287301
Iteration 70, loss = 0.16380379
Iteration 71, loss = 0.15910811
Iteration 72, loss = 0.15996938
Iteration 73, loss = 0.16189314
Iteration 74, loss = 0.16139345
Iteration 75, loss = 0.15717135
Iteration 76, loss = 0.15569131
Iteration 77, loss = 0.16433773
Iteration 78, loss = 0.17637179
Iteration 79, loss = 0.17432106
Iteration 80, loss = 0.16834400
Iteration 81, loss = 0.15785156
Iteration 82, loss = 0.15729316
Iteration 83, loss = 0.14983792
Iteration 84, loss = 0.15167256
Iteration 85, loss = 0.15123915
Iteration 86, loss = 0.15625449
Iteration 87, loss = 0.15537357
Iteration 88, loss = 0.15055219
Iteration 89, loss = 0.15056184
Iteration 90, loss = 0.15381656
Iteration 91, loss = 0.14941531
Iteration 92, loss = 0.15280287
Iteration 93, loss = 0.14508834
Iteration 94, loss = 0.14538286
Iteration 95, loss = 0.14519725
Iteration 96, loss = 0.15261154
Iteration 97, loss = 0.15432010
Iteration 98, loss = 0.15518114
Iteration 99, loss = 0.14956065
Iteration 100, loss = 0.14287542
Iteration 101, loss = 0.14770725
Iteration 102, loss = 0.15357928
Iteration 103, loss = 0.15382038
Iteration 104, loss = 0.15428981
Iteration 105, loss = 0.14086557
Iteration 106, loss = 0.13907089
Iteration 107, loss = 0.14451675
Iteration 108, loss = 0.14271697
Iteration 109, loss = 0.14939001
Iteration 110, loss = 0.14164646
Iteration 111, loss = 0.14207605
Iteration 112, loss = 0.13916286
Iteration 113, loss = 0.13826109
Iteration 114, loss = 0.13564532
Iteration 115, loss = 0.13707314
Iteration 116, loss = 0.13858151
Iteration 117, loss = 0.13712935
Iteration 118, loss = 0.14351292
Iteration 119, loss = 0.14463139
Iteration 120, loss = 0.13856093
Iteration 121, loss = 0.13738787
Iteration 122, loss = 0.14001193
Iteration 123, loss = 0.13787302
Iteration 124, loss = 0.13657888
Iteration 125, loss = 0.14665306
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73499297
Iteration 2, loss = 0.51581944
Iteration 3, loss = 0.47122558
Iteration 4, loss = 0.44870911
Iteration 5, loss = 0.42649110
Iteration 6, loss = 0.41232267
Iteration 7, loss = 0.40171243
Iteration 8, loss = 0.38990782
Iteration 9, loss = 0.37944132
Iteration 10, loss = 0.37047062
Iteration 11, loss = 0.35999931
Iteration 12, loss = 0.35304484
Iteration 13, loss = 0.34895550
Iteration 14, loss = 0.34240465
Iteration 15, loss = 0.33550257Iteration 1, loss = 0.89062668
Iteration 2, loss = 0.68865489
Iteration 3, loss = 0.57880614
Iteration 4, loss = 0.50593152
Iteration 5, loss = 0.46414251
Iteration 6, loss = 0.43912634
Iteration 7, loss = 0.41507698
Iteration 8, loss = 0.39605991
Iteration 9, loss = 0.38195941
Iteration 10, loss = 0.36784425
Iteration 11, loss = 0.35447070
Iteration 12, loss = 0.34613713
Iteration 13, loss = 0.33377047
Iteration 14, loss = 0.32269692
Iteration 15, loss = 0.31340259
Iteration 16, loss = 0.30772210
Iteration 17, loss = 0.30643781
Iteration 18, loss = 0.29528130
Iteration 19, loss = 0.29201863
Iteration 20, loss = 0.28084231
Iteration 21, loss = 0.27856233
Iteration 22, loss = 0.26738644
Iteration 23, loss = 0.26331319
Iteration 24, loss = 0.25425777
Iteration 25, loss = 0.25198552
Iteration 26, loss = 0.24930138
Iteration 27, loss = 0.24660533
Iteration 28, loss = 0.24518784
Iteration 29, loss = 0.24667986
Iteration 30, loss = 0.23670946
Iteration 31, loss = 0.22995493
Iteration 32, loss = 0.22782122
Iteration 33, loss = 0.22167593
Iteration 34, loss = 0.22252675
Iteration 35, loss = 0.21416645
Iteration 36, loss = 0.21507602
Iteration 37, loss = 0.21717567
Iteration 38, loss = 0.21097418
Iteration 39, loss = 0.21286586
Iteration 40, loss = 0.20576351
Iteration 41, loss = 0.20302161
Iteration 42, loss = 0.20432022
Iteration 43, loss = 0.20457991
Iteration 44, loss = 0.20673160
Iteration 45, loss = 0.20705639
Iteration 46, loss = 0.20037852
Iteration 47, loss = 0.18909714
Iteration 48, loss = 0.20375419
Iteration 49, loss = 0.19847754
Iteration 50, loss = 0.19302549
Iteration 51, loss = 0.18907625
Iteration 52, loss = 0.18864359
Iteration 53, loss = 0.18614993
Iteration 54, loss = 0.18290588
Iteration 55, loss = 0.18443183
Iteration 56, loss = 0.18893214
Iteration 57, loss = 0.18418770
Iteration 58, loss = 0.18151277
Iteration 59, loss = 0.18402338
Iteration 60, loss = 0.18235055
Iteration 61, loss = 0.18182755
Iteration 62, loss = 0.18018676
Iteration 63, loss = 0.17316003
Iteration 64, loss = 0.17700848
Iteration 65, loss = 0.17331123
Iteration 66, loss = 0.17851255
Iteration 67, loss = 0.17522373
Iteration 68, loss = 0.17562219
Iteration 69, loss = 0.17554864
Iteration 70, loss = 0.17248814
Iteration 71, loss = 0.16741707
Iteration 72, loss = 0.17054128
Iteration 73, loss = 0.16923834
Iteration 74, loss = 0.16710554
Iteration 75, loss = 0.16902109
Iteration 76, loss = 0.17263576
Iteration 77, loss = 0.17293777
Iteration 78, loss = 0.17762437
Iteration 79, loss = 0.17041477
Iteration 80, loss = 0.16918425
Iteration 81, loss = 0.16483363
Iteration 82, loss = 0.16490873
Iteration 83, loss = 0.16152393
Iteration 84, loss = 0.16386469
Iteration 85, loss = 0.17098679
Iteration 86, loss = 0.16418568
Iteration 87, loss = 0.16242178
Iteration 88, loss = 0.16192341
Iteration 89, loss = 0.15750097
Iteration 90, loss = 0.15756894
Iteration 91, loss = 0.15314135
Iteration 92, loss = 0.15253051
Iteration 93, loss = 0.16200719
Iteration 94, loss = 0.15686524
Iteration 95, loss = 0.15857950
Iteration 96, loss = 0.15212718
Iteration 97, loss = 0.15407583
Iteration 98, loss = 0.15348476
Iteration 99, loss = 0.15945502
Iteration 100, loss = 0.15451450
Iteration 101, loss = 0.15202157
Iteration 102, loss = 0.14785026
Iteration 103, loss = 0.15082676
Iteration 104, loss = 0.15097902
Iteration 105, loss = 0.14864115
Iteration 106, loss = 0.15206952
Iteration 107, loss = 0.14633622
Iteration 108, loss = 0.14709536
Iteration 109, loss = 0.14590059
Iteration 110, loss = 0.14557205
Iteration 111, loss = 0.14826869
Iteration 112, loss = 0.14783958
Iteration 113, loss = 0.14559620
Iteration 114, loss = 0.14823568
Iteration 115, loss = 0.14708532
Iteration 116, loss = 0.14800583
Iteration 117, loss = 0.14401014
Iteration 118, loss = 0.14372459
Iteration 119, loss = 0.14410714
Iteration 120, loss = 0.14074071
Iteration 121, loss = 0.14331745
Iteration 122, loss = 0.14008984
Iteration 123, loss = 0.14428464
Iteration 124, loss = 0.14701723
Iteration 125, loss = 0.14745921
Iteration 126, loss = 0.14128728
Iteration 127, loss = 0.14069404
Iteration 128, loss = 0.14211873
Iteration 129, loss = 0.14057976
Iteration 130, loss = 0.13765791
Iteration 131, loss = 0.13926273
Iteration 132, loss = 0.14053352
Iteration 133, loss = 0.14369935
Iteration 134, loss = 0.14546523
Iteration 135, loss = 0.13878357
Iteration 136, loss = 0.13503067
Iteration 137, loss = 0.13636463
Iteration 138, loss = 0.13483879
Iteration 139, loss = 0.13502037
Iteration 140, loss = 0.13837119
Iteration 141, loss = 0.14068062
Iteration 142, loss = 0.13820486
Iteration 143, loss = 0.13339523
Iteration 144, loss = 0.13370565
Iteration 145, loss = 0.13462993
Iteration 146, loss = 0.13802052
Iteration 147, loss = 0.13721346
Iteration 148, loss = 0.13761994
Iteration 149, loss = 0.13557853
Iteration 150, loss = 0.13308510
Iteration 151, loss = 0.13524961
Iteration 152, loss = 0.13309680
Iteration 153, loss = 0.13120686
Iteration 154, loss = 0.13733496
Iteration 155, loss = 0.13549148
Iteration 156, loss = 0.13646009
Iteration 157, loss = 0.14101480
Iteration 158, loss = 0.13968161
Iteration 159, loss = 0.14630744
Iteration 160, loss = 0.13955534
Iteration 161, loss = 0.13780765
Iteration 162, loss = 0.13598964
Iteration 163, loss = 0.12779370
Iteration 164, loss = 0.12417894
Iteration 165, loss = 0.12135908
Iteration 166, loss = 0.12448016
Iteration 167, loss = 0.12748922
Iteration 168, loss = 0.12785629
Iteration 169, loss = 0.12685593
Iteration 170, loss = 0.12261054
Iteration 171, loss = 0.12777252
Iteration 172, loss = 0.13255394
Iteration 173, loss = 0.13286458
Iteration 174, loss = 0.13307461
Iteration 175, loss = 0.12978411
Iteration 176, loss = 0.12491568
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.58487708
Iteration 2, loss = 0.45588491
Iteration 3, loss = 0.42419867
Iteration 4, loss = 0.40196877
Iteration 5, loss = 0.38752911
Iteration 6, loss = 0.37523002
Iteration 7, loss = 0.36449330
Iteration 8, loss = 0.35578697
Iteration 9, loss = 0.34975540
Iteration 10, loss = 0.34215362
Iteration 11, loss = 0.33743789
Iteration 12, loss = 0.33071477
Iteration 13, loss = 0.32941740
Iteration 14, loss = 0.32586834
Iteration 15, loss = 0.31970412
Iteration 16, loss = 0.31704760
Iteration 17, loss = 0.31410705
Iteration 18, loss = 0.30944754
Iteration 19, loss = 0.30850870
Iteration 20, loss = 0.30783679
Iteration 21, loss = 0.30328048
Iteration 22, loss = 0.30126940
Iteration 23, loss = 0.29885843
Iteration 24, loss = 0.29753664
Iteration 25, loss = 0.29996505
Iteration 26, loss = 0.29514983
Iteration 27, loss = 0.29341006
Iteration 28, loss = 0.29231869
Iteration 29, loss = 0.29124821
Iteration 30, loss = 0.29011975
Iteration 31, loss = 0.28877261
Iteration 32, loss = 0.28698400
Iteration 33, loss = 0.28648926
Iteration 34, loss = 0.28707171
Iteration 35, loss = 0.28353285
Iteration 36, loss = 0.28362216
Iteration 37, loss = 0.28285794
Iteration 38, loss = 0.28282931
Iteration 39, loss = 0.28205405
Iteration 40, loss = 0.28028703
Iteration 41, loss = 0.28041290
Iteration 42, loss = 0.27890183
Iteration 43, loss = 0.28065378
Iteration 44, loss = 0.27782408
Iteration 45, loss = 0.27883355
Iteration 46, loss = 0.27606389
Iteration 47, loss = 0.27734787
Iteration 48, loss = 0.27656098
Iteration 49, loss = 0.27510823
Iteration 50, loss = 0.27516491
Iteration 51, loss = 0.27315776
Iteration 52, loss = 0.27382566
Iteration 53, loss = 0.27216748
Iteration 54, loss = 0.27162368
Iteration 55, loss = 0.27391558
Iteration 56, loss = 0.27219215
Iteration 57, loss = 0.27138619
Iteration 58, loss = 0.27178381
Iteration 59, loss = 0.27112663
Iteration 60, loss = 0.26972779
Iteration 61, loss = 0.26974895
Iteration 62, loss = 0.26903921
Iteration 63, loss = 0.26960001
Iteration 64, loss = 0.26927864
Iteration 65, loss = 0.26800054
Iteration 66, loss = 0.26794840
Iteration 67, loss = 0.26709742
Iteration 68, loss = 0.26655677
Iteration 69, loss = 0.26733618
Iteration 70, loss = 0.26640105
Iteration 71, loss = 0.26588928
Iteration 72, loss = 0.26497717
Iteration 73, loss = 0.26498880
Iteration 74, loss = 0.26483190
Iteration 75, loss = 0.26604193
Iteration 76, loss = 0.26494046
Iteration 16, loss = 0.33092098
Iteration 17, loss = 0.32471797
Iteration 18, loss = 0.32015169
Iteration 19, loss = 0.31922469
Iteration 20, loss = 0.31430771
Iteration 21, loss = 0.30827649
Iteration 22, loss = 0.30528481
Iteration 23, loss = 0.29962936
Iteration 24, loss = 0.29870652
Iteration 25, loss = 0.29538942
Iteration 26, loss = 0.29065054
Iteration 27, loss = 0.28782272
Iteration 28, loss = 0.28538873
Iteration 29, loss = 0.28528367
Iteration 30, loss = 0.28361828
Iteration 31, loss = 0.28103713
Iteration 32, loss = 0.27784828
Iteration 33, loss = 0.27692865
Iteration 34, loss = 0.27173803
Iteration 35, loss = 0.26895498
Iteration 36, loss = 0.27481026
Iteration 37, loss = 0.26716603
Iteration 38, loss = 0.26612708
Iteration 39, loss = 0.26514525
Iteration 40, loss = 0.26019867
Iteration 41, loss = 0.26273306
Iteration 42, loss = 0.25905886
Iteration 43, loss = 0.25993169
Iteration 44, loss = 0.25705828
Iteration 45, loss = 0.25423603
Iteration 46, loss = 0.25329315
Iteration 47, loss = 0.25105387
Iteration 48, loss = 0.25428512
Iteration 49, loss = 0.24655462
Iteration 50, loss = 0.24322858
Iteration 51, loss = 0.24262908
Iteration 52, loss = 0.24312162
Iteration 53, loss = 0.24417758
Iteration 54, loss = 0.23908243
Iteration 55, loss = 0.24196319
Iteration 56, loss = 0.24285958
Iteration 57, loss = 0.24113970
Iteration 58, loss = 0.23806581
Iteration 59, loss = 0.23469387
Iteration 60, loss = 0.23354280
Iteration 61, loss = 0.23035175
Iteration 62, loss = 0.23921494
Iteration 63, loss = 0.23461854
Iteration 64, loss = 0.23062719
Iteration 65, loss = 0.23051981
Iteration 66, loss = 0.22875908
Iteration 67, loss = 0.23201173
Iteration 68, loss = 0.22589269
Iteration 69, loss = 0.22886387
Iteration 70, loss = 0.22615784
Iteration 71, loss = 0.22363521
Iteration 72, loss = 0.22412946
Iteration 73, loss = 0.22161323
Iteration 74, loss = 0.22047717
Iteration 75, loss = 0.22328102
Iteration 76, loss = 0.22259084
Iteration 77, loss = 0.22274933
Iteration 78, loss = 0.22172983
Iteration 79, loss = 0.22256788
Iteration 80, loss = 0.21854630
Iteration 81, loss = 0.22229435
Iteration 82, loss = 0.22109530
Iteration 83, loss = 0.21482216
Iteration 84, loss = 0.21641119
Iteration 85, loss = 0.21604727
Iteration 86, loss = 0.21721421
Iteration 87, loss = 0.21583065
Iteration 88, loss = 0.21304032
Iteration 89, loss = 0.21831444
Iteration 90, loss = 0.21443300
Iteration 91, loss = 0.21118603
Iteration 92, loss = 0.21134291
Iteration 93, loss = 0.21318228
Iteration 94, loss = 0.20863931
Iteration 95, loss = 0.21015569
Iteration 96, loss = 0.21005337
Iteration 97, loss = 0.21335093
Iteration 98, loss = 0.21333686
Iteration 99, loss = 0.21083315
Iteration 100, loss = 0.21122432
Iteration 101, loss = 0.20789137
Iteration 102, loss = 0.20977394
Iteration 103, loss = 0.20515237
Iteration 104, loss = 0.20698963
Iteration 105, loss = 0.20661754
Iteration 106, loss = 0.20730005
Iteration 107, loss = 0.20548977
Iteration 108, loss = 0.20571297
Iteration 109, loss = 0.20537906
Iteration 110, loss = 0.20703819
Iteration 111, loss = 0.20364019
Iteration 112, loss = 0.20683830
Iteration 113, loss = 0.20717953
Iteration 114, loss = 0.21499012
Iteration 115, loss = 0.20669915
Iteration 116, loss = 0.20587954
Iteration 117, loss = 0.20053583
Iteration 118, loss = 0.20275302
Iteration 119, loss = 0.20494194
Iteration 120, loss = 0.19855123
Iteration 121, loss = 0.19926168
Iteration 122, loss = 0.19931594
Iteration 123, loss = 0.20099477
Iteration 124, loss = 0.20021793
Iteration 125, loss = 0.19686419
Iteration 126, loss = 0.19829243
Iteration 127, loss = 0.20070589
Iteration 128, loss = 0.20191408
Iteration 129, loss = 0.20121915
Iteration 130, loss = 0.20110655
Iteration 131, loss = 0.19700888
Iteration 132, loss = 0.19644523
Iteration 133, loss = 0.20017268
Iteration 134, loss = 0.19956575
Iteration 135, loss = 0.19626780
Iteration 136, loss = 0.19811468
Iteration 137, loss = 0.19913510
Iteration 138, loss = 0.19714920
Iteration 139, loss = 0.19632436
Iteration 140, loss = 0.19492908
Iteration 141, loss = 0.19661502
Iteration 142, loss = 0.19640424
Iteration 143, loss = 0.20012836
Iteration 144, loss = 0.19652430
Iteration 145, loss = 0.19834444
Iteration 146, loss = 0.19440452
Iteration 147, loss = 0.19642355
Iteration 148, loss = 0.19384057
Iteration 149, loss = 0.19458118
Iteration 150, loss = 0.19820932
Iteration 151, loss = 0.19580854
Iteration 152, loss = 0.20113309
Iteration 153, loss = 0.19671230
Iteration 154, loss = 0.19522483
Iteration 155, loss = 0.19098250
Iteration 156, loss = 0.19143558
Iteration 157, loss = 0.19516519
Iteration 158, loss = 0.19372559
Iteration 159, loss = 0.19039821
Iteration 160, loss = 0.19397792
Iteration 161, loss = 0.19283089
Iteration 162, loss = 0.19129312
Iteration 163, loss = 0.19319935
Iteration 164, loss = 0.19193321
Iteration 165, loss = 0.19073206
Iteration 166, loss = 0.19225694
Iteration 167, loss = 0.19028046
Iteration 168, loss = 0.18954370
Iteration 169, loss = 0.18840327
Iteration 170, loss = 0.18888564
Iteration 171, loss = 0.19007127
Iteration 172, loss = 0.18994334
Iteration 173, loss = 0.19147219
Iteration 174, loss = 0.19049624
Iteration 175, loss = 0.19092173
Iteration 176, loss = 0.18944289
Iteration 177, loss = 0.18814972
Iteration 178, loss = 0.19003542
Iteration 179, loss = 0.18782001
Iteration 180, loss = 0.18866357
Iteration 181, loss = 0.20432340
Iteration 182, loss = 0.18829177
Iteration 183, loss = 0.19064218
Iteration 184, loss = 0.19027260
Iteration 185, loss = 0.19218817
Iteration 186, loss = 0.18728634
Iteration 187, loss = 0.18892614
Iteration 188, loss = 0.18746844
Iteration 189, loss = 0.18653906
Iteration 190, loss = 0.18887291
Iteration 191, loss = 0.18745608
Iteration 192, loss = 0.18467339
Iteration 193, loss = 0.18801233
Iteration 194, loss = 0.18637231
Iteration 195, loss = 0.18465973
Iteration 196, loss = 0.19211096
Iteration 197, loss = 0.18682792
Iteration 198, loss = 0.18738275
Iteration 199, loss = 0.18745100
Iteration 200, loss = 0.18672483
Iteration 1, loss = 0.60631361
Iteration 2, loss = 0.46610747
Iteration 3, loss = 0.43430102
Iteration 4, loss = 0.41327813
Iteration 5, loss = 0.39551889
Iteration 6, loss = 0.38301008
Iteration 7, loss = 0.37680423
Iteration 8, loss = 0.36631950
Iteration 9, loss = 0.35674223
Iteration 10, loss = 0.35090898
Iteration 11, loss = 0.34388235
Iteration 12, loss = 0.34007360
Iteration 13, loss = 0.33687868
Iteration 14, loss = 0.32891545
Iteration 15, loss = 0.32657490
Iteration 16, loss = 0.32107117
Iteration 17, loss = 0.32012221
Iteration 18, loss = 0.31506115
Iteration 19, loss = 0.31399851
Iteration 20, loss = 0.31137428
Iteration 21, loss = 0.30681579
Iteration 22, loss = 0.30508242
Iteration 23, loss = 0.30406646
Iteration 24, loss = 0.30215355
Iteration 25, loss = 0.30061118
Iteration 26, loss = 0.29644745
Iteration 27, loss = 0.29631839
Iteration 28, loss = 0.29534580
Iteration 29, loss = 0.29157253
Iteration 30, loss = 0.29030691
Iteration 31, loss = 0.28850982
Iteration 32, loss = 0.28903989
Iteration 33, loss = 0.28565478
Iteration 34, loss = 0.28455061
Iteration 35, loss = 0.28598381
Iteration 36, loss = 0.28259327
Iteration 37, loss = 0.28298305
Iteration 38, loss = 0.27953260
Iteration 39, loss = 0.28026588
Iteration 40, loss = 0.28081038
Iteration 41, loss = 0.27940609
Iteration 42, loss = 0.27802175
Iteration 43, loss = 0.27740057
Iteration 44, loss = 0.27409268
Iteration 45, loss = 0.27459674
Iteration 46, loss = 0.27532138
Iteration 47, loss = 0.27374033
Iteration 48, loss = 0.27299241
Iteration 49, loss = 0.27229832
Iteration 50, loss = 0.27027587
Iteration 51, loss = 0.27270933
Iteration 52, loss = 0.27025538
Iteration 53, loss = 0.27028950
Iteration 54, loss = 0.26882643
Iteration 55, loss = 0.26954504
Iteration 56, loss = 0.26598702
Iteration 57, loss = 0.26736334
Iteration 58, loss = 0.26876007
Iteration 59, loss = 0.26627503
Iteration 60, loss = 0.26593134
Iteration 61, loss = 0.26543662
Iteration 62, loss = 0.26379360
Iteration 63, loss = 0.26436131
Iteration 64, loss = 0.26578707
Iteration 65, loss = 0.26267443
Iteration 66, loss = 0.26292066
Iteration 67, loss = 0.26493047
Iteration 68, loss = 0.26092541
Iteration 69, loss = 0.26236936Iteration 1, loss = 0.63625595
Iteration 2, loss = 0.47522226
Iteration 3, loss = 0.44114691
Iteration 4, loss = 0.42322391
Iteration 5, loss = 0.40447369
Iteration 6, loss = 0.38841819
Iteration 7, loss = 0.37781244
Iteration 8, loss = 0.37054605
Iteration 9, loss = 0.36117832
Iteration 10, loss = 0.35177844
Iteration 11, loss = 0.34386089
Iteration 12, loss = 0.33877003
Iteration 13, loss = 0.33418321
Iteration 14, loss = 0.33163831
Iteration 15, loss = 0.32149875
Iteration 16, loss = 0.32104930
Iteration 17, loss = 0.31670292
Iteration 18, loss = 0.31320677
Iteration 19, loss = 0.30932206
Iteration 20, loss = 0.30397815
Iteration 21, loss = 0.30436668
Iteration 22, loss = 0.29746245
Iteration 23, loss = 0.29567700
Iteration 24, loss = 0.29844398
Iteration 25, loss = 0.29028957
Iteration 26, loss = 0.28938454
Iteration 27, loss = 0.28753148
Iteration 28, loss = 0.28493098
Iteration 29, loss = 0.28289243
Iteration 30, loss = 0.28350516
Iteration 31, loss = 0.27802605
Iteration 32, loss = 0.27878803
Iteration 33, loss = 0.27669822
Iteration 34, loss = 0.27442258
Iteration 35, loss = 0.27501892
Iteration 36, loss = 0.27262740
Iteration 37, loss = 0.27071744
Iteration 38, loss = 0.27114907
Iteration 39, loss = 0.26749249
Iteration 40, loss = 0.26786419
Iteration 41, loss = 0.26737633
Iteration 42, loss = 0.26672867
Iteration 43, loss = 0.26712109
Iteration 44, loss = 0.26121964
Iteration 45, loss = 0.26206816
Iteration 46, loss = 0.26244550
Iteration 47, loss = 0.26076989
Iteration 48, loss = 0.26037605
Iteration 49, loss = 0.25828209
Iteration 50, loss = 0.25657098
Iteration 51, loss = 0.25722624
Iteration 52, loss = 0.26113612
Iteration 53, loss = 0.25846366
Iteration 54, loss = 0.25751878
Iteration 55, loss = 0.25291397
Iteration 56, loss = 0.25408925
Iteration 57, loss = 0.25070341
Iteration 58, loss = 0.25203630
Iteration 59, loss = 0.25103105
Iteration 60, loss = 0.25145941
Iteration 61, loss = 0.24962035
Iteration 62, loss = 0.24889738
Iteration 63, loss = 0.24893401
Iteration 64, loss = 0.24693521
Iteration 65, loss = 0.24814737
Iteration 66, loss = 0.24603186
Iteration 67, loss = 0.24453029
Iteration 68, loss = 0.24564747
Iteration 69, loss = 0.24641225
Iteration 70, loss = 0.24769523
Iteration 71, loss = 0.24584783
Iteration 72, loss = 0.24372946
Iteration 73, loss = 0.24523307
Iteration 74, loss = 0.24156348
Iteration 75, loss = 0.24069051
Iteration 76, loss = 0.24324935
Iteration 77, loss = 0.24220806
Iteration 78, loss = 0.24308226
Iteration 79, loss = 0.23944209
Iteration 80, loss = 0.24138411
Iteration 81, loss = 0.24032094
Iteration 82, loss = 0.23919226
Iteration 83, loss = 0.23936780
Iteration 84, loss = 0.23985348
Iteration 85, loss = 0.24037744
Iteration 86, loss = 0.23983929
Iteration 87, loss = 0.24012582
Iteration 88, loss = 0.23823679
Iteration 89, loss = 0.23619960
Iteration 90, loss = 0.23883479
Iteration 91, loss = 0.23523855
Iteration 92, loss = 0.23730221
Iteration 93, loss = 0.23663388
Iteration 94, loss = 0.23541148
Iteration 95, loss = 0.23588395
Iteration 96, loss = 0.23569402
Iteration 97, loss = 0.23424309
Iteration 98, loss = 0.23336397
Iteration 99, loss = 0.23755905
Iteration 100, loss = 0.23371361
Iteration 101, loss = 0.23300418
Iteration 102, loss = 0.23541113
Iteration 103, loss = 0.23356598
Iteration 104, loss = 0.23434935
Iteration 105, loss = 0.23431290
Iteration 106, loss = 0.23268134
Iteration 107, loss = 0.23286422
Iteration 108, loss = 0.23190761
Iteration 109, loss = 0.23005705
Iteration 110, loss = 0.23149457
Iteration 111, loss = 0.23032612
Iteration 112, loss = 0.23193093
Iteration 113, loss = 0.23055419
Iteration 114, loss = 0.22789324
Iteration 115, loss = 0.23047552
Iteration 116, loss = 0.23016961
Iteration 117, loss = 0.22857997
Iteration 118, loss = 0.23267342
Iteration 119, loss = 0.23004539
Iteration 120, loss = 0.22714460
Iteration 121, loss = 0.23086031
Iteration 122, loss = 0.22908319
Iteration 123, loss = 0.22804754
Iteration 124, loss = 0.22996722
Iteration 125, loss = 0.22815772
Iteration 126, loss = 0.22759737
Iteration 127, loss = 0.22731455
Iteration 128, loss = 0.22696051
Iteration 129, loss = 0.22650026
Iteration 130, loss = 0.22616509
Iteration 131, loss = 0.22476763
Iteration 132, loss = 0.22702285
Iteration 133, loss = 0.22682729
Iteration 134, loss = 0.22629353
Iteration 135, loss = 0.22483863
Iteration 136, loss = 0.22761834
Iteration 137, loss = 0.22563255
Iteration 138, loss = 0.22480640
Iteration 139, loss = 0.22421741
Iteration 140, loss = 0.22604830
Iteration 141, loss = 0.22532720
Iteration 142, loss = 0.22613640
Iteration 143, loss = 0.22582124
Iteration 144, loss = 0.22514319
Iteration 145, loss = 0.22317214
Iteration 146, loss = 0.22582337
Iteration 147, loss = 0.22762362
Iteration 148, loss = 0.22503895
Iteration 149, loss = 0.22466249
Iteration 150, loss = 0.22427752
Iteration 151, loss = 0.22376946
Iteration 152, loss = 0.22232086
Iteration 153, loss = 0.22394141
Iteration 154, loss = 0.22387258
Iteration 155, loss = 0.22405819
Iteration 156, loss = 0.22246578
Iteration 157, loss = 0.22184370
Iteration 158, loss = 0.22050096
Iteration 159, loss = 0.22459343
Iteration 160, loss = 0.22296975
Iteration 161, loss = 0.22256012
Iteration 162, loss = 0.22187810
Iteration 163, loss = 0.21964056
Iteration 164, loss = 0.22047195
Iteration 165, loss = 0.22388719
Iteration 166, loss = 0.22240472
Iteration 167, loss = 0.22061021
Iteration 168, loss = 0.21919114
Iteration 169, loss = 0.22241220
Iteration 170, loss = 0.22109965
Iteration 171, loss = 0.22346314
Iteration 172, loss = 0.22232710
Iteration 173, loss = 0.21807113
Iteration 174, loss = 0.22104117
Iteration 175, loss = 0.22018786
Iteration 176, loss = 0.21951942
Iteration 177, loss = 0.21880062
Iteration 178, loss = 0.22045975
Iteration 179, loss = 0.22088689
Iteration 180, loss = 0.22031571
Iteration 181, loss = 0.21926525
Iteration 182, loss = 0.22018574
Iteration 183, loss = 0.21740214
Iteration 184, loss = 0.21960884
Iteration 185, loss = 0.21824895
Iteration 186, loss = 0.22339630
Iteration 187, loss = 0.21930320
Iteration 188, loss = 0.21899812
Iteration 189, loss = 0.22147075
Iteration 190, loss = 0.22027160
Iteration 191, loss = 0.22186544
Iteration 192, loss = 0.21895405
Iteration 193, loss = 0.21904826
Iteration 194, loss = 0.21860151
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.57839443
Iteration 2, loss = 0.45299335
Iteration 3, loss = 0.42171920
Iteration 4, loss = 0.40214725
Iteration 5, loss = 0.38711197
Iteration 6, loss = 0.37629280
Iteration 7, loss = 0.36655962
Iteration 8, loss = 0.35916409
Iteration 9, loss = 0.35169597
Iteration 10, loss = 0.34387696
Iteration 11, loss = 0.33880655
Iteration 12, loss = 0.33378131
Iteration 13, loss = 0.32972310
Iteration 14, loss = 0.32710481
Iteration 15, loss = 0.32158469
Iteration 16, loss = 0.31762826
Iteration 17, loss = 0.31566775
Iteration 18, loss = 0.31177050
Iteration 19, loss = 0.31032573
Iteration 20, loss = 0.30744950
Iteration 21, loss = 0.30504084
Iteration 22, loss = 0.30273603
Iteration 23, loss = 0.30306971
Iteration 24, loss = 0.29860859
Iteration 25, loss = 0.29724779
Iteration 26, loss = 0.29680670
Iteration 27, loss = 0.29287669
Iteration 28, loss = 0.29283634
Iteration 29, loss = 0.29218098
Iteration 30, loss = 0.29143988
Iteration 31, loss = 0.29061033
Iteration 32, loss = 0.29137788
Iteration 33, loss = 0.28742930
Iteration 34, loss = 0.28663700
Iteration 35, loss = 0.28604599
Iteration 36, loss = 0.28450517
Iteration 37, loss = 0.28473657
Iteration 38, loss = 0.28255470
Iteration 39, loss = 0.28262583
Iteration 40, loss = 0.28092343
Iteration 41, loss = 0.28018811
Iteration 42, loss = 0.28142820
Iteration 43, loss = 0.28023008
Iteration 44, loss = 0.27756393
Iteration 45, loss = 0.27908742
Iteration 46, loss = 0.27681103
Iteration 47, loss = 0.27792297
Iteration 48, loss = 0.27603711
Iteration 49, loss = 0.27777521
Iteration 50, loss = 0.27440762
Iteration 51, loss = 0.27364501
Iteration 52, loss = 0.27474848
Iteration 53, loss = 0.27239223
Iteration 54, loss = 0.27323112
Iteration 55, loss = 0.27260189
Iteration 56, loss = 0.27259729
Iteration 57, loss = 0.27159175Iteration 1, loss = 0.60090692
Iteration 2, loss = 0.46381631
Iteration 3, loss = 0.43250989
Iteration 4, loss = 0.40994799
Iteration 5, loss = 0.39597606
Iteration 6, loss = 0.38200032
Iteration 7, loss = 0.37327762
Iteration 8, loss = 0.36390764
Iteration 9, loss = 0.35536073
Iteration 10, loss = 0.34994215
Iteration 11, loss = 0.34482218
Iteration 12, loss = 0.33722437
Iteration 13, loss = 0.33500121
Iteration 14, loss = 0.33002935
Iteration 15, loss = 0.32297033
Iteration 16, loss = 0.32280930
Iteration 17, loss = 0.32110320
Iteration 18, loss = 0.31320955
Iteration 19, loss = 0.31198357
Iteration 20, loss = 0.30807506
Iteration 21, loss = 0.30851497
Iteration 22, loss = 0.30476929
Iteration 23, loss = 0.30100468
Iteration 24, loss = 0.30064262
Iteration 25, loss = 0.29825669
Iteration 26, loss = 0.29787766
Iteration 27, loss = 0.29586580
Iteration 28, loss = 0.29434434
Iteration 29, loss = 0.29134457
Iteration 30, loss = 0.28866001
Iteration 31, loss = 0.28838860
Iteration 32, loss = 0.28664695
Iteration 33, loss = 0.28551980
Iteration 34, loss = 0.28346364
Iteration 35, loss = 0.28324595
Iteration 36, loss = 0.28122293
Iteration 37, loss = 0.28020988
Iteration 38, loss = 0.27974590
Iteration 39, loss = 0.27746141
Iteration 40, loss = 0.27779178
Iteration 41, loss = 0.27949669
Iteration 42, loss = 0.27672372
Iteration 43, loss = 0.27509498
Iteration 44, loss = 0.27635803
Iteration 45, loss = 0.27460767
Iteration 46, loss = 0.27443401
Iteration 47, loss = 0.27040165
Iteration 48, loss = 0.27094011
Iteration 49, loss = 0.27216963
Iteration 50, loss = 0.26840856
Iteration 51, loss = 0.26943083
Iteration 52, loss = 0.26964425
Iteration 53, loss = 0.26640155
Iteration 54, loss = 0.26865938
Iteration 55, loss = 0.26884069
Iteration 56, loss = 0.26549797
Iteration 57, loss = 0.26541858
Iteration 58, loss = 0.26502176
Iteration 59, loss = 0.26636613
Iteration 60, loss = 0.26547140
Iteration 61, loss = 0.26332307
Iteration 62, loss = 0.26192554
Iteration 63, loss = 0.26367984
Iteration 64, loss = 0.26350388
Iteration 65, loss = 0.26162735
Iteration 66, loss = 0.26127146
Iteration 67, loss = 0.26382356
Iteration 68, loss = 0.26205007
Iteration 69, loss = 0.26013163
Iteration 70, loss = 0.26244390
Iteration 71, loss = 0.26157407
Iteration 72, loss = 0.26031416
Iteration 73, loss = 0.26006879
Iteration 74, loss = 0.26045920
Iteration 75, loss = 0.25919850
Iteration 76, loss = 0.25762323
Iteration 77, loss = 0.25795665
Iteration 78, loss = 0.25807570
Iteration 79, loss = 0.25680805
Iteration 80, loss = 0.25810415
Iteration 81, loss = 0.25785447
Iteration 82, loss = 0.25631173
Iteration 83, loss = 0.25714018
Iteration 84, loss = 0.25554055
Iteration 85, loss = 0.25768522
Iteration 86, loss = 0.25589027
Iteration 87, loss = 0.25441831
Iteration 88, loss = 0.25593418
Iteration 89, loss = 0.25721591
Iteration 90, loss = 0.25466120
Iteration 91, loss = 0.25386604
Iteration 92, loss = 0.25530873
Iteration 93, loss = 0.25566158
Iteration 94, loss = 0.25551622
Iteration 95, loss = 0.25376243
Iteration 96, loss = 0.25307272
Iteration 97, loss = 0.25301273
Iteration 98, loss = 0.25358549
Iteration 99, loss = 0.25256160
Iteration 100, loss = 0.25241261
Iteration 101, loss = 0.25120164
Iteration 102, loss = 0.25367508
Iteration 103, loss = 0.25229820
Iteration 104, loss = 0.25038882
Iteration 105, loss = 0.24958419
Iteration 106, loss = 0.24990134
Iteration 107, loss = 0.25031230
Iteration 108, loss = 0.25220354
Iteration 109, loss = 0.24870686
Iteration 110, loss = 0.24907816
Iteration 111, loss = 0.25144931
Iteration 112, loss = 0.24932829
Iteration 113, loss = 0.24870530
Iteration 114, loss = 0.24994491
Iteration 115, loss = 0.25018547
Iteration 116, loss = 0.24932737
Iteration 117, loss = 0.24798267
Iteration 118, loss = 0.24967556
Iteration 119, loss = 0.24900543
Iteration 120, loss = 0.24729827
Iteration 121, loss = 0.24669126
Iteration 122, loss = 0.24639480
Iteration 123, loss = 0.24820871
Iteration 124, loss = 0.24949266
Iteration 125, loss = 0.24856359
Iteration 126, loss = 0.24702690
Iteration 127, loss = 0.24747362
Iteration 128, loss = 0.24781960
Iteration 129, loss = 0.24710608
Iteration 130, loss = 0.24674993
Iteration 131, loss = 0.24697437
Iteration 132, loss = 0.24629937
Iteration 133, loss = 0.24629093
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.64518348
Iteration 2, loss = 0.48247410
Iteration 3, loss = 0.44653834
Iteration 4, loss = 0.42720607
Iteration 5, loss = 0.40804219
Iteration 6, loss = 0.39820282
Iteration 7, loss = 0.38657733
Iteration 8, loss = 0.37820638
Iteration 9, loss = 0.36646057
Iteration 10, loss = 0.35844837
Iteration 11, loss = 0.35079626
Iteration 12, loss = 0.34584530
Iteration 13, loss = 0.34211964
Iteration 14, loss = 0.33823088
Iteration 15, loss = 0.33172957
Iteration 16, loss = 0.32488641
Iteration 17, loss = 0.32005233
Iteration 18, loss = 0.31903730
Iteration 19, loss = 0.31716218
Iteration 20, loss = 0.31249851
Iteration 21, loss = 0.30858460
Iteration 22, loss = 0.30665384
Iteration 23, loss = 0.30270405
Iteration 24, loss = 0.30024140
Iteration 25, loss = 0.29787909
Iteration 26, loss = 0.29346635
Iteration 27, loss = 0.29222912
Iteration 28, loss = 0.29113905
Iteration 29, loss = 0.29119954
Iteration 30, loss = 0.28486934
Iteration 31, loss = 0.28730569
Iteration 32, loss = 0.28337461
Iteration 33, loss = 0.28250553
Iteration 34, loss = 0.28184500
Iteration 35, loss = 0.28104807
Iteration 36, loss = 0.27708137
Iteration 37, loss = 0.27440337
Iteration 38, loss = 0.27266785
Iteration 39, loss = 0.27545017
Iteration 40, loss = 0.27054887
Iteration 41, loss = 0.26945250
Iteration 42, loss = 0.27013947
Iteration 43, loss = 0.26787609
Iteration 44, loss = 0.26728478
Iteration 45, loss = 0.26717914
Iteration 46, loss = 0.26572521
Iteration 47, loss = 0.26617579
Iteration 48, loss = 0.26433712
Iteration 49, loss = 0.26457026
Iteration 50, loss = 0.26066451
Iteration 51, loss = 0.26047211
Iteration 52, loss = 0.25822270
Iteration 53, loss = 0.25753686
Iteration 54, loss = 0.25830845
Iteration 55, loss = 0.25871870
Iteration 56, loss = 0.25783426
Iteration 57, loss = 0.25568937
Iteration 58, loss = 0.25527713
Iteration 59, loss = 0.25663306
Iteration 60, loss = 0.25483813
Iteration 61, loss = 0.25601782
Iteration 62, loss = 0.25520135
Iteration 63, loss = 0.25224878
Iteration 64, loss = 0.25262381
Iteration 65, loss = 0.25310232
Iteration 66, loss = 0.24908123
Iteration 67, loss = 0.25030097
Iteration 68, loss = 0.25000021
Iteration 69, loss = 0.24940412
Iteration 70, loss = 0.25111956
Iteration 71, loss = 0.24779054
Iteration 72, loss = 0.24924670
Iteration 73, loss = 0.24795044
Iteration 74, loss = 0.24391706
Iteration 75, loss = 0.24513568
Iteration 76, loss = 0.24850035
Iteration 77, loss = 0.24494200
Iteration 78, loss = 0.24633471
Iteration 79, loss = 0.24393567
Iteration 80, loss = 0.24334309
Iteration 81, loss = 0.24596041
Iteration 82, loss = 0.24344393
Iteration 83, loss = 0.24488651
Iteration 84, loss = 0.24474253
Iteration 85, loss = 0.24251026
Iteration 86, loss = 0.24230614
Iteration 87, loss = 0.24234656
Iteration 88, loss = 0.24261668
Iteration 89, loss = 0.24151290
Iteration 90, loss = 0.24024343
Iteration 91, loss = 0.23846412
Iteration 92, loss = 0.23895520
Iteration 93, loss = 0.23767067
Iteration 94, loss = 0.24058898
Iteration 95, loss = 0.23874896
Iteration 96, loss = 0.24100562
Iteration 97, loss = 0.24165961
Iteration 98, loss = 0.23816384
Iteration 99, loss = 0.23799780
Iteration 100, loss = 0.23582489
Iteration 101, loss = 0.23729921
Iteration 102, loss = 0.23599209
Iteration 103, loss = 0.23703972
Iteration 104, loss = 0.23536265
Iteration 105, loss = 0.23661592
Iteration 106, loss = 0.23688494
Iteration 107, loss = 0.23591052
Iteration 108, loss = 0.23581238
Iteration 109, loss = 0.23376541
Iteration 110, loss = 0.23632519
Iteration 111, loss = 0.23580528
Iteration 112, loss = 0.23689120
Iteration 113, loss = 0.23581580
Iteration 114, loss = 0.23486273
Iteration 115, loss = 0.23237282
Iteration 116, loss = 0.23512383
Iteration 117, loss = 0.23471494
Iteration 118, loss = 0.23456298
Iteration 119, loss = 0.23177474
Iteration 120, loss = 0.23130471
Iteration 121, loss = 0.23290503
Iteration 122, loss = 0.23247577
Iteration 123, loss = 0.23255227
Iteration 124, loss = 0.23286062
Iteration 125, loss = 0.23147478
Iteration 126, loss = 0.22862948
Iteration 127, loss = 0.23236458
Iteration 128, loss = 0.23200046
Iteration 129, loss = 0.22948066
Iteration 130, loss = 0.23164824
Iteration 131, loss = 0.23002949
Iteration 132, loss = 0.23031581
Iteration 133, loss = 0.22944829
Iteration 134, loss = 0.22887425
Iteration 135, loss = 0.22970526
Iteration 136, loss = 0.23023577
Iteration 137, loss = 0.23065014
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.89927270
Iteration 2, loss = 0.68243442
Iteration 3, loss = 0.57265520
Iteration 4, loss = 0.50317260
Iteration 5, loss = 0.46303695
Iteration 6, loss = 0.43353180
Iteration 7, loss = 0.41523665
Iteration 8, loss = 0.39702837
Iteration 9, loss = 0.37965073
Iteration 10, loss = 0.36602984
Iteration 11, loss = 0.35341742
Iteration 12, loss = 0.34203223
Iteration 13, loss = 0.33081387
Iteration 14, loss = 0.33065245
Iteration 15, loss = 0.32104245
Iteration 16, loss = 0.30686133
Iteration 17, loss = 0.29836664
Iteration 18, loss = 0.28859550
Iteration 19, loss = 0.27822964
Iteration 20, loss = 0.28177547
Iteration 21, loss = 0.27087874
Iteration 22, loss = 0.26191065
Iteration 23, loss = 0.25690244
Iteration 24, loss = 0.26520317
Iteration 25, loss = 0.25164254
Iteration 26, loss = 0.24590742
Iteration 27, loss = 0.23931575
Iteration 28, loss = 0.23627748
Iteration 29, loss = 0.23357859
Iteration 30, loss = 0.22831680
Iteration 31, loss = 0.22162627
Iteration 32, loss = 0.21615356
Iteration 33, loss = 0.21322053
Iteration 34, loss = 0.21275638
Iteration 35, loss = 0.20541182
Iteration 36, loss = 0.20455785
Iteration 37, loss = 0.20738361
Iteration 38, loss = 0.19843185
Iteration 39, loss = 0.19751114
Iteration 40, loss = 0.19519692
Iteration 41, loss = 0.21393199
Iteration 42, loss = 0.21089524
Iteration 43, loss = 0.20541887
Iteration 44, loss = 0.19214106
Iteration 45, loss = 0.19384958
Iteration 46, loss = 0.18343370
Iteration 47, loss = 0.18610807
Iteration 48, loss = 0.18012576
Iteration 49, loss = 0.17979836
Iteration 50, loss = 0.18105358
Iteration 51, loss = 0.18302800
Iteration 52, loss = 0.17457456
Iteration 53, loss = 0.18075859
Iteration 54, loss = 0.18635627
Iteration 55, loss = 0.18722403
Iteration 56, loss = 0.17488062
Iteration 57, loss = 0.18102170
Iteration 58, loss = 0.17196151
Iteration 59, loss = 0.16678578
Iteration 60, loss = 0.16510430
Iteration 61, loss = 0.16767555
Iteration 62, loss = 0.16474796
Iteration 63, loss = 0.16805441
Iteration 64, loss = 0.16158125
Iteration 65, loss = 0.16568509
Iteration 66, loss = 0.16717052
Iteration 67, loss = 0.16829912
Iteration 68, loss = 0.17292068
Iteration 69, loss = 0.16332505
Iteration 70, loss = 0.16153136
Iteration 71, loss = 0.16533628
Iteration 72, loss = 0.16946896
Iteration 73, loss = 0.16533051
Iteration 74, loss = 0.17082184
Iteration 75, loss = 0.16628684
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72481693
Iteration 2, loss = 0.50840104
Iteration 3, loss = 0.46125243
Iteration 4, loss = 0.43224015
Iteration 5, loss = 0.42031890
Iteration 6, loss = 0.40442722
Iteration 7, loss = 0.38849547
Iteration 8, loss = 0.38010347
Iteration 9, loss = 0.36902794
Iteration 10, loss = 0.35989026
Iteration 11, loss = 0.35155404
Iteration 12, loss = 0.34594488
Iteration 13, loss = 0.33394333
Iteration 14, loss = 0.33735841
Iteration 15, loss = 0.33000999
Iteration 16, loss = 0.32117819
Iteration 17, loss = 0.32218384
Iteration 18, loss = 0.31259707
Iteration 19, loss = 0.30764331
Iteration 20, loss = 0.30448472
Iteration 21, loss = 0.29953381
Iteration 22, loss = 0.29615151
Iteration 23, loss = 0.29568643
Iteration 24, loss = 0.29241357
Iteration 25, loss = 0.28745159
Iteration 26, loss = 0.28094673
Iteration 27, loss = 0.27922584
Iteration 28, loss = 0.28053681
Iteration 29, loss = 0.27800257
Iteration 30, loss = 0.26856124
Iteration 31, loss = 0.27110389
Iteration 32, loss = 0.26344322
Iteration 33, loss = 0.26823276
Iteration 34, loss = 0.26324653
Iteration 35, loss = 0.26222919
Iteration 36, loss = 0.25773435
Iteration 37, loss = 0.25625300
Iteration 38, loss = 0.25719687
Iteration 39, loss = 0.25148216
Iteration 40, loss = 0.25032279
Iteration 41, loss = 0.24704645
Iteration 42, loss = 0.24904754
Iteration 43, loss = 0.24909894
Iteration 44, loss = 0.24991759
Iteration 45, loss = 0.24530094
Iteration 46, loss = 0.24141351
Iteration 47, loss = 0.24091838
Iteration 48, loss = 0.23883234
Iteration 49, loss = 0.23650325
Iteration 50, loss = 0.23717928
Iteration 51, loss = 0.23799410
Iteration 52, loss = 0.23403696
Iteration 53, loss = 0.23470764
Iteration 54, loss = 0.23480852
Iteration 55, loss = 0.23149769
Iteration 56, loss = 0.23206399
Iteration 57, loss = 0.23223221
Iteration 58, loss = 0.22913893
Iteration 59, loss = 0.22901069
Iteration 60, loss = 0.22972518
Iteration 61, loss = 0.22801017
Iteration 62, loss = 0.22758286
Iteration 63, loss = 0.22548726
Iteration 64, loss = 0.22632920
Iteration 65, loss = 0.22350061
Iteration 66, loss = 0.22102402
Iteration 67, loss = 0.22067546
Iteration 68, loss = 0.22048364
Iteration 69, loss = 0.22077658
Iteration 70, loss = 0.22099634
Iteration 71, loss = 0.21881668
Iteration 72, loss = 0.21845368
Iteration 73, loss = 0.21836304
Iteration 74, loss = 0.21904100
Iteration 75, loss = 0.21788374
Iteration 76, loss = 0.21830369
Iteration 77, loss = 0.21683153
Iteration 78, loss = 0.21360501
Iteration 79, loss = 0.21245932
Iteration 80, loss = 0.21952925
Iteration 81, loss = 0.21739245
Iteration 82, loss = 0.21478910
Iteration 83, loss = 0.21154505
Iteration 84, loss = 0.20998407
Iteration 85, loss = 0.20868221
Iteration 86, loss = 0.21161281
Iteration 87, loss = 0.20644078
Iteration 88, loss = 0.21187510
Iteration 89, loss = 0.21162602
Iteration 90, loss = 0.21177481
Iteration 91, loss = 0.20667966
Iteration 92, loss = 0.20646674
Iteration 93, loss = 0.20546222
Iteration 94, loss = 0.20848293
Iteration 95, loss = 0.20873879
Iteration 96, loss = 0.20914168
Iteration 97, loss = 0.20599820
Iteration 98, loss = 0.20859139
Iteration 99, loss = 0.20415144
Iteration 100, loss = 0.20502866
Iteration 101, loss = 0.20255467
Iteration 102, loss = 0.20390739
Iteration 103, loss = 0.20250401
Iteration 104, loss = 0.20272950
Iteration 105, loss = 0.20136369
Iteration 106, loss = 0.20378160
Iteration 107, loss = 0.20525040
Iteration 108, loss = 0.20038659
Iteration 109, loss = 0.20302004
Iteration 110, loss = 0.20493672
Iteration 111, loss = 0.20109313
Iteration 112, loss = 0.20193828
Iteration 113, loss = 0.19841920
Iteration 114, loss = 0.20030588
Iteration 115, loss = 0.20153776
Iteration 116, loss = 0.20059971
Iteration 117, loss = 0.19896513
Iteration 118, loss = 0.19963041
Iteration 119, loss = 0.20004140
Iteration 120, loss = 0.19936617
Iteration 121, loss = 0.19716340
Iteration 122, loss = 0.19506411
Iteration 123, loss = 0.19684895
Iteration 124, loss = 0.19735489
Iteration 125, loss = 0.19676203
Iteration 126, loss = 0.19772973
Iteration 127, loss = 0.19568936
Iteration 128, loss = 0.19733770
Iteration 129, loss = 0.19654630
Iteration 130, loss = 0.19610873
Iteration 131, loss = 0.19362386
Iteration 132, loss = 0.19581184
Iteration 133, loss = 0.19378149
Iteration 134, loss = 0.19512910
Iteration 135, loss = 0.19199743
Iteration 136, loss = 0.19598830
Iteration 137, loss = 0.19418101
Iteration 138, loss = 0.19369655
Iteration 139, loss = 0.19410002
Iteration 140, loss = 0.19485844
Iteration 141, loss = 0.19016670
Iteration 142, loss = 0.19292304
Iteration 143, loss = 0.19238160
Iteration 144, loss = 0.19321965
Iteration 145, loss = 0.19059247
Iteration 146, loss = 0.19277816
Iteration 147, loss = 0.19274252
Iteration 148, loss = 0.18861031
Iteration 149, loss = 0.19055400
Iteration 150, loss = 0.19092707
Iteration 151, loss = 0.19264158
Iteration 152, loss = 0.19134321
Iteration 153, loss = 0.19076882
Iteration 154, loss = 0.18961584
Iteration 155, loss = 0.18924259
Iteration 156, loss = 0.19021734
Iteration 157, loss = 0.19068098
Iteration 158, loss = 0.19018856
Iteration 159, loss = 0.19185351
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.64389624
Iteration 2, loss = 0.48281671
Iteration 3, loss = 0.44540813
Iteration 4, loss = 0.42541327
Iteration 5, loss = 0.40693600
Iteration 6, loss = 0.39271020
Iteration 7, loss = 0.38758970
Iteration 8, loss = 0.37338183
Iteration 9, loss = 0.36468610
Iteration 10, loss = 0.36114094
Iteration 11, loss = 0.35523960
Iteration 12, loss = 0.34667142
Iteration 13, loss = 0.34100336
Iteration 14, loss = 0.33900517
Iteration 15, loss = 0.32965241
Iteration 16, loss = 0.32560555
Iteration 17, loss = 0.32221521
Iteration 18, loss = 0.32022208
Iteration 19, loss = 0.31416873
Iteration 20, loss = 0.31295314
Iteration 21, loss = 0.30644901
Iteration 22, loss = 0.30796258
Iteration 23, loss = 0.30320187
Iteration 24, loss = 0.30151886
Iteration 25, loss = 0.29738954
Iteration 26, loss = 0.29799128
Iteration 27, loss = 0.29340521
Iteration 28, loss = 0.29203701
Iteration 29, loss = 0.28975907
Iteration 30, loss = 0.28658259
Iteration 31, loss = 0.28605094
Iteration 32, loss = 0.28521598
Iteration 33, loss = 0.28268083
Iteration 34, loss = 0.28175939
Iteration 35, loss = 0.28179665
Iteration 36, loss = 0.27809480
Iteration 37, loss = 0.27986135
Iteration 38, loss = 0.27334275
Iteration 39, loss = 0.27475686
Iteration 40, loss = 0.27379905
Iteration 41, loss = 0.27406698
Iteration 42, loss = 0.26904529
Iteration 43, loss = 0.26995590
Iteration 44, loss = 0.26691164
Iteration 45, loss = 0.26863293
Iteration 46, loss = 0.26774001
Iteration 47, loss = 0.26694219
Iteration 48, loss = 0.26366178
Iteration 49, loss = 0.26498626
Iteration 50, loss = 0.26249841
Iteration 51, loss = 0.26351510
Iteration 52, loss = 0.26003998
Iteration 53, loss = 0.26001219
Iteration 54, loss = 0.25971488
Iteration 55, loss = 0.25751790
Iteration 56, loss = 0.25697068
Iteration 57, loss = 0.25625581
Iteration 58, loss = 0.25770374
Iteration 59, loss = 0.25525414
Iteration 60, loss = 0.25559062
Iteration 61, loss = 0.25503302
Iteration 62, loss = 0.25647767
Iteration 63, loss = 0.25521614
Iteration 64, loss = 0.25134037
Iteration 65, loss = 0.25429083
Iteration 66, loss = 0.25004459
Iteration 67, loss = 0.25072795
Iteration 68, loss = 0.25233797
Iteration 69, loss = 0.25125023
Iteration 70, loss = 0.24968021
Iteration 71, loss = 0.24818171
Iteration 72, loss = 0.24830010
Iteration 73, loss = 0.24928697
Iteration 74, loss = 0.24612940
Iteration 75, loss = 0.24825141
Iteration 76, loss = 0.24577013
Iteration 77, loss = 0.24607312
Iteration 78, loss = 0.24435635
Iteration 79, loss = 0.24609407
Iteration 80, loss = 0.24834190
Iteration 81, loss = 0.24353761
Iteration 82, loss = 0.24574163
Iteration 83, loss = 0.24260641
Iteration 84, loss = 0.24394653
Iteration 85, loss = 0.24443000
Iteration 86, loss = 0.24020369
Iteration 87, loss = 0.24184650
Iteration 88, loss = 0.24270438
Iteration 89, loss = 0.24211488
Iteration 90, loss = 0.24338704
Iteration 91, loss = 0.24155483
Iteration 92, loss = 0.23943628
Iteration 93, loss = 0.24139895
Iteration 94, loss = 0.23865167
Iteration 95, loss = 0.24075618
Iteration 96, loss = 0.23917765
Iteration 97, loss = 0.23953872
Iteration 98, loss = 0.23999079
Iteration 99, loss = 0.24053408
Iteration 100, loss = 0.23797580
Iteration 101, loss = 0.23857048
Iteration 102, loss = 0.23704314
Iteration 103, loss = 0.23787733
Iteration 104, loss = 0.23808832
Iteration 105, loss = 0.23619723
Iteration 106, loss = 0.23549674
Iteration 107, loss = 0.23723357
Iteration 108, loss = 0.23437925
Iteration 109, loss = 0.23638365
Iteration 110, loss = 0.23513588
Iteration 111, loss = 0.23497795
Iteration 112, loss = 0.23389649
Iteration 113, loss = 0.23357865
Iteration 114, loss = 0.23374855
Iteration 115, loss = 0.23355506
Iteration 116, loss = 0.23363142
Iteration 117, loss = 0.23446407
Iteration 118, loss = 0.23183224
Iteration 119, loss = 0.23080728
Iteration 120, loss = 0.23509479
Iteration 121, loss = 0.23219258
Iteration 122, loss = 0.23417983
Iteration 123, loss = 0.23364743
Iteration 124, loss = 0.23279178
Iteration 125, loss = 0.23143867
Iteration 126, loss = 0.23025275
Iteration 127, loss = 0.23276910
Iteration 128, loss = 0.22767683
Iteration 129, loss = 0.23170507
Iteration 130, loss = 0.23333184
Iteration 131, loss = 0.22993405
Iteration 132, loss = 0.22850095
Iteration 133, loss = 0.22820117
Iteration 134, loss = 0.22980066
Iteration 135, loss = 0.22958175
Iteration 136, loss = 0.23033639
Iteration 137, loss = 0.23106529
Iteration 138, loss = 0.22879487
Iteration 139, loss = 0.23121638
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.90474323
Iteration 2, loss = 0.68031817
Iteration 3, loss = 0.57003127
Iteration 4, loss = 0.49926802
Iteration 5, loss = 0.46517590
Iteration 6, loss = 0.43802357
Iteration 7, loss = 0.41429129
Iteration 8, loss = 0.39832508
Iteration 9, loss = 0.38355354
Iteration 10, loss = 0.37168607
Iteration 11, loss = 0.35601394
Iteration 12, loss = 0.34617528
Iteration 13, loss = 0.33920468
Iteration 14, loss = 0.32607675
Iteration 15, loss = 0.31686693
Iteration 16, loss = 0.30691476
Iteration 17, loss = 0.29981491
Iteration 18, loss = 0.29264502
Iteration 19, loss = 0.28453148
Iteration 20, loss = 0.28151432
Iteration 21, loss = 0.27522358
Iteration 22, loss = 0.26788420
Iteration 23, loss = 0.26908726
Iteration 24, loss = 0.26378736
Iteration 25, loss = 0.25569641
Iteration 26, loss = 0.24804503
Iteration 27, loss = 0.24360669
Iteration 28, loss = 0.24135850
Iteration 29, loss = 0.24144053
Iteration 30, loss = 0.23807798
Iteration 31, loss = 0.23673086
Iteration 32, loss = 0.22642011
Iteration 33, loss = 0.22392372
Iteration 34, loss = 0.22884066
Iteration 35, loss = 0.22342955
Iteration 36, loss = 0.22087629
Iteration 37, loss = 0.22090014
Iteration 38, loss = 0.21248858
Iteration 39, loss = 0.21660328
Iteration 40, loss = 0.21004530
Iteration 41, loss = 0.20915019
Iteration 42, loss = 0.20933609
Iteration 43, loss = 0.20352921
Iteration 44, loss = 0.20453499
Iteration 45, loss = 0.19674585
Iteration 46, loss = 0.19695611
Iteration 47, loss = 0.19492905
Iteration 48, loss = 0.19247967
Iteration 49, loss = 0.19056622
Iteration 50, loss = 0.19477776
Iteration 51, loss = 0.19397529
Iteration 52, loss = 0.19296497
Iteration 53, loss = 0.19727652
Iteration 54, loss = 0.20022856
Iteration 55, loss = 0.19818147
Iteration 56, loss = 0.18291020
Iteration 57, loss = 0.18834281
Iteration 58, loss = 0.19039453
Iteration 59, loss = 0.18253761
Iteration 60, loss = 0.17466187
Iteration 61, loss = 0.17851112
Iteration 62, loss = 0.17401451
Iteration 63, loss = 0.17368697
Iteration 64, loss = 0.18065882
Iteration 65, loss = 0.17663138
Iteration 66, loss = 0.17474607
Iteration 67, loss = 0.17624886
Iteration 68, loss = 0.16853674
Iteration 69, loss = 0.16952622
Iteration 70, loss = 0.17369456
Iteration 71, loss = 0.16832268
Iteration 72, loss = 0.16997450
Iteration 73, loss = 0.16567543
Iteration 74, loss = 0.16707779
Iteration 75, loss = 0.16890447
Iteration 76, loss = 0.16708924
Iteration 77, loss = 0.16314120
Iteration 78, loss = 0.16691236
Iteration 79, loss = 0.16494122
Iteration 80, loss = 0.16267272
Iteration 81, loss = 0.16516186
Iteration 82, loss = 0.16397199
Iteration 83, loss = 0.16585166
Iteration 84, loss = 0.16357208
Iteration 85, loss = 0.16026016
Iteration 86, loss = 0.15940668
Iteration 87, loss = 0.16402071
Iteration 88, loss = 0.15678895
Iteration 89, loss = 0.15637788
Iteration 90, loss = 0.15262678
Iteration 91, loss = 0.15502785
Iteration 92, loss = 0.15621516
Iteration 93, loss = 0.15747503
Iteration 94, loss = 0.15905434
Iteration 95, loss = 0.15060183
Iteration 96, loss = 0.16059399
Iteration 97, loss = 0.15897849
Iteration 98, loss = 0.16092879
Iteration 99, loss = 0.16075330
Iteration 100, loss = 0.15522192
Iteration 101, loss = 0.15900756
Iteration 102, loss = 0.15222671
Iteration 103, loss = 0.15520146
Iteration 104, loss = 0.15174189
Iteration 105, loss = 0.14960321
Iteration 106, loss = 0.14860358
Iteration 107, loss = 0.15574191
Iteration 108, loss = 0.15208843
Iteration 109, loss = 0.14873334
Iteration 77, loss = 0.26527590
Iteration 78, loss = 0.26585082
Iteration 79, loss = 0.26517710
Iteration 80, loss = 0.26349367
Iteration 81, loss = 0.26283174
Iteration 82, loss = 0.26383451
Iteration 83, loss = 0.26289869
Iteration 84, loss = 0.26310852
Iteration 85, loss = 0.26393344
Iteration 86, loss = 0.26285706
Iteration 87, loss = 0.26167153
Iteration 88, loss = 0.26255270
Iteration 89, loss = 0.26144537
Iteration 90, loss = 0.26229320
Iteration 91, loss = 0.26047816
Iteration 92, loss = 0.25975530
Iteration 93, loss = 0.26051792
Iteration 94, loss = 0.26058615
Iteration 95, loss = 0.26111687
Iteration 96, loss = 0.26113992
Iteration 97, loss = 0.26043700
Iteration 98, loss = 0.26072147
Iteration 99, loss = 0.25935209
Iteration 100, loss = 0.25920312
Iteration 101, loss = 0.25935673
Iteration 102, loss = 0.25890292
Iteration 103, loss = 0.26077510
Iteration 104, loss = 0.25895707
Iteration 105, loss = 0.26055869
Iteration 106, loss = 0.25805259
Iteration 107, loss = 0.25822030
Iteration 108, loss = 0.25911628
Iteration 109, loss = 0.25961458
Iteration 110, loss = 0.25660722
Iteration 111, loss = 0.25825956
Iteration 112, loss = 0.25721516
Iteration 113, loss = 0.25799000
Iteration 114, loss = 0.25735947
Iteration 115, loss = 0.25671756
Iteration 116, loss = 0.25707144
Iteration 117, loss = 0.25791363
Iteration 118, loss = 0.25639257
Iteration 119, loss = 0.25742685
Iteration 120, loss = 0.25666184
Iteration 121, loss = 0.25717348
Iteration 122, loss = 0.25745705
Iteration 123, loss = 0.25677378
Iteration 124, loss = 0.25599601
Iteration 125, loss = 0.25690878
Iteration 126, loss = 0.25740006
Iteration 127, loss = 0.25444384
Iteration 128, loss = 0.25726789
Iteration 129, loss = 0.25640802
Iteration 130, loss = 0.25729081
Iteration 131, loss = 0.25557172
Iteration 132, loss = 0.25413341
Iteration 133, loss = 0.25565237
Iteration 134, loss = 0.25479467
Iteration 135, loss = 0.25450158
Iteration 136, loss = 0.25479603
Iteration 137, loss = 0.25450606
Iteration 138, loss = 0.25568942
Iteration 139, loss = 0.25406464
Iteration 140, loss = 0.25336252
Iteration 141, loss = 0.25546989
Iteration 142, loss = 0.25375390
Iteration 143, loss = 0.25435755
Iteration 144, loss = 0.25550528
Iteration 145, loss = 0.25380166
Iteration 146, loss = 0.25335172
Iteration 147, loss = 0.25283315
Iteration 148, loss = 0.25305750
Iteration 149, loss = 0.25429984
Iteration 150, loss = 0.25289041
Iteration 151, loss = 0.25166210
Iteration 152, loss = 0.25408550
Iteration 153, loss = 0.25449609
Iteration 154, loss = 0.25417117
Iteration 155, loss = 0.25181563
Iteration 156, loss = 0.25215081
Iteration 157, loss = 0.25324932
Iteration 158, loss = 0.25351619
Iteration 159, loss = 0.25367413
Iteration 160, loss = 0.25225537
Iteration 161, loss = 0.25300634
Iteration 162, loss = 0.25235804
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.60373723
Iteration 2, loss = 0.46359762
Iteration 3, loss = 0.43354734
Iteration 4, loss = 0.41328742
Iteration 5, loss = 0.39804074
Iteration 6, loss = 0.38381245
Iteration 7, loss = 0.37547750
Iteration 8, loss = 0.36452684
Iteration 9, loss = 0.35877942
Iteration 10, loss = 0.35477836
Iteration 11, loss = 0.34415442
Iteration 12, loss = 0.34143028
Iteration 13, loss = 0.33388674
Iteration 14, loss = 0.33326229
Iteration 15, loss = 0.32618191
Iteration 16, loss = 0.32408684
Iteration 17, loss = 0.32175509
Iteration 18, loss = 0.31671694
Iteration 19, loss = 0.31243350
Iteration 20, loss = 0.31187342
Iteration 21, loss = 0.30841666
Iteration 22, loss = 0.30635418
Iteration 23, loss = 0.30211262
Iteration 24, loss = 0.30245007
Iteration 25, loss = 0.29961593
Iteration 26, loss = 0.30034023
Iteration 27, loss = 0.29640127
Iteration 28, loss = 0.29612798
Iteration 29, loss = 0.29182841
Iteration 30, loss = 0.29051985
Iteration 31, loss = 0.29067204
Iteration 32, loss = 0.28948539
Iteration 33, loss = 0.28756556
Iteration 34, loss = 0.28685254
Iteration 35, loss = 0.28480471
Iteration 36, loss = 0.28247745
Iteration 37, loss = 0.28303262
Iteration 38, loss = 0.28290077
Iteration 39, loss = 0.28018765
Iteration 40, loss = 0.27976182
Iteration 41, loss = 0.27830193
Iteration 42, loss = 0.27836745
Iteration 43, loss = 0.27650918
Iteration 44, loss = 0.27619426
Iteration 45, loss = 0.27526915
Iteration 46, loss = 0.27614633
Iteration 47, loss = 0.27445904
Iteration 48, loss = 0.27254413
Iteration 49, loss = 0.27302873
Iteration 50, loss = 0.27176010
Iteration 51, loss = 0.27151874
Iteration 52, loss = 0.27053320
Iteration 53, loss = 0.27313218
Iteration 54, loss = 0.26978146
Iteration 55, loss = 0.26954306
Iteration 56, loss = 0.26743303
Iteration 57, loss = 0.26786911
Iteration 58, loss = 0.26820694
Iteration 59, loss = 0.26678747
Iteration 60, loss = 0.26620632
Iteration 61, loss = 0.26655159
Iteration 62, loss = 0.26453935
Iteration 63, loss = 0.26617866
Iteration 64, loss = 0.26256095
Iteration 65, loss = 0.26389254
Iteration 66, loss = 0.26230885
Iteration 67, loss = 0.26451268
Iteration 68, loss = 0.26245302
Iteration 69, loss = 0.26280032
Iteration 70, loss = 0.26370303
Iteration 71, loss = 0.26141539
Iteration 72, loss = 0.26100443
Iteration 73, loss = 0.26186301
Iteration 74, loss = 0.25899817
Iteration 75, loss = 0.26160533
Iteration 76, loss = 0.26097701
Iteration 77, loss = 0.25908817
Iteration 78, loss = 0.25785193
Iteration 79, loss = 0.25798240
Iteration 80, loss = 0.25747672
Iteration 81, loss = 0.26024626
Iteration 82, loss = 0.25830628
Iteration 83, loss = 0.25776221
Iteration 84, loss = 0.25877422
Iteration 85, loss = 0.25700475
Iteration 86, loss = 0.25847926
Iteration 87, loss = 0.25625645
Iteration 88, loss = 0.25729385
Iteration 89, loss = 0.25526849
Iteration 90, loss = 0.25588930
Iteration 91, loss = 0.25504099
Iteration 92, loss = 0.25602308
Iteration 93, loss = 0.25681059
Iteration 94, loss = 0.25512968
Iteration 95, loss = 0.25274158
Iteration 96, loss = 0.25332293
Iteration 97, loss = 0.25472248
Iteration 98, loss = 0.25445139
Iteration 99, loss = 0.25376854
Iteration 100, loss = 0.25415215
Iteration 101, loss = 0.25273696
Iteration 102, loss = 0.25483106
Iteration 103, loss = 0.25395597
Iteration 104, loss = 0.25333485
Iteration 105, loss = 0.25319376
Iteration 106, loss = 0.25240555
Iteration 107, loss = 0.25150388
Iteration 108, loss = 0.25162166
Iteration 109, loss = 0.25171825
Iteration 110, loss = 0.25219135
Iteration 111, loss = 0.25140171
Iteration 112, loss = 0.24961306
Iteration 113, loss = 0.25127750
Iteration 114, loss = 0.24931160
Iteration 115, loss = 0.25075902
Iteration 116, loss = 0.25096240
Iteration 117, loss = 0.25039754
Iteration 118, loss = 0.24810897
Iteration 119, loss = 0.24991010
Iteration 120, loss = 0.25032783
Iteration 121, loss = 0.24916419
Iteration 122, loss = 0.24951591
Iteration 123, loss = 0.24896087
Iteration 124, loss = 0.25066901
Iteration 125, loss = 0.25156302
Iteration 126, loss = 0.24890730
Iteration 127, loss = 0.24804939
Iteration 128, loss = 0.24875647
Iteration 129, loss = 0.24676807
Iteration 130, loss = 0.24916177
Iteration 131, loss = 0.24879487
Iteration 132, loss = 0.24709387
Iteration 133, loss = 0.24778520
Iteration 134, loss = 0.24777606
Iteration 135, loss = 0.24778763
Iteration 136, loss = 0.24752959
Iteration 137, loss = 0.24695398
Iteration 138, loss = 0.24829259
Iteration 139, loss = 0.24704370
Iteration 140, loss = 0.24892243
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.64361053
Iteration 2, loss = 0.47983285
Iteration 3, loss = 0.44339975
Iteration 4, loss = 0.42236043
Iteration 5, loss = 0.40610866
Iteration 6, loss = 0.39271235
Iteration 7, loss = 0.38247642
Iteration 8, loss = 0.36990143
Iteration 9, loss = 0.36770648
Iteration 10, loss = 0.35639065
Iteration 11, loss = 0.35235731
Iteration 12, loss = 0.34243114
Iteration 13, loss = 0.33811961
Iteration 14, loss = 0.33374934
Iteration 15, loss = 0.32807676
Iteration 16, loss = 0.32265403
Iteration 17, loss = 0.32001789
Iteration 18, loss = 0.31587168
Iteration 19, loss = 0.31211320
Iteration 20, loss = 0.30849927
Iteration 21, loss = 0.30606555
Iteration 22, loss = 0.30313849
Iteration 110, loss = 0.14644311
Iteration 111, loss = 0.15040655
Iteration 112, loss = 0.14935079
Iteration 113, loss = 0.14425072
Iteration 114, loss = 0.14199030
Iteration 115, loss = 0.14618576
Iteration 116, loss = 0.14842269
Iteration 117, loss = 0.15623489
Iteration 118, loss = 0.16003246
Iteration 119, loss = 0.14979192
Iteration 120, loss = 0.14513944
Iteration 121, loss = 0.14788880
Iteration 122, loss = 0.14321341
Iteration 123, loss = 0.14435681
Iteration 124, loss = 0.14320158
Iteration 125, loss = 0.13963804
Iteration 126, loss = 0.14339161
Iteration 127, loss = 0.14187584
Iteration 128, loss = 0.14035238
Iteration 129, loss = 0.13614338
Iteration 130, loss = 0.14021927
Iteration 131, loss = 0.13775103
Iteration 132, loss = 0.13687182
Iteration 133, loss = 0.14272332
Iteration 134, loss = 0.14492015
Iteration 135, loss = 0.14074494
Iteration 136, loss = 0.13557694
Iteration 137, loss = 0.13676863
Iteration 138, loss = 0.13966978
Iteration 139, loss = 0.14357509
Iteration 140, loss = 0.14370418
Iteration 141, loss = 0.14110728
Iteration 142, loss = 0.14588816
Iteration 143, loss = 0.13627086
Iteration 144, loss = 0.13807116
Iteration 145, loss = 0.13505449
Iteration 146, loss = 0.13481823
Iteration 147, loss = 0.13832708
Iteration 148, loss = 0.13616832
Iteration 149, loss = 0.13699008
Iteration 150, loss = 0.13756207
Iteration 151, loss = 0.13224440
Iteration 152, loss = 0.13271697
Iteration 153, loss = 0.13631905
Iteration 154, loss = 0.13730042
Iteration 155, loss = 0.13318744
Iteration 156, loss = 0.13460914
Iteration 157, loss = 0.13374143
Iteration 158, loss = 0.13478207
Iteration 159, loss = 0.13826371
Iteration 160, loss = 0.13894319
Iteration 161, loss = 0.13454312
Iteration 162, loss = 0.14005201
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71706458
Iteration 2, loss = 0.50684175
Iteration 3, loss = 0.46544606
Iteration 4, loss = 0.44022763
Iteration 5, loss = 0.42621195
Iteration 6, loss = 0.40482451
Iteration 7, loss = 0.39328029
Iteration 8, loss = 0.38665288
Iteration 9, loss = 0.37164138
Iteration 10, loss = 0.36403112
Iteration 11, loss = 0.35668485
Iteration 12, loss = 0.34755592
Iteration 13, loss = 0.34511191
Iteration 14, loss = 0.33939874
Iteration 15, loss = 0.33531717
Iteration 16, loss = 0.32524555
Iteration 17, loss = 0.32983501
Iteration 18, loss = 0.31794343
Iteration 19, loss = 0.31371394
Iteration 20, loss = 0.31239212
Iteration 21, loss = 0.30440275
Iteration 22, loss = 0.30328475
Iteration 23, loss = 0.29681348
Iteration 24, loss = 0.29649732
Iteration 25, loss = 0.29120471
Iteration 26, loss = 0.29302985
Iteration 27, loss = 0.29046412
Iteration 28, loss = 0.28749469
Iteration 29, loss = 0.28268285
Iteration 30, loss = 0.28018416
Iteration 31, loss = 0.27722268
Iteration 32, loss = 0.27308913
Iteration 33, loss = 0.27453917
Iteration 34, loss = 0.27196935
Iteration 35, loss = 0.26905901
Iteration 36, loss = 0.26871987
Iteration 37, loss = 0.26772064
Iteration 38, loss = 0.26116767
Iteration 39, loss = 0.26288886
Iteration 40, loss = 0.26017770
Iteration 41, loss = 0.25922477
Iteration 42, loss = 0.25794165
Iteration 43, loss = 0.25534430
Iteration 44, loss = 0.25478291
Iteration 45, loss = 0.25821532
Iteration 46, loss = 0.25218371
Iteration 47, loss = 0.25036412
Iteration 48, loss = 0.24912846
Iteration 49, loss = 0.24817872
Iteration 50, loss = 0.24989117
Iteration 51, loss = 0.24443627
Iteration 52, loss = 0.24474137
Iteration 53, loss = 0.24308049
Iteration 54, loss = 0.24070595
Iteration 55, loss = 0.24726008
Iteration 56, loss = 0.24008556
Iteration 57, loss = 0.23977634
Iteration 58, loss = 0.23761472
Iteration 59, loss = 0.23758440
Iteration 60, loss = 0.23512751
Iteration 61, loss = 0.23413552
Iteration 62, loss = 0.23382508
Iteration 63, loss = 0.23369993
Iteration 64, loss = 0.23365982
Iteration 65, loss = 0.23562912
Iteration 66, loss = 0.23367127
Iteration 67, loss = 0.23153477
Iteration 68, loss = 0.23139136
Iteration 69, loss = 0.23142726
Iteration 70, loss = 0.22966299
Iteration 71, loss = 0.22985333
Iteration 72, loss = 0.22666977
Iteration 73, loss = 0.22471906
Iteration 74, loss = 0.22524457
Iteration 75, loss = 0.22328188
Iteration 76, loss = 0.22412239
Iteration 77, loss = 0.22461425
Iteration 78, loss = 0.22471142
Iteration 79, loss = 0.22190161
Iteration 80, loss = 0.22229867
Iteration 81, loss = 0.22165981
Iteration 82, loss = 0.21713364
Iteration 83, loss = 0.22036582
Iteration 84, loss = 0.22163566
Iteration 85, loss = 0.21995804
Iteration 86, loss = 0.22307756
Iteration 87, loss = 0.21865274
Iteration 88, loss = 0.21873499
Iteration 89, loss = 0.21673100
Iteration 90, loss = 0.21658073
Iteration 91, loss = 0.21748372
Iteration 92, loss = 0.21395049
Iteration 93, loss = 0.21567347
Iteration 94, loss = 0.21557903
Iteration 95, loss = 0.21138620
Iteration 96, loss = 0.21668599
Iteration 97, loss = 0.21308639
Iteration 98, loss = 0.21186887
Iteration 99, loss = 0.21589780
Iteration 100, loss = 0.21082834
Iteration 101, loss = 0.21358195
Iteration 102, loss = 0.21065981
Iteration 103, loss = 0.21289454
Iteration 104, loss = 0.21063802
Iteration 105, loss = 0.21018459
Iteration 106, loss = 0.21046540
Iteration 107, loss = 0.21038280
Iteration 108, loss = 0.20738646
Iteration 109, loss = 0.20646293
Iteration 110, loss = 0.20772291
Iteration 111, loss = 0.21014798
Iteration 112, loss = 0.20690186
Iteration 113, loss = 0.20629955
Iteration 114, loss = 0.20748549
Iteration 115, loss = 0.20659876
Iteration 116, loss = 0.20586544
Iteration 117, loss = 0.20488294
Iteration 118, loss = 0.20825389
Iteration 119, loss = 0.20693274
Iteration 120, loss = 0.20658068
Iteration 121, loss = 0.20557619
Iteration 122, loss = 0.20908366
Iteration 123, loss = 0.20613362
Iteration 124, loss = 0.20088546
Iteration 125, loss = 0.20080798
Iteration 126, loss = 0.20397581
Iteration 127, loss = 0.20271514
Iteration 128, loss = 0.20269244
Iteration 129, loss = 0.20449211
Iteration 130, loss = 0.20737291
Iteration 131, loss = 0.20285046
Iteration 132, loss = 0.20271215
Iteration 133, loss = 0.20249545
Iteration 134, loss = 0.19965067
Iteration 135, loss = 0.20169546
Iteration 136, loss = 0.19870842
Iteration 137, loss = 0.20608153
Iteration 138, loss = 0.19940787
Iteration 139, loss = 0.19888343
Iteration 140, loss = 0.20176873
Iteration 141, loss = 0.19875811
Iteration 142, loss = 0.20117119
Iteration 143, loss = 0.19617973
Iteration 144, loss = 0.20063773
Iteration 145, loss = 0.19799192
Iteration 146, loss = 0.19973386
Iteration 147, loss = 0.20040210
Iteration 148, loss = 0.19752444
Iteration 149, loss = 0.19706335
Iteration 150, loss = 0.20074187
Iteration 151, loss = 0.19789749
Iteration 152, loss = 0.20023984
Iteration 153, loss = 0.20188159
Iteration 154, loss = 0.19624353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.57520683
Iteration 2, loss = 0.45314557
Iteration 3, loss = 0.41952403
Iteration 4, loss = 0.40037114
Iteration 5, loss = 0.38648193
Iteration 6, loss = 0.37626495
Iteration 7, loss = 0.36340276
Iteration 8, loss = 0.35639428
Iteration 9, loss = 0.35023195
Iteration 10, loss = 0.34262194
Iteration 11, loss = 0.33642045
Iteration 12, loss = 0.33209396
Iteration 13, loss = 0.32698348
Iteration 14, loss = 0.32379890
Iteration 15, loss = 0.31905751
Iteration 16, loss = 0.31645255
Iteration 17, loss = 0.31345659
Iteration 18, loss = 0.31200130
Iteration 19, loss = 0.30898005
Iteration 20, loss = 0.30481910
Iteration 21, loss = 0.30376173
Iteration 22, loss = 0.30253908
Iteration 23, loss = 0.30052285
Iteration 24, loss = 0.29801924
Iteration 25, loss = 0.29451591
Iteration 26, loss = 0.29360550
Iteration 27, loss = 0.29436570
Iteration 28, loss = 0.29220491
Iteration 29, loss = 0.29187421
Iteration 30, loss = 0.29021052
Iteration 31, loss = 0.28760341
Iteration 32, loss = 0.28707109
Iteration 33, loss = 0.28753847
Iteration 34, loss = 0.28552797
Iteration 35, loss = 0.28280960
Iteration 36, loss = 0.28352805
Iteration 37, loss = 0.28217888
Iteration 38, loss = 0.28174126
Iteration 39, loss = 0.28229719
Iteration 40, loss = 0.27894370
Iteration 41, loss = 0.28116740
Iteration 70, loss = 0.26222548
Iteration 71, loss = 0.26012177
Iteration 72, loss = 0.26067885
Iteration 73, loss = 0.25886645
Iteration 74, loss = 0.25828635
Iteration 75, loss = 0.25986242
Iteration 76, loss = 0.25992563
Iteration 77, loss = 0.25955132
Iteration 78, loss = 0.25808230
Iteration 79, loss = 0.25856013
Iteration 80, loss = 0.25833317
Iteration 81, loss = 0.25633333
Iteration 82, loss = 0.25752111
Iteration 83, loss = 0.25610464
Iteration 84, loss = 0.25653741
Iteration 85, loss = 0.25605068
Iteration 86, loss = 0.25437727
Iteration 87, loss = 0.25800873
Iteration 88, loss = 0.25317755
Iteration 89, loss = 0.25559633
Iteration 90, loss = 0.25674007
Iteration 91, loss = 0.25610036
Iteration 92, loss = 0.25495557
Iteration 93, loss = 0.25461744
Iteration 94, loss = 0.25537294
Iteration 95, loss = 0.25630676
Iteration 96, loss = 0.25252281
Iteration 97, loss = 0.25191394
Iteration 98, loss = 0.25261313
Iteration 99, loss = 0.25214467
Iteration 100, loss = 0.25248418
Iteration 101, loss = 0.25281149
Iteration 102, loss = 0.25234140
Iteration 103, loss = 0.25220957
Iteration 104, loss = 0.25222331
Iteration 105, loss = 0.25030188
Iteration 106, loss = 0.25222883
Iteration 107, loss = 0.25064122
Iteration 108, loss = 0.25001242
Iteration 109, loss = 0.24974236
Iteration 110, loss = 0.24912928
Iteration 111, loss = 0.25057976
Iteration 112, loss = 0.24956611
Iteration 113, loss = 0.24994058
Iteration 114, loss = 0.25031941
Iteration 115, loss = 0.24996427
Iteration 116, loss = 0.24878766
Iteration 117, loss = 0.24781530
Iteration 118, loss = 0.24791644
Iteration 119, loss = 0.24843890
Iteration 120, loss = 0.24833967
Iteration 121, loss = 0.24878294
Iteration 122, loss = 0.24950012
Iteration 123, loss = 0.24719081
Iteration 124, loss = 0.24719343
Iteration 125, loss = 0.24722262
Iteration 126, loss = 0.24766429
Iteration 127, loss = 0.24777045
Iteration 128, loss = 0.24618992
Iteration 129, loss = 0.24836898
Iteration 130, loss = 0.24564929
Iteration 131, loss = 0.24625857
Iteration 132, loss = 0.24646116
Iteration 133, loss = 0.24822668
Iteration 134, loss = 0.24515349
Iteration 135, loss = 0.24891421
Iteration 136, loss = 0.24503078
Iteration 137, loss = 0.24651403
Iteration 138, loss = 0.24525622
Iteration 139, loss = 0.24676227
Iteration 140, loss = 0.24542430
Iteration 141, loss = 0.24535232
Iteration 142, loss = 0.24343054
Iteration 143, loss = 0.24395732
Iteration 144, loss = 0.24559376
Iteration 145, loss = 0.24637477
Iteration 146, loss = 0.24616404
Iteration 147, loss = 0.24446819
Iteration 148, loss = 0.24425039
Iteration 149, loss = 0.24643733
Iteration 150, loss = 0.24326914
Iteration 151, loss = 0.24595661
Iteration 152, loss = 0.24518491
Iteration 153, loss = 0.24357891
Iteration 154, loss = 0.24317273
Iteration 155, loss = 0.24395053
Iteration 156, loss = 0.24713468
Iteration 157, loss = 0.24247131
Iteration 158, loss = 0.24516733
Iteration 159, loss = 0.24269467
Iteration 160, loss = 0.24304564
Iteration 161, loss = 0.24365861
Iteration 162, loss = 0.24181737
Iteration 163, loss = 0.24112580
Iteration 164, loss = 0.24221531
Iteration 165, loss = 0.24221156
Iteration 166, loss = 0.24376793
Iteration 167, loss = 0.24483716
Iteration 168, loss = 0.24358819
Iteration 169, loss = 0.24244816
Iteration 170, loss = 0.24382901
Iteration 171, loss = 0.24306619
Iteration 172, loss = 0.24175746
Iteration 173, loss = 0.24397254
Iteration 174, loss = 0.24151225
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.57930677
Iteration 2, loss = 0.45650077
Iteration 3, loss = 0.42256076
Iteration 4, loss = 0.40427183
Iteration 5, loss = 0.39037593
Iteration 6, loss = 0.37774412
Iteration 7, loss = 0.36835924
Iteration 8, loss = 0.35947756
Iteration 9, loss = 0.35173743
Iteration 10, loss = 0.34541502
Iteration 11, loss = 0.33976601
Iteration 12, loss = 0.33641361
Iteration 13, loss = 0.33171408
Iteration 14, loss = 0.32714186
Iteration 15, loss = 0.32368262
Iteration 16, loss = 0.31988101
Iteration 17, loss = 0.31861169
Iteration 18, loss = 0.31536384
Iteration 19, loss = 0.31321251
Iteration 20, loss = 0.30895082
Iteration 21, loss = 0.30936862
Iteration 22, loss = 0.30458408
Iteration 23, loss = 0.30372386
Iteration 24, loss = 0.30159724
Iteration 25, loss = 0.30144211
Iteration 26, loss = 0.29788135
Iteration 27, loss = 0.29785451
Iteration 28, loss = 0.29574542
Iteration 29, loss = 0.29443248
Iteration 30, loss = 0.29243178
Iteration 31, loss = 0.29191266
Iteration 32, loss = 0.29131620
Iteration 33, loss = 0.28977863
Iteration 34, loss = 0.29100285
Iteration 35, loss = 0.28872896
Iteration 36, loss = 0.28597217
Iteration 37, loss = 0.28519663
Iteration 38, loss = 0.28670331
Iteration 39, loss = 0.28420729
Iteration 40, loss = 0.28482633
Iteration 41, loss = 0.28302372
Iteration 42, loss = 0.28074196
Iteration 43, loss = 0.27979534
Iteration 44, loss = 0.28167816
Iteration 45, loss = 0.27987346
Iteration 46, loss = 0.28144813
Iteration 47, loss = 0.28083573
Iteration 48, loss = 0.28030061
Iteration 49, loss = 0.27876229
Iteration 50, loss = 0.27854314
Iteration 51, loss = 0.27843399
Iteration 52, loss = 0.27583596
Iteration 53, loss = 0.27626289
Iteration 54, loss = 0.27490186
Iteration 55, loss = 0.27514021
Iteration 56, loss = 0.27719169
Iteration 57, loss = 0.27464099
Iteration 58, loss = 0.27356992
Iteration 59, loss = 0.27249288
Iteration 60, loss = 0.27525314
Iteration 61, loss = 0.27380865
Iteration 62, loss = 0.27485916
Iteration 63, loss = 0.27271334
Iteration 64, loss = 0.27136260
Iteration 65, loss = 0.27146025
Iteration 66, loss = 0.27053969
Iteration 67, loss = 0.26951950
Iteration 68, loss = 0.27052476
Iteration 69, loss = 0.26994485
Iteration 70, loss = 0.27096696
Iteration 71, loss = 0.27005555
Iteration 72, loss = 0.26895143
Iteration 73, loss = 0.26966864
Iteration 74, loss = 0.26738209
Iteration 75, loss = 0.26792168
Iteration 76, loss = 0.26744369
Iteration 77, loss = 0.26711712
Iteration 78, loss = 0.26810492
Iteration 79, loss = 0.26866047
Iteration 80, loss = 0.26670870
Iteration 81, loss = 0.26693479
Iteration 82, loss = 0.26678606
Iteration 83, loss = 0.26804806
Iteration 84, loss = 0.26466832
Iteration 85, loss = 0.26618885
Iteration 86, loss = 0.26603503
Iteration 87, loss = 0.26559544
Iteration 88, loss = 0.26486568
Iteration 89, loss = 0.26547523
Iteration 90, loss = 0.26438216
Iteration 91, loss = 0.26470031
Iteration 92, loss = 0.26333754
Iteration 93, loss = 0.26399516
Iteration 94, loss = 0.26390705
Iteration 95, loss = 0.26287683
Iteration 96, loss = 0.26549335
Iteration 97, loss = 0.26249000
Iteration 98, loss = 0.26314616
Iteration 99, loss = 0.26278405
Iteration 100, loss = 0.26461203
Iteration 101, loss = 0.26251820
Iteration 102, loss = 0.26241246
Iteration 103, loss = 0.26210021
Iteration 104, loss = 0.26130027
Iteration 105, loss = 0.26230224
Iteration 106, loss = 0.26152624
Iteration 107, loss = 0.26278269
Iteration 108, loss = 0.26158353
Iteration 109, loss = 0.26105494
Iteration 110, loss = 0.26207168
Iteration 111, loss = 0.26204699
Iteration 112, loss = 0.26021013
Iteration 113, loss = 0.26128743
Iteration 114, loss = 0.26101471
Iteration 115, loss = 0.26174618
Iteration 116, loss = 0.26123488
Iteration 117, loss = 0.25985225
Iteration 118, loss = 0.25996125
Iteration 119, loss = 0.26018725
Iteration 120, loss = 0.25979424
Iteration 121, loss = 0.26073798
Iteration 122, loss = 0.25939215
Iteration 123, loss = 0.25969997
Iteration 124, loss = 0.26021257
Iteration 125, loss = 0.25760286
Iteration 126, loss = 0.25869178
Iteration 127, loss = 0.26058484
Iteration 128, loss = 0.25883408
Iteration 129, loss = 0.25878859
Iteration 130, loss = 0.25836224
Iteration 131, loss = 0.25954203
Iteration 132, loss = 0.25758202
Iteration 133, loss = 0.25988425
Iteration 134, loss = 0.25807691
Iteration 135, loss = 0.25759929
Iteration 136, loss = 0.25897609
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.89974268
Iteration 2, loss = 0.68178445
Iteration 3, loss = 0.56330486
Iteration 4, loss = 0.49323290
Iteration 5, loss = 0.45703697
Iteration 6, loss = 0.43438055
Iteration 7, loss = 0.41293041
Iteration 8, loss = 0.39071484
Iteration 9, loss = 0.37476331
Iteration 10, loss = 0.36223602
Iteration 11, loss = 0.34967601
Iteration 12, loss = 0.33746379
Iteration 13, loss = 0.32843527
Iteration 14, loss = 0.31692870
Iteration 15, loss = 0.30705967
Iteration 16, loss = 0.30532103
Iteration 17, loss = 0.29587252
Iteration 18, loss = 0.28737217
Iteration 19, loss = 0.27368169
Iteration 20, loss = 0.27202988
Iteration 21, loss = 0.26129990
Iteration 22, loss = 0.25913451
Iteration 23, loss = 0.25851265
Iteration 24, loss = 0.25210907
Iteration 25, loss = 0.24298999
Iteration 26, loss = 0.25230031
Iteration 27, loss = 0.25683283
Iteration 28, loss = 0.25177482
Iteration 29, loss = 0.24146326
Iteration 30, loss = 0.22996584
Iteration 31, loss = 0.22679105
Iteration 32, loss = 0.22082728
Iteration 33, loss = 0.22199517
Iteration 34, loss = 0.22685293
Iteration 35, loss = 0.21875862
Iteration 36, loss = 0.21316479
Iteration 37, loss = 0.20871820
Iteration 38, loss = 0.20197022
Iteration 39, loss = 0.20388966
Iteration 40, loss = 0.19919961
Iteration 41, loss = 0.20136864
Iteration 42, loss = 0.20656713
Iteration 43, loss = 0.20332817
Iteration 44, loss = 0.19926795
Iteration 45, loss = 0.19187424
Iteration 46, loss = 0.19864748
Iteration 47, loss = 0.19273626
Iteration 48, loss = 0.19288731
Iteration 49, loss = 0.18951694
Iteration 50, loss = 0.19423927
Iteration 51, loss = 0.18846866
Iteration 52, loss = 0.18196494
Iteration 53, loss = 0.18908120
Iteration 54, loss = 0.18770708
Iteration 55, loss = 0.18049258
Iteration 56, loss = 0.18212973
Iteration 57, loss = 0.18033181
Iteration 58, loss = 0.18534146
Iteration 59, loss = 0.18141240
Iteration 60, loss = 0.18025502
Iteration 61, loss = 0.17486260
Iteration 62, loss = 0.16863583
Iteration 63, loss = 0.17099997
Iteration 64, loss = 0.17845871
Iteration 65, loss = 0.17455013
Iteration 66, loss = 0.16670249
Iteration 67, loss = 0.17070880
Iteration 68, loss = 0.17085048
Iteration 69, loss = 0.17160139
Iteration 70, loss = 0.16922215
Iteration 71, loss = 0.17106566
Iteration 72, loss = 0.16824938
Iteration 73, loss = 0.17382350
Iteration 74, loss = 0.17440307
Iteration 75, loss = 0.16256822
Iteration 76, loss = 0.16407979
Iteration 77, loss = 0.16346971
Iteration 78, loss = 0.16465499
Iteration 79, loss = 0.16253101
Iteration 80, loss = 0.16000566
Iteration 81, loss = 0.15720331
Iteration 82, loss = 0.16948903
Iteration 83, loss = 0.17496290
Iteration 84, loss = 0.16103372
Iteration 85, loss = 0.15408783
Iteration 86, loss = 0.15629851
Iteration 87, loss = 0.16016085
Iteration 88, loss = 0.15769780
Iteration 89, loss = 0.15693513
Iteration 90, loss = 0.15287677
Iteration 91, loss = 0.15748247
Iteration 92, loss = 0.15922401
Iteration 93, loss = 0.15361662
Iteration 94, loss = 0.15558611
Iteration 95, loss = 0.15698707
Iteration 96, loss = 0.14923146
Iteration 97, loss = 0.14861520
Iteration 98, loss = 0.15458139
Iteration 99, loss = 0.15060964
Iteration 100, loss = 0.15376823
Iteration 101, loss = 0.15055478
Iteration 102, loss = 0.14887286
Iteration 103, loss = 0.14963752
Iteration 104, loss = 0.14893713
Iteration 105, loss = 0.14831571
Iteration 106, loss = 0.14521813
Iteration 107, loss = 0.14795164
Iteration 108, loss = 0.14791339
Iteration 109, loss = 0.14440398
Iteration 110, loss = 0.14140613
Iteration 111, loss = 0.14351623
Iteration 112, loss = 0.14854377
Iteration 113, loss = 0.14542703
Iteration 114, loss = 0.14670591
Iteration 115, loss = 0.14503894
Iteration 116, loss = 0.13951230
Iteration 117, loss = 0.14208964
Iteration 118, loss = 0.14746825
Iteration 119, loss = 0.14274536
Iteration 120, loss = 0.14928270
Iteration 121, loss = 0.14931818
Iteration 122, loss = 0.14962141
Iteration 123, loss = 0.13911492
Iteration 124, loss = 0.14240393
Iteration 125, loss = 0.14417175
Iteration 126, loss = 0.14307248
Iteration 127, loss = 0.14400135
Iteration 128, loss = 0.14984309
Iteration 129, loss = 0.13949740
Iteration 130, loss = 0.14213702
Iteration 131, loss = 0.13857498
Iteration 132, loss = 0.13990906
Iteration 133, loss = 0.14107927
Iteration 134, loss = 0.14518757
Iteration 135, loss = 0.14705974
Iteration 136, loss = 0.14464217
Iteration 137, loss = 0.13491436
Iteration 138, loss = 0.14273947
Iteration 139, loss = 0.14305425
Iteration 140, loss = 0.13520235
Iteration 141, loss = 0.13893398
Iteration 142, loss = 0.14114029
Iteration 143, loss = 0.13788424
Iteration 144, loss = 0.14123298
Iteration 145, loss = 0.13920941
Iteration 146, loss = 0.14093790
Iteration 147, loss = 0.13707014
Iteration 148, loss = 0.13336070
Iteration 149, loss = 0.13568527
Iteration 150, loss = 0.13260168
Iteration 151, loss = 0.12987533
Iteration 152, loss = 0.13284620
Iteration 153, loss = 0.12943688
Iteration 154, loss = 0.13002950
Iteration 155, loss = 0.12745050
Iteration 156, loss = 0.13194432
Iteration 157, loss = 0.13020592
Iteration 158, loss = 0.12762895
Iteration 159, loss = 0.12942789
Iteration 160, loss = 0.12610445
Iteration 161, loss = 0.13135206
Iteration 162, loss = 0.13445527
Iteration 163, loss = 0.13252742
Iteration 164, loss = 0.12992299
Iteration 165, loss = 0.12903533
Iteration 166, loss = 0.12911889
Iteration 167, loss = 0.12589822
Iteration 168, loss = 0.13055341
Iteration 169, loss = 0.12844551
Iteration 170, loss = 0.12578412
Iteration 171, loss = 0.12930699
Iteration 172, loss = 0.13146591
Iteration 173, loss = 0.13514631
Iteration 174, loss = 0.12693799
Iteration 175, loss = 0.12724426
Iteration 176, loss = 0.12423035
Iteration 177, loss = 0.12682689
Iteration 178, loss = 0.12734007
Iteration 179, loss = 0.13254498
Iteration 180, loss = 0.12788166
Iteration 181, loss = 0.12642995
Iteration 182, loss = 0.12401198
Iteration 183, loss = 0.12386677
Iteration 184, loss = 0.12580129
Iteration 185, loss = 0.12397869
Iteration 186, loss = 0.12492966
Iteration 187, loss = 0.12326773
Iteration 188, loss = 0.12523203
Iteration 189, loss = 0.12062124
Iteration 190, loss = 0.12436893
Iteration 191, loss = 0.12099949
Iteration 192, loss = 0.12467189
Iteration 193, loss = 0.12128546
Iteration 194, loss = 0.12247301
Iteration 195, loss = 0.12264062
Iteration 196, loss = 0.12412497
Iteration 197, loss = 0.12300442
Iteration 198, loss = 0.12070007
Iteration 199, loss = 0.12630741
Iteration 200, loss = 0.12465436
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72146755
Iteration 2, loss = 0.50571013
Iteration 3, loss = 0.46598981
Iteration 4, loss = 0.43883629
Iteration 5, loss = 0.42184971
Iteration 6, loss = 0.40723760
Iteration 7, loss = 0.39695084
Iteration 8, loss = 0.38217307
Iteration 9, loss = 0.37410249
Iteration 10, loss = 0.36279045
Iteration 11, loss = 0.35956032
Iteration 12, loss = 0.34959007
Iteration 13, loss = 0.34288518
Iteration 14, loss = 0.33555729
Iteration 15, loss = 0.33064134
Iteration 16, loss = 0.32333532
Iteration 17, loss = 0.31646768
Iteration 18, loss = 0.31304986
Iteration 19, loss = 0.31310122
Iteration 20, loss = 0.31040661
Iteration 21, loss = 0.30290230
Iteration 22, loss = 0.30224668
Iteration 23, loss = 0.29926848
Iteration 24, loss = 0.29334561
Iteration 25, loss = 0.29542045
Iteration 26, loss = 0.29193686
Iteration 27, loss = 0.28653385
Iteration 28, loss = 0.28475283
Iteration 29, loss = 0.28265394
Iteration 30, loss = 0.28132375
Iteration 31, loss = 0.27942056
Iteration 32, loss = 0.27507923
Iteration 33, loss = 0.26876880
Iteration 34, loss = 0.26971616
Iteration 35, loss = 0.26965673
Iteration 36, loss = 0.26381851
Iteration 37, loss = 0.26494345
Iteration 38, loss = 0.26298620
Iteration 39, loss = 0.25993595
Iteration 40, loss = 0.25688317
Iteration 41, loss = 0.25821309
Iteration 42, loss = 0.25475076
Iteration 43, loss = 0.25327818
Iteration 44, loss = 0.25589328
Iteration 45, loss = 0.25244638
Iteration 46, loss = 0.24966917
Iteration 47, loss = 0.25003087
Iteration 48, loss = 0.24971897
Iteration 49, loss = 0.24479281
Iteration 50, loss = 0.24158022
Iteration 51, loss = 0.24191559
Iteration 52, loss = 0.24233737
Iteration 53, loss = 0.24382004
Iteration 54, loss = 0.23995502
Iteration 55, loss = 0.23790689
Iteration 56, loss = 0.23903019
Iteration 57, loss = 0.23957716
Iteration 58, loss = 0.23817300
Iteration 58, loss = 0.27040442
Iteration 59, loss = 0.27172319
Iteration 60, loss = 0.27008784
Iteration 61, loss = 0.26910909
Iteration 62, loss = 0.26945849
Iteration 63, loss = 0.26947548
Iteration 64, loss = 0.26960211
Iteration 65, loss = 0.27038937
Iteration 66, loss = 0.26806025
Iteration 67, loss = 0.26778233
Iteration 68, loss = 0.26801588
Iteration 69, loss = 0.26693194
Iteration 70, loss = 0.26800466
Iteration 71, loss = 0.26560570
Iteration 72, loss = 0.26590261
Iteration 73, loss = 0.26641633
Iteration 74, loss = 0.26739538
Iteration 75, loss = 0.26668614
Iteration 76, loss = 0.26556408
Iteration 77, loss = 0.26503303
Iteration 78, loss = 0.26556987
Iteration 79, loss = 0.26467228
Iteration 80, loss = 0.26417613
Iteration 81, loss = 0.26444434
Iteration 82, loss = 0.26447754
Iteration 83, loss = 0.26284056
Iteration 84, loss = 0.26487059
Iteration 85, loss = 0.26398143
Iteration 86, loss = 0.26297618
Iteration 87, loss = 0.26185180
Iteration 88, loss = 0.26222898
Iteration 89, loss = 0.26247367
Iteration 90, loss = 0.26214089
Iteration 91, loss = 0.26479644
Iteration 92, loss = 0.26129328
Iteration 93, loss = 0.26129850
Iteration 94, loss = 0.26273947
Iteration 95, loss = 0.26039849
Iteration 96, loss = 0.26071067
Iteration 97, loss = 0.26155350
Iteration 98, loss = 0.26076260
Iteration 99, loss = 0.26169125
Iteration 100, loss = 0.26095308
Iteration 101, loss = 0.26052328
Iteration 102, loss = 0.25931794
Iteration 103, loss = 0.25899924
Iteration 104, loss = 0.26043829
Iteration 105, loss = 0.25847477
Iteration 106, loss = 0.25925951
Iteration 107, loss = 0.26003838
Iteration 108, loss = 0.25989010
Iteration 109, loss = 0.25785630
Iteration 110, loss = 0.25858096
Iteration 111, loss = 0.25824472
Iteration 112, loss = 0.25922006
Iteration 113, loss = 0.26026581
Iteration 114, loss = 0.25832152
Iteration 115, loss = 0.25890870
Iteration 116, loss = 0.25734863
Iteration 117, loss = 0.25674794
Iteration 118, loss = 0.25712035
Iteration 119, loss = 0.25821595
Iteration 120, loss = 0.25841253
Iteration 121, loss = 0.25756419
Iteration 122, loss = 0.25670165
Iteration 123, loss = 0.25699645
Iteration 124, loss = 0.25602090
Iteration 125, loss = 0.25500462
Iteration 126, loss = 0.25631125
Iteration 127, loss = 0.25827176
Iteration 128, loss = 0.25661983
Iteration 129, loss = 0.25482846
Iteration 130, loss = 0.25597545
Iteration 131, loss = 0.25619507
Iteration 132, loss = 0.25557873
Iteration 133, loss = 0.25482872
Iteration 134, loss = 0.25497663
Iteration 135, loss = 0.25634131
Iteration 136, loss = 0.25594809
Iteration 137, loss = 0.25683528
Iteration 138, loss = 0.25473107
Iteration 139, loss = 0.25440777
Iteration 140, loss = 0.25377857
Iteration 141, loss = 0.25615462
Iteration 142, loss = 0.25641541
Iteration 143, loss = 0.25423923
Iteration 144, loss = 0.25592977
Iteration 145, loss = 0.25373143
Iteration 146, loss = 0.25334070
Iteration 147, loss = 0.25417055
Iteration 148, loss = 0.25223197
Iteration 149, loss = 0.25301275
Iteration 150, loss = 0.25280380
Iteration 151, loss = 0.25382469
Iteration 152, loss = 0.25323276
Iteration 153, loss = 0.25469055
Iteration 154, loss = 0.25588722
Iteration 155, loss = 0.25328462
Iteration 156, loss = 0.25471525
Iteration 157, loss = 0.25252660
Iteration 158, loss = 0.25189032
Iteration 159, loss = 0.25254169
Iteration 160, loss = 0.25435470
Iteration 161, loss = 0.25243343
Iteration 162, loss = 0.25282932
Iteration 163, loss = 0.25326673
Iteration 164, loss = 0.25308312
Iteration 165, loss = 0.25113299
Iteration 166, loss = 0.25284750
Iteration 167, loss = 0.25278528
Iteration 168, loss = 0.25188158
Iteration 169, loss = 0.25378047
Iteration 170, loss = 0.25297715
Iteration 171, loss = 0.25262146
Iteration 172, loss = 0.25270276
Iteration 173, loss = 0.25159037
Iteration 174, loss = 0.25064080
Iteration 175, loss = 0.25134896
Iteration 176, loss = 0.25243881
Iteration 177, loss = 0.25089938
Iteration 178, loss = 0.24978843
Iteration 179, loss = 0.25176116
Iteration 180, loss = 0.25195134
Iteration 181, loss = 0.25095554
Iteration 182, loss = 0.25085494
Iteration 183, loss = 0.25207109
Iteration 184, loss = 0.25116069
Iteration 185, loss = 0.25174206
Iteration 186, loss = 0.25091761
Iteration 187, loss = 0.25024870
Iteration 188, loss = 0.24994784
Iteration 189, loss = 0.25169427
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.59888185
Iteration 2, loss = 0.46192388
Iteration 3, loss = 0.42826842
Iteration 4, loss = 0.40951998
Iteration 5, loss = 0.39327390
Iteration 6, loss = 0.38342312
Iteration 7, loss = 0.37199160
Iteration 8, loss = 0.36293704
Iteration 9, loss = 0.35683322
Iteration 10, loss = 0.34891315
Iteration 11, loss = 0.34118183
Iteration 12, loss = 0.33759581
Iteration 13, loss = 0.33131935
Iteration 14, loss = 0.32851858
Iteration 15, loss = 0.32508861
Iteration 16, loss = 0.32316095
Iteration 17, loss = 0.31857678
Iteration 18, loss = 0.31415992
Iteration 19, loss = 0.31096721
Iteration 20, loss = 0.30974266
Iteration 21, loss = 0.30704555
Iteration 22, loss = 0.30404966
Iteration 23, loss = 0.30243414
Iteration 24, loss = 0.29939743
Iteration 25, loss = 0.29520352
Iteration 26, loss = 0.29656407
Iteration 27, loss = 0.29328023
Iteration 28, loss = 0.29133754
Iteration 29, loss = 0.28989938
Iteration 30, loss = 0.28888125
Iteration 31, loss = 0.28960850
Iteration 32, loss = 0.28500884
Iteration 33, loss = 0.28424045
Iteration 34, loss = 0.28384157
Iteration 35, loss = 0.28326257
Iteration 36, loss = 0.28061988
Iteration 37, loss = 0.28078661
Iteration 38, loss = 0.27709653
Iteration 39, loss = 0.27902924
Iteration 40, loss = 0.27616179
Iteration 41, loss = 0.27524149
Iteration 42, loss = 0.27676015
Iteration 43, loss = 0.27526686
Iteration 44, loss = 0.27468228
Iteration 45, loss = 0.27418295
Iteration 46, loss = 0.27259633
Iteration 47, loss = 0.27109585
Iteration 48, loss = 0.27106404
Iteration 49, loss = 0.27035246
Iteration 50, loss = 0.26902786
Iteration 51, loss = 0.26948312
Iteration 52, loss = 0.26772367
Iteration 53, loss = 0.26846495
Iteration 54, loss = 0.26638002
Iteration 55, loss = 0.26577096
Iteration 56, loss = 0.26889383
Iteration 57, loss = 0.26543359
Iteration 58, loss = 0.26479729
Iteration 59, loss = 0.26521216
Iteration 60, loss = 0.26269287
Iteration 61, loss = 0.26454918
Iteration 62, loss = 0.26308328
Iteration 63, loss = 0.26226607
Iteration 64, loss = 0.26171228
Iteration 65, loss = 0.26187661
Iteration 66, loss = 0.25945496
Iteration 67, loss = 0.26183565
Iteration 68, loss = 0.25936381
Iteration 69, loss = 0.26032877
Iteration 70, loss = 0.25859601
Iteration 71, loss = 0.25713236
Iteration 72, loss = 0.25908519
Iteration 73, loss = 0.25794950
Iteration 74, loss = 0.25733959
Iteration 75, loss = 0.25670169
Iteration 76, loss = 0.25680064
Iteration 77, loss = 0.25672809
Iteration 78, loss = 0.25549200
Iteration 79, loss = 0.25461775
Iteration 80, loss = 0.25429061
Iteration 81, loss = 0.25433032
Iteration 82, loss = 0.25642628
Iteration 83, loss = 0.25486663
Iteration 84, loss = 0.25409799
Iteration 85, loss = 0.25335065
Iteration 86, loss = 0.25420728
Iteration 87, loss = 0.25517306
Iteration 88, loss = 0.25350501
Iteration 89, loss = 0.25194022
Iteration 90, loss = 0.25264045
Iteration 91, loss = 0.25390591
Iteration 92, loss = 0.25161499
Iteration 93, loss = 0.25090648
Iteration 94, loss = 0.25146704
Iteration 95, loss = 0.25140877
Iteration 96, loss = 0.25017588
Iteration 97, loss = 0.25011433
Iteration 98, loss = 0.24931503
Iteration 99, loss = 0.25040704
Iteration 100, loss = 0.25136531
Iteration 101, loss = 0.24957596
Iteration 102, loss = 0.24999989
Iteration 103, loss = 0.24918260
Iteration 104, loss = 0.25006287
Iteration 105, loss = 0.24882180
Iteration 106, loss = 0.24859370
Iteration 107, loss = 0.24747604
Iteration 108, loss = 0.24949123
Iteration 109, loss = 0.24881952
Iteration 110, loss = 0.24740164
Iteration 111, loss = 0.24710280
Iteration 112, loss = 0.24809392
Iteration 113, loss = 0.24735931
Iteration 114, loss = 0.24668903
Iteration 115, loss = 0.24763462
Iteration 116, loss = 0.24786219
Iteration 117, loss = 0.24785176
Iteration 118, loss = 0.24599419
Iteration 119, loss = 0.24576843
Iteration 23, loss = 0.30301731
Iteration 24, loss = 0.29731369
Iteration 25, loss = 0.29711140
Iteration 26, loss = 0.29312312
Iteration 27, loss = 0.29159698
Iteration 28, loss = 0.28847313
Iteration 29, loss = 0.28921993
Iteration 30, loss = 0.28392811
Iteration 31, loss = 0.28422386
Iteration 32, loss = 0.28164107
Iteration 33, loss = 0.27971646
Iteration 34, loss = 0.27828849
Iteration 35, loss = 0.27661120
Iteration 36, loss = 0.27665104
Iteration 37, loss = 0.27356059
Iteration 38, loss = 0.27260929
Iteration 39, loss = 0.27254495
Iteration 40, loss = 0.27330224
Iteration 41, loss = 0.26857234
Iteration 42, loss = 0.27040848
Iteration 43, loss = 0.26902816
Iteration 44, loss = 0.26610618
Iteration 45, loss = 0.26561321
Iteration 46, loss = 0.26559415
Iteration 47, loss = 0.26440456
Iteration 48, loss = 0.26227539
Iteration 49, loss = 0.26213988
Iteration 50, loss = 0.26220025
Iteration 51, loss = 0.25734045
Iteration 52, loss = 0.26040151
Iteration 53, loss = 0.25937150
Iteration 54, loss = 0.25431177
Iteration 55, loss = 0.25907400
Iteration 56, loss = 0.25528942
Iteration 57, loss = 0.25559248
Iteration 58, loss = 0.25713530
Iteration 59, loss = 0.25052205
Iteration 60, loss = 0.25440434
Iteration 61, loss = 0.25114597
Iteration 62, loss = 0.25285031
Iteration 63, loss = 0.25252486
Iteration 64, loss = 0.25444887
Iteration 65, loss = 0.25357047
Iteration 66, loss = 0.24764809
Iteration 67, loss = 0.25021575
Iteration 68, loss = 0.24818589
Iteration 69, loss = 0.24997990
Iteration 70, loss = 0.24837261
Iteration 71, loss = 0.25170746
Iteration 72, loss = 0.24793663
Iteration 73, loss = 0.24855390
Iteration 74, loss = 0.24864659
Iteration 75, loss = 0.24587311
Iteration 76, loss = 0.24419748
Iteration 77, loss = 0.24369923
Iteration 78, loss = 0.24266942
Iteration 79, loss = 0.24540861
Iteration 80, loss = 0.24246868
Iteration 81, loss = 0.24413489
Iteration 82, loss = 0.24377573
Iteration 83, loss = 0.24171926
Iteration 84, loss = 0.23965419
Iteration 85, loss = 0.23999678
Iteration 86, loss = 0.23938447
Iteration 87, loss = 0.24151122
Iteration 88, loss = 0.24141623
Iteration 89, loss = 0.24136262
Iteration 90, loss = 0.24177502
Iteration 91, loss = 0.24001634
Iteration 92, loss = 0.23756861
Iteration 93, loss = 0.23733399
Iteration 94, loss = 0.23893151
Iteration 95, loss = 0.23733352
Iteration 96, loss = 0.23818875
Iteration 97, loss = 0.23683169
Iteration 98, loss = 0.23703678
Iteration 99, loss = 0.23942007
Iteration 100, loss = 0.23817451
Iteration 101, loss = 0.23863111
Iteration 102, loss = 0.23843318
Iteration 103, loss = 0.23597943
Iteration 104, loss = 0.23723553
Iteration 105, loss = 0.23587234
Iteration 106, loss = 0.23816579
Iteration 107, loss = 0.23719594
Iteration 108, loss = 0.23753950
Iteration 109, loss = 0.23482226
Iteration 110, loss = 0.23407950
Iteration 111, loss = 0.23535824
Iteration 112, loss = 0.23339193
Iteration 113, loss = 0.23519757
Iteration 114, loss = 0.23532995
Iteration 115, loss = 0.23316401
Iteration 116, loss = 0.23412070
Iteration 117, loss = 0.23049994
Iteration 118, loss = 0.23240049
Iteration 119, loss = 0.23205962
Iteration 120, loss = 0.22942860
Iteration 121, loss = 0.23289064
Iteration 122, loss = 0.23213188
Iteration 123, loss = 0.23224297
Iteration 124, loss = 0.23224405
Iteration 125, loss = 0.22969633
Iteration 126, loss = 0.22874400
Iteration 127, loss = 0.23281800
Iteration 128, loss = 0.23072108
Iteration 129, loss = 0.23068248
Iteration 130, loss = 0.22919030
Iteration 131, loss = 0.23153979
Iteration 132, loss = 0.23094024
Iteration 133, loss = 0.23157075
Iteration 134, loss = 0.23023741
Iteration 135, loss = 0.22907337
Iteration 136, loss = 0.22890916
Iteration 137, loss = 0.22696249
Iteration 138, loss = 0.23171278
Iteration 139, loss = 0.22929617
Iteration 140, loss = 0.23036293
Iteration 141, loss = 0.23123290
Iteration 142, loss = 0.22872499
Iteration 143, loss = 0.23022603
Iteration 144, loss = 0.23023271
Iteration 145, loss = 0.22721423
Iteration 146, loss = 0.22678097
Iteration 147, loss = 0.22741156
Iteration 148, loss = 0.22715724
Iteration 149, loss = 0.22643783
Iteration 150, loss = 0.22732554
Iteration 151, loss = 0.22885095
Iteration 152, loss = 0.22705791
Iteration 153, loss = 0.22489799
Iteration 154, loss = 0.22899437
Iteration 155, loss = 0.22638017
Iteration 156, loss = 0.22545263
Iteration 157, loss = 0.22626414
Iteration 158, loss = 0.22692346
Iteration 159, loss = 0.22587554
Iteration 160, loss = 0.22528418
Iteration 161, loss = 0.22326420
Iteration 162, loss = 0.22362828
Iteration 163, loss = 0.22559554
Iteration 164, loss = 0.22472329
Iteration 165, loss = 0.22495142
Iteration 166, loss = 0.22477170
Iteration 167, loss = 0.22265667
Iteration 168, loss = 0.22347078
Iteration 169, loss = 0.22426098
Iteration 170, loss = 0.22463455
Iteration 171, loss = 0.22560881
Iteration 172, loss = 0.22330451
Iteration 173, loss = 0.22268365
Iteration 174, loss = 0.22177655
Iteration 175, loss = 0.22366007
Iteration 176, loss = 0.22141960
Iteration 177, loss = 0.22278643
Iteration 178, loss = 0.22182958
Iteration 179, loss = 0.22310268
Iteration 180, loss = 0.22294343
Iteration 181, loss = 0.22378886
Iteration 182, loss = 0.22343590
Iteration 183, loss = 0.22294036
Iteration 184, loss = 0.22079623
Iteration 185, loss = 0.22347047
Iteration 186, loss = 0.22177588
Iteration 187, loss = 0.22020548
Iteration 188, loss = 0.22057915
Iteration 189, loss = 0.22544583
Iteration 190, loss = 0.22088691
Iteration 191, loss = 0.22128928
Iteration 192, loss = 0.21997814
Iteration 193, loss = 0.22020832
Iteration 194, loss = 0.21865289
Iteration 195, loss = 0.22072944
Iteration 196, loss = 0.22157310
Iteration 197, loss = 0.22269832
Iteration 198, loss = 0.22125305
Iteration 199, loss = 0.21978825
Iteration 200, loss = 0.22350338
Iteration 1, loss = 0.64370700
Iteration 2, loss = 0.48093214
Iteration 3, loss = 0.44555310
Iteration 4, loss = 0.42278219
Iteration 5, loss = 0.40809994
Iteration 6, loss = 0.39587720
Iteration 7, loss = 0.38285449
Iteration 8, loss = 0.37371706
Iteration 9, loss = 0.36410491
Iteration 10, loss = 0.35969814
Iteration 11, loss = 0.35172984
Iteration 12, loss = 0.34574607
Iteration 13, loss = 0.33766370
Iteration 14, loss = 0.33594270
Iteration 15, loss = 0.33222249
Iteration 16, loss = 0.32555434
Iteration 17, loss = 0.32167720
Iteration 18, loss = 0.32081718
Iteration 19, loss = 0.31368617
Iteration 20, loss = 0.31320562
Iteration 21, loss = 0.30883753
Iteration 22, loss = 0.30421746
Iteration 23, loss = 0.30127888
Iteration 24, loss = 0.29794404
Iteration 25, loss = 0.29872728
Iteration 26, loss = 0.29674202
Iteration 27, loss = 0.29560059
Iteration 28, loss = 0.29018015
Iteration 29, loss = 0.28982702
Iteration 30, loss = 0.28588033
Iteration 31, loss = 0.28633240
Iteration 32, loss = 0.28331078
Iteration 33, loss = 0.28195936
Iteration 34, loss = 0.28439665
Iteration 35, loss = 0.27951501
Iteration 36, loss = 0.27572572
Iteration 37, loss = 0.27365806
Iteration 38, loss = 0.27314144
Iteration 39, loss = 0.27292091
Iteration 40, loss = 0.27054442
Iteration 41, loss = 0.27035825
Iteration 42, loss = 0.27072763
Iteration 43, loss = 0.26707882
Iteration 44, loss = 0.26833207
Iteration 45, loss = 0.26465043
Iteration 46, loss = 0.26484923
Iteration 47, loss = 0.26392206
Iteration 48, loss = 0.26214501
Iteration 49, loss = 0.26224455
Iteration 50, loss = 0.26079328
Iteration 51, loss = 0.25913129
Iteration 52, loss = 0.25916642
Iteration 53, loss = 0.25985877
Iteration 54, loss = 0.25762958
Iteration 55, loss = 0.25643107
Iteration 56, loss = 0.25788598
Iteration 57, loss = 0.25514967
Iteration 58, loss = 0.25608259
Iteration 59, loss = 0.25714738
Iteration 60, loss = 0.25120144
Iteration 61, loss = 0.25375668
Iteration 62, loss = 0.25033862
Iteration 63, loss = 0.25193981
Iteration 64, loss = 0.25271750
Iteration 65, loss = 0.24904755
Iteration 66, loss = 0.25117657
Iteration 67, loss = 0.24956084
Iteration 68, loss = 0.25318322
Iteration 69, loss = 0.24730582
Iteration 70, loss = 0.24797436
Iteration 71, loss = 0.24874071
Iteration 72, loss = 0.24737244
Iteration 73, loss = 0.24681861
Iteration 74, loss = 0.24748346
Iteration 75, loss = 0.24849252
Iteration 76, loss = 0.24660466
Iteration 59, loss = 0.23893383
Iteration 60, loss = 0.23993705
Iteration 61, loss = 0.23497949
Iteration 62, loss = 0.23024558
Iteration 63, loss = 0.23276445
Iteration 64, loss = 0.22771357
Iteration 65, loss = 0.22953346
Iteration 66, loss = 0.22610076
Iteration 67, loss = 0.22861075
Iteration 68, loss = 0.22796107
Iteration 69, loss = 0.22427775
Iteration 70, loss = 0.22518421
Iteration 71, loss = 0.23158374
Iteration 72, loss = 0.22470937
Iteration 73, loss = 0.22468952
Iteration 74, loss = 0.22492857
Iteration 75, loss = 0.22577190
Iteration 76, loss = 0.22007484
Iteration 77, loss = 0.21754401
Iteration 78, loss = 0.22077417
Iteration 79, loss = 0.21950459
Iteration 80, loss = 0.21984597
Iteration 81, loss = 0.22068960
Iteration 82, loss = 0.21774566
Iteration 83, loss = 0.22098115
Iteration 84, loss = 0.21688553
Iteration 85, loss = 0.22120653
Iteration 86, loss = 0.21816893
Iteration 87, loss = 0.21585234
Iteration 88, loss = 0.22011926
Iteration 89, loss = 0.21278931
Iteration 90, loss = 0.21172711
Iteration 91, loss = 0.21593244
Iteration 92, loss = 0.21511888
Iteration 93, loss = 0.21232794
Iteration 94, loss = 0.21388375
Iteration 95, loss = 0.21211207
Iteration 96, loss = 0.21256465
Iteration 97, loss = 0.20849240
Iteration 98, loss = 0.21046997
Iteration 99, loss = 0.20913084
Iteration 100, loss = 0.20971366
Iteration 101, loss = 0.21086608
Iteration 102, loss = 0.21096745
Iteration 103, loss = 0.20995692
Iteration 104, loss = 0.21044920
Iteration 105, loss = 0.20682543
Iteration 106, loss = 0.21151266
Iteration 107, loss = 0.20869933
Iteration 108, loss = 0.20638170
Iteration 109, loss = 0.20986029
Iteration 110, loss = 0.20748921
Iteration 111, loss = 0.21151266
Iteration 112, loss = 0.20594016
Iteration 113, loss = 0.20583513
Iteration 114, loss = 0.20575576
Iteration 115, loss = 0.20364678
Iteration 116, loss = 0.20444273
Iteration 117, loss = 0.20461036
Iteration 118, loss = 0.20472461
Iteration 119, loss = 0.20749932
Iteration 120, loss = 0.20412026
Iteration 121, loss = 0.20158784
Iteration 122, loss = 0.20044941
Iteration 123, loss = 0.19854834
Iteration 124, loss = 0.20002587
Iteration 125, loss = 0.20170022
Iteration 126, loss = 0.20269935
Iteration 127, loss = 0.20087999
Iteration 128, loss = 0.20144902
Iteration 129, loss = 0.20143132
Iteration 130, loss = 0.19826770
Iteration 131, loss = 0.20052997
Iteration 132, loss = 0.20479712
Iteration 133, loss = 0.20260296
Iteration 134, loss = 0.19892957
Iteration 135, loss = 0.20376313
Iteration 136, loss = 0.20116745
Iteration 137, loss = 0.19750198
Iteration 138, loss = 0.19850486
Iteration 139, loss = 0.20092095
Iteration 140, loss = 0.19565326
Iteration 141, loss = 0.19794119
Iteration 142, loss = 0.19924175
Iteration 143, loss = 0.20029100
Iteration 144, loss = 0.19721715
Iteration 145, loss = 0.19653082
Iteration 146, loss = 0.19631665
Iteration 147, loss = 0.19641563
Iteration 148, loss = 0.19758372
Iteration 149, loss = 0.19420412
Iteration 150, loss = 0.19785361
Iteration 151, loss = 0.19557866
Iteration 152, loss = 0.19459933
Iteration 153, loss = 0.19605967
Iteration 154, loss = 0.19800652
Iteration 155, loss = 0.19988453
Iteration 156, loss = 0.19338791
Iteration 157, loss = 0.19422189
Iteration 158, loss = 0.19613424
Iteration 159, loss = 0.19666291
Iteration 160, loss = 0.19613180
Iteration 161, loss = 0.19445858
Iteration 162, loss = 0.19144530
Iteration 163, loss = 0.19287547
Iteration 164, loss = 0.19631967
Iteration 165, loss = 0.19435745
Iteration 166, loss = 0.19186381
Iteration 167, loss = 0.19199664
Iteration 168, loss = 0.19468556
Iteration 169, loss = 0.19246750
Iteration 170, loss = 0.19373379
Iteration 171, loss = 0.19213898
Iteration 172, loss = 0.19060386
Iteration 173, loss = 0.19115833
Iteration 174, loss = 0.19058366
Iteration 175, loss = 0.19682771
Iteration 176, loss = 0.19264403
Iteration 177, loss = 0.19534720
Iteration 178, loss = 0.19097661
Iteration 179, loss = 0.19142237
Iteration 180, loss = 0.18660890
Iteration 181, loss = 0.18687899
Iteration 182, loss = 0.19098805
Iteration 183, loss = 0.19067567
Iteration 184, loss = 0.18972344
Iteration 185, loss = 0.18883915
Iteration 186, loss = 0.18918646
Iteration 187, loss = 0.18973581
Iteration 188, loss = 0.18775168
Iteration 189, loss = 0.19537774
Iteration 190, loss = 0.18978215
Iteration 191, loss = 0.18960688
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.57910152
Iteration 2, loss = 0.44976199
Iteration 3, loss = 0.42027252
Iteration 4, loss = 0.40023229
Iteration 5, loss = 0.38612682
Iteration 6, loss = 0.37297800
Iteration 7, loss = 0.36489011
Iteration 8, loss = 0.35445190
Iteration 9, loss = 0.34676308
Iteration 10, loss = 0.34102123
Iteration 11, loss = 0.33666251
Iteration 12, loss = 0.33097465
Iteration 13, loss = 0.32729505
Iteration 14, loss = 0.32288043
Iteration 15, loss = 0.31876457
Iteration 16, loss = 0.31672158
Iteration 17, loss = 0.31388210
Iteration 18, loss = 0.30971594
Iteration 19, loss = 0.30837655
Iteration 20, loss = 0.30541797
Iteration 21, loss = 0.30464624
Iteration 22, loss = 0.30143987
Iteration 23, loss = 0.29829500
Iteration 24, loss = 0.29737783
Iteration 25, loss = 0.29535120
Iteration 26, loss = 0.29343626
Iteration 27, loss = 0.29412066
Iteration 28, loss = 0.29068165
Iteration 29, loss = 0.28868660
Iteration 30, loss = 0.28944472
Iteration 31, loss = 0.28556802
Iteration 32, loss = 0.28702036
Iteration 33, loss = 0.28400164
Iteration 34, loss = 0.28384931
Iteration 35, loss = 0.28376488
Iteration 36, loss = 0.28165221
Iteration 37, loss = 0.28076752
Iteration 38, loss = 0.28112862
Iteration 39, loss = 0.27868838
Iteration 40, loss = 0.27852567
Iteration 41, loss = 0.27714494
Iteration 42, loss = 0.27725635
Iteration 43, loss = 0.27685075
Iteration 44, loss = 0.27599518
Iteration 45, loss = 0.27396280
Iteration 46, loss = 0.27603877
Iteration 47, loss = 0.27432561
Iteration 48, loss = 0.27344832
Iteration 49, loss = 0.27272614
Iteration 50, loss = 0.27177429
Iteration 51, loss = 0.27150330
Iteration 52, loss = 0.26986954
Iteration 53, loss = 0.27166673
Iteration 54, loss = 0.26947620
Iteration 55, loss = 0.26764253
Iteration 56, loss = 0.26790933
Iteration 57, loss = 0.26939826
Iteration 58, loss = 0.26656279
Iteration 59, loss = 0.26789772
Iteration 60, loss = 0.26890162
Iteration 61, loss = 0.26667692
Iteration 62, loss = 0.26819990
Iteration 63, loss = 0.26725033
Iteration 64, loss = 0.26591993
Iteration 65, loss = 0.26591952
Iteration 66, loss = 0.26520022
Iteration 67, loss = 0.26549038
Iteration 68, loss = 0.26472674
Iteration 69, loss = 0.26475456
Iteration 70, loss = 0.26413880
Iteration 71, loss = 0.26428147
Iteration 72, loss = 0.26287990
Iteration 73, loss = 0.26394641
Iteration 74, loss = 0.26298210
Iteration 75, loss = 0.26320870
Iteration 76, loss = 0.26118549
Iteration 77, loss = 0.26255845
Iteration 78, loss = 0.26265880
Iteration 79, loss = 0.26209632
Iteration 80, loss = 0.26171104
Iteration 81, loss = 0.26034948
Iteration 82, loss = 0.25926407
Iteration 83, loss = 0.26207252
Iteration 84, loss = 0.25968859
Iteration 85, loss = 0.25955048
Iteration 86, loss = 0.25955609
Iteration 87, loss = 0.25924715
Iteration 88, loss = 0.25938306
Iteration 89, loss = 0.26077114
Iteration 90, loss = 0.25948505
Iteration 91, loss = 0.25896683
Iteration 92, loss = 0.25908936
Iteration 93, loss = 0.25786227
Iteration 94, loss = 0.25822270
Iteration 95, loss = 0.25809617
Iteration 96, loss = 0.25935097
Iteration 97, loss = 0.25893728
Iteration 98, loss = 0.25880540
Iteration 99, loss = 0.25708785
Iteration 100, loss = 0.25610360
Iteration 101, loss = 0.25908721
Iteration 102, loss = 0.25651160
Iteration 103, loss = 0.25609073
Iteration 104, loss = 0.25731421
Iteration 105, loss = 0.25745023
Iteration 106, loss = 0.25566995
Iteration 107, loss = 0.25538433
Iteration 108, loss = 0.25643062
Iteration 109, loss = 0.25490684
Iteration 110, loss = 0.25454196
Iteration 111, loss = 0.25441346
Iteration 112, loss = 0.25684581
Iteration 113, loss = 0.25322121
Iteration 114, loss = 0.25590897
Iteration 115, loss = 0.25542364
Iteration 116, loss = 0.25539091
Iteration 117, loss = 0.25407490
Iteration 118, loss = 0.25394667
Iteration 77, loss = 0.24700165
Iteration 78, loss = 0.24561409
Iteration 79, loss = 0.24371020
Iteration 80, loss = 0.24398473
Iteration 81, loss = 0.24289896
Iteration 82, loss = 0.24322586
Iteration 83, loss = 0.24414416
Iteration 84, loss = 0.24214351
Iteration 85, loss = 0.24310388
Iteration 86, loss = 0.24163449
Iteration 87, loss = 0.24329041
Iteration 88, loss = 0.23962304
Iteration 89, loss = 0.24168528
Iteration 90, loss = 0.24088751
Iteration 91, loss = 0.24068267
Iteration 92, loss = 0.23911905
Iteration 93, loss = 0.23927154
Iteration 94, loss = 0.23754345
Iteration 95, loss = 0.23895869
Iteration 96, loss = 0.23692305
Iteration 97, loss = 0.23934027
Iteration 98, loss = 0.23926937
Iteration 99, loss = 0.23914801
Iteration 100, loss = 0.23889159
Iteration 101, loss = 0.23704298
Iteration 102, loss = 0.23844088
Iteration 103, loss = 0.23926734
Iteration 104, loss = 0.23700439
Iteration 105, loss = 0.23406015
Iteration 106, loss = 0.23619405
Iteration 107, loss = 0.23708878
Iteration 108, loss = 0.23460943
Iteration 109, loss = 0.23691097
Iteration 110, loss = 0.23407559
Iteration 111, loss = 0.23492761
Iteration 112, loss = 0.23562511
Iteration 113, loss = 0.23344064
Iteration 114, loss = 0.23535669
Iteration 115, loss = 0.23314958
Iteration 116, loss = 0.23616919
Iteration 117, loss = 0.23330602
Iteration 118, loss = 0.23334545
Iteration 119, loss = 0.23534475
Iteration 120, loss = 0.23242552
Iteration 121, loss = 0.23298596
Iteration 122, loss = 0.23398426
Iteration 123, loss = 0.23208446
Iteration 124, loss = 0.23182148
Iteration 125, loss = 0.23092747
Iteration 126, loss = 0.23198587
Iteration 127, loss = 0.23502171
Iteration 128, loss = 0.23033488
Iteration 129, loss = 0.23095881
Iteration 130, loss = 0.22944660
Iteration 131, loss = 0.23077194
Iteration 132, loss = 0.23092125
Iteration 133, loss = 0.23112498
Iteration 134, loss = 0.23060838
Iteration 135, loss = 0.22975440
Iteration 136, loss = 0.23291010
Iteration 137, loss = 0.23057067
Iteration 138, loss = 0.22954608
Iteration 139, loss = 0.22975762
Iteration 140, loss = 0.22953540
Iteration 141, loss = 0.22916258
Iteration 142, loss = 0.22993709
Iteration 143, loss = 0.22856828
Iteration 144, loss = 0.22806515
Iteration 145, loss = 0.22822092
Iteration 146, loss = 0.22872153
Iteration 147, loss = 0.22726277
Iteration 148, loss = 0.22991411
Iteration 149, loss = 0.22811918
Iteration 150, loss = 0.22815036
Iteration 151, loss = 0.22713887
Iteration 152, loss = 0.22603273
Iteration 153, loss = 0.23082416
Iteration 154, loss = 0.22502542
Iteration 155, loss = 0.22852714
Iteration 156, loss = 0.22564519
Iteration 157, loss = 0.22718704
Iteration 158, loss = 0.22614628
Iteration 159, loss = 0.22935422
Iteration 160, loss = 0.22769408
Iteration 161, loss = 0.22762300
Iteration 162, loss = 0.22655853
Iteration 163, loss = 0.22619733
Iteration 164, loss = 0.22796986
Iteration 165, loss = 0.22872383
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

Iteration 120, loss = 0.24474036
Iteration 121, loss = 0.24589951
Iteration 122, loss = 0.25070676
Iteration 123, loss = 0.24471410
Iteration 124, loss = 0.24812092
Iteration 125, loss = 0.24425364
Iteration 126, loss = 0.24648850
Iteration 127, loss = 0.24384013
Iteration 128, loss = 0.24555131
Iteration 129, loss = 0.24455281
Iteration 130, loss = 0.24367347
Iteration 131, loss = 0.24392103
Iteration 132, loss = 0.24559409
Iteration 133, loss = 0.24461935
Iteration 134, loss = 0.24362130
Iteration 135, loss = 0.24504325
Iteration 136, loss = 0.24393961
Iteration 137, loss = 0.24478444
Iteration 138, loss = 0.24539526
Iteration 139, loss = 0.24441081
Iteration 140, loss = 0.24426406
Iteration 141, loss = 0.24173435
Iteration 142, loss = 0.24395147
Iteration 143, loss = 0.24265283
Iteration 144, loss = 0.24366510
Iteration 145, loss = 0.24176479
Iteration 146, loss = 0.24326799
Iteration 147, loss = 0.24231391
Iteration 148, loss = 0.24297274
Iteration 149, loss = 0.24292206
Iteration 150, loss = 0.24292362
Iteration 151, loss = 0.24276978
Iteration 152, loss = 0.24234308
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.60508804
Iteration 2, loss = 0.46443059
Iteration 3, loss = 0.43135148
Iteration 4, loss = 0.41000940
Iteration 5, loss = 0.39630415
Iteration 6, loss = 0.38336150
Iteration 7, loss = 0.37132957
Iteration 8, loss = 0.36196955
Iteration 9, loss = 0.35381411
Iteration 10, loss = 0.34802930
Iteration 11, loss = 0.34084317
Iteration 12, loss = 0.33848107
Iteration 13, loss = 0.33203054
Iteration 14, loss = 0.32907857
Iteration 15, loss = 0.32614812
Iteration 16, loss = 0.31943939
Iteration 17, loss = 0.31618349
Iteration 18, loss = 0.31380259
Iteration 19, loss = 0.31070363
Iteration 20, loss = 0.30865763
Iteration 21, loss = 0.30516410
Iteration 22, loss = 0.30212107
Iteration 23, loss = 0.30164791
Iteration 24, loss = 0.30089433
Iteration 25, loss = 0.29665696
Iteration 26, loss = 0.29413941
Iteration 27, loss = 0.29198739
Iteration 28, loss = 0.29012812
Iteration 29, loss = 0.28921614
Iteration 30, loss = 0.28748853
Iteration 31, loss = 0.28688584
Iteration 32, loss = 0.28539671
Iteration 33, loss = 0.28239686
Iteration 34, loss = 0.28346708
Iteration 35, loss = 0.28165591
Iteration 36, loss = 0.28238895
Iteration 37, loss = 0.27773598
Iteration 38, loss = 0.27787571
Iteration 39, loss = 0.27703496
Iteration 40, loss = 0.27580963
Iteration 41, loss = 0.27605304
Iteration 42, loss = 0.27179700
Iteration 43, loss = 0.27559352
Iteration 44, loss = 0.27149725
Iteration 45, loss = 0.27311345
Iteration 46, loss = 0.27261965
Iteration 47, loss = 0.26988114
Iteration 48, loss = 0.26956712
Iteration 49, loss = 0.26782520
Iteration 50, loss = 0.26664056
Iteration 51, loss = 0.26781772
Iteration 52, loss = 0.26529417
Iteration 53, loss = 0.26634794
Iteration 54, loss = 0.26614266
Iteration 55, loss = 0.26421789
Iteration 56, loss = 0.26451432
Iteration 57, loss = 0.26250889
Iteration 58, loss = 0.26251946
Iteration 59, loss = 0.26238540
Iteration 60, loss = 0.26227526
Iteration 61, loss = 0.26154024
Iteration 62, loss = 0.26113532
Iteration 63, loss = 0.26037752
Iteration 64, loss = 0.25973647
Iteration 65, loss = 0.26038576
Iteration 66, loss = 0.26100089
Iteration 67, loss = 0.25959797
Iteration 68, loss = 0.25884760
Iteration 69, loss = 0.25785508
Iteration 70, loss = 0.25762054
Iteration 71, loss = 0.25789970
Iteration 72, loss = 0.25710945
Iteration 73, loss = 0.25617898
Iteration 74, loss = 0.25790090
Iteration 75, loss = 0.25685990
Iteration 76, loss = 0.25527820
Iteration 77, loss = 0.25695749
Iteration 78, loss = 0.25535359
Iteration 79, loss = 0.25722275
Iteration 80, loss = 0.25430506
Iteration 81, loss = 0.25479865
Iteration 82, loss = 0.25632406
Iteration 83, loss = 0.25454226
Iteration 84, loss = 0.25523116
Iteration 85, loss = 0.25194185
Iteration 86, loss = 0.25297627
Iteration 87, loss = 0.25370886
Iteration 88, loss = 0.25308692
Iteration 89, loss = 0.25398351
Iteration 90, loss = 0.25124541
Iteration 91, loss = 0.25086173
Iteration 92, loss = 0.25313421
Iteration 93, loss = 0.25102486
Iteration 94, loss = 0.25236311
Iteration 95, loss = 0.25197695
Iteration 96, loss = 0.25105041
Iteration 97, loss = 0.24946029
Iteration 98, loss = 0.24858992
Iteration 99, loss = 0.25050609
Iteration 100, loss = 0.25008268
Iteration 101, loss = 0.24951222
Iteration 102, loss = 0.24841098
Iteration 103, loss = 0.25028545
Iteration 104, loss = 0.25052413
Iteration 105, loss = 0.24887510
Iteration 106, loss = 0.24815151
Iteration 107, loss = 0.24899089
Iteration 108, loss = 0.24926667
Iteration 109, loss = 0.24767035
Iteration 110, loss = 0.24732186
Iteration 111, loss = 0.24803867
Iteration 112, loss = 0.24837015
Iteration 113, loss = 0.24763541
Iteration 114, loss = 0.24652448
Iteration 115, loss = 0.24812099
Iteration 116, loss = 0.24790899
Iteration 117, loss = 0.24653885
Iteration 118, loss = 0.24753767
Iteration 119, loss = 0.24487088
Iteration 120, loss = 0.24743096
Iteration 121, loss = 0.24624459
Iteration 122, loss = 0.24682660
Iteration 123, loss = 0.24561913
Iteration 124, loss = 0.24514730
Iteration 125, loss = 0.24743423
Iteration 126, loss = 0.24499699
Iteration 127, loss = 0.24483578
Iteration 128, loss = 0.24719191
Iteration 129, loss = 0.24385401
Iteration 130, loss = 0.24517258
Iteration 131, loss = 0.24448484
Iteration 132, loss = 0.24470862
Iteration 133, loss = 0.24832999
Iteration 134, loss = 0.24509346
Iteration 135, loss = 0.24497466
Iteration 136, loss = 0.24557141
Iteration 137, loss = 0.24500909
Iteration 138, loss = 0.24494804
Iteration 139, loss = 0.24433558
Iteration 140, loss = 0.24516211
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

Iteration 42, loss = 0.27774776
Iteration 43, loss = 0.27769747
Iteration 44, loss = 0.27692409
Iteration 45, loss = 0.27689772
Iteration 46, loss = 0.27674008
Iteration 47, loss = 0.27504216
Iteration 48, loss = 0.27586014
Iteration 49, loss = 0.27337253
Iteration 50, loss = 0.27399853
Iteration 51, loss = 0.27280933
Iteration 52, loss = 0.27189936
Iteration 53, loss = 0.27238131
Iteration 54, loss = 0.26975662
Iteration 55, loss = 0.27252259
Iteration 56, loss = 0.27118107
Iteration 57, loss = 0.26946465
Iteration 58, loss = 0.26949840
Iteration 59, loss = 0.27018211
Iteration 60, loss = 0.27065051
Iteration 61, loss = 0.26895119
Iteration 62, loss = 0.26946174
Iteration 63, loss = 0.26850799
Iteration 64, loss = 0.26771731
Iteration 65, loss = 0.26653365
Iteration 66, loss = 0.26738487
Iteration 67, loss = 0.26743198
Iteration 68, loss = 0.26506068
Iteration 69, loss = 0.26553044
Iteration 70, loss = 0.26442652
Iteration 71, loss = 0.26412061
Iteration 72, loss = 0.26584339
Iteration 73, loss = 0.26373207
Iteration 74, loss = 0.26330197
Iteration 75, loss = 0.26458824
Iteration 76, loss = 0.26317564
Iteration 77, loss = 0.26444358
Iteration 78, loss = 0.26359848
Iteration 79, loss = 0.26412119
Iteration 80, loss = 0.26313308
Iteration 81, loss = 0.26270174
Iteration 82, loss = 0.26162185
Iteration 83, loss = 0.26239946
Iteration 84, loss = 0.26056896
Iteration 85, loss = 0.26314984
Iteration 86, loss = 0.26154824
Iteration 87, loss = 0.26270604
Iteration 88, loss = 0.26120383
Iteration 89, loss = 0.26030492
Iteration 90, loss = 0.26094164
Iteration 91, loss = 0.25995401
Iteration 92, loss = 0.25947932
Iteration 93, loss = 0.25991051
Iteration 94, loss = 0.25958544
Iteration 95, loss = 0.25893885
Iteration 96, loss = 0.26043935
Iteration 97, loss = 0.25943210
Iteration 98, loss = 0.25940639
Iteration 99, loss = 0.25917354
Iteration 100, loss = 0.25822930
Iteration 101, loss = 0.25789662
Iteration 102, loss = 0.25849920
Iteration 103, loss = 0.25749559
Iteration 104, loss = 0.25829343
Iteration 105, loss = 0.25931114
Iteration 106, loss = 0.25693088
Iteration 107, loss = 0.25900593
Iteration 108, loss = 0.25781874
Iteration 109, loss = 0.25682722
Iteration 110, loss = 0.25582383
Iteration 111, loss = 0.25651437
Iteration 112, loss = 0.25566056
Iteration 113, loss = 0.25555084
Iteration 114, loss = 0.25681339
Iteration 115, loss = 0.25686000
Iteration 116, loss = 0.25613402
Iteration 117, loss = 0.25657661
Iteration 118, loss = 0.25688345
Iteration 119, loss = 0.25575150
Iteration 120, loss = 0.25614077
Iteration 121, loss = 0.25497621
Iteration 122, loss = 0.25517986
Iteration 123, loss = 0.25607887
Iteration 124, loss = 0.25550391
Iteration 125, loss = 0.25450303
Iteration 126, loss = 0.25595945
Iteration 127, loss = 0.25533465
Iteration 128, loss = 0.25510610
Iteration 129, loss = 0.25311182
Iteration 130, loss = 0.25440631
Iteration 131, loss = 0.25348091
Iteration 132, loss = 0.25327174
Iteration 133, loss = 0.25483110
Iteration 134, loss = 0.25494215
Iteration 135, loss = 0.25374770
Iteration 136, loss = 0.25295864
Iteration 137, loss = 0.25319062
Iteration 138, loss = 0.25413257
Iteration 139, loss = 0.25177026
Iteration 140, loss = 0.25316862
Iteration 141, loss = 0.25371734
Iteration 142, loss = 0.25227138
Iteration 143, loss = 0.25170268
Iteration 144, loss = 0.25504423
Iteration 145, loss = 0.25416149
Iteration 146, loss = 0.25303534
Iteration 147, loss = 0.25253980
Iteration 148, loss = 0.25159329
Iteration 149, loss = 0.25343451
Iteration 150, loss = 0.25201328
Iteration 151, loss = 0.25232139
Iteration 152, loss = 0.25306645
Iteration 153, loss = 0.25319013
Iteration 154, loss = 0.25093244
Iteration 155, loss = 0.25206547
Iteration 156, loss = 0.25321252
Iteration 157, loss = 0.25359808
Iteration 158, loss = 0.25246587
Iteration 159, loss = 0.25196079
Iteration 160, loss = 0.25132785
Iteration 161, loss = 0.25181370
Iteration 162, loss = 0.25275389
Iteration 163, loss = 0.25195206
Iteration 164, loss = 0.25056235
Iteration 165, loss = 0.25000639
Iteration 166, loss = 0.25118944
Iteration 167, loss = 0.25175640
Iteration 168, loss = 0.25050902
Iteration 169, loss = 0.25052121
Iteration 170, loss = 0.25027567
Iteration 171, loss = 0.25154230
Iteration 172, loss = 0.25149182
Iteration 173, loss = 0.25181092
Iteration 174, loss = 0.25096662
Iteration 175, loss = 0.24866289
Iteration 176, loss = 0.25017960
Iteration 177, loss = 0.25117073
Iteration 178, loss = 0.25032062
Iteration 179, loss = 0.25085499
Iteration 180, loss = 0.24953457
Iteration 181, loss = 0.25108534
Iteration 182, loss = 0.24942482
Iteration 183, loss = 0.24976199
Iteration 184, loss = 0.24941147
Iteration 185, loss = 0.24923059
Iteration 186, loss = 0.24987467
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

Iteration 119, loss = 0.25484891
Iteration 120, loss = 0.25523707
Iteration 121, loss = 0.25378058
Iteration 122, loss = 0.25527781
Iteration 123, loss = 0.25395987
Iteration 124, loss = 0.25399649
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
